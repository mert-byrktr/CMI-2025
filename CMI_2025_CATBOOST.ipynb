{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPH1bbFZz7V",
        "outputId": "34daa1ca-3277-4f4f-d0b4-e7883bd95a15"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "2e33772371384c5a94ea080fcac7bd6a",
            "fa38b9a8fa2749bea474c97b395e3657",
            "c602d20290cb47948e9bb4ef32b1dd94",
            "48090fab89be459890e851cb18d2a0fe",
            "d494bffc569747708e63197f71e45d2e",
            "5d64969ed4954d278d7b3abdcac196f5",
            "ea86cc6d22524aba8334d6e1dbc350e7",
            "9285236320d44b3f9c82a8a173e7fbf3",
            "acd4a123d22149308d2e93d7384d4688",
            "7136bbe018af4c6aa15ea70d8375566d",
            "3efe2783c5c54a3a96086febb10c1ddf",
            "61544e61a6084a74ace8c9edbbb5ee06",
            "8cb062c5ea3642d7965e6495ff2dde3d",
            "68b1ceb7439d4660b0881734c8576e63",
            "9e62f6d73aa746ad9d1ed267bd8e3dda",
            "a3c9f57f9bda47c592f7a9c126897b07",
            "990250e2c3c4463db96cf61f9d718ab6"
          ]
        },
        "id": "ka_oHwl5_5MN",
        "outputId": "788ee205-9280-4ce5-b99d-7f8bd62dd38b"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_g6C36S_5MR",
        "outputId": "1b19c2c6-f194-4c21-f029-cf7281e14837"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "cmi_detect_behavior_with_sensor_data_path = kagglehub.competition_download('cmi-detect-behavior-with-sensor-data')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DABZ640TE-5v"
      },
      "outputs": [],
      "source": [
        "!pip install catboost -qq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7jDg6tfTvdp"
      },
      "outputs": [],
      "source": [
        "!pip install iterative-stratification==0.1.7 -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rYRqCy_5MT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import polars as pl\n",
        "import sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "pd.set_option('display.max_columns', 2000)\n",
        "pd.set_option('display.max_rows', 2000)\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8MqxrueedN6",
        "outputId": "3e03cca9-3068-4cda-8fd9-51ecf9351805"
      },
      "outputs": [],
      "source": [
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(pl.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(joblib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "gY6LM36r_5MW",
        "outputId": "c4d3076e-927d-43c7-f526-35fc0090bd7f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "train = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train.csv')\n",
        "test = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test.csv')\n",
        "train_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train_demographics.csv')\n",
        "test_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test_demographics.csv')\n",
        "\n",
        "train = train.to_pandas()\n",
        "test = test.to_pandas()\n",
        "train_demographics = train_demographics.to_pandas()\n",
        "test_demographics = test_demographics.to_pandas()\n",
        "\n",
        "train = pd.merge(train, train_demographics, on='subject', how='left')\n",
        "test = pd.merge(test, test_demographics, on='subject', how='left')\n",
        "\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STB7TmHka7dy",
        "outputId": "45fa63c6-588a-4c22-fd53-ebb6cccfcf4e"
      },
      "outputs": [],
      "source": [
        "train.gesture.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI0x997JyzZm"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE1eosIxt8ow"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# for target in train['gesture'].unique():\n",
        "#     print(f\"--- Analyzing gesture: {target} ---\")\n",
        "\n",
        "#     gesture_df = train[train['gesture'] == target].copy()\n",
        "#     print(f\"Found {gesture_df['sequence_id'].nunique()} sequences.\")\n",
        "\n",
        "#     aligned_dfs = []\n",
        "#     for seq_id in gesture_df['sequence_id'].unique():\n",
        "#         seq_data = gesture_df[gesture_df['sequence_id'] == seq_id].copy()\n",
        "#         try:\n",
        "#             gesture_start_time = seq_data[seq_data['behavior'] == 'Performs gesture']['sequence_counter'].iloc[0]\n",
        "#             seq_data['relative_time'] = seq_data['sequence_counter'] - gesture_start_time\n",
        "#             aligned_dfs.append(seq_data)\n",
        "#         except IndexError:\n",
        "#             continue\n",
        "\n",
        "#     if not aligned_dfs:\n",
        "#         print(\"No valid sequences with a 'Gesture' phase found. Skipping plot.\\n\")\n",
        "#         continue\n",
        "\n",
        "#     aligned_df = pd.concat(aligned_dfs)\n",
        "\n",
        "#     if len(aligned_df['handedness'].unique()) < 2:\n",
        "#         print(\"Not enough handedness diversity for this gesture. Skipping plot.\\n\")\n",
        "#         continue\n",
        "\n",
        "#     stats_df = aligned_df.groupby(['handedness', 'relative_time']).agg(\n",
        "#         acc_x_mean=('acc_x', 'mean'), acc_x_std=('acc_x', 'std'),\n",
        "#         acc_y_mean=('acc_y', 'mean'), acc_y_std=('acc_y', 'std'),\n",
        "#         acc_z_mean=('acc_z', 'mean'), acc_z_std=('acc_z', 'std'),\n",
        "#         rot_w_mean=('rot_w', 'mean'), rot_w_std=('rot_w', 'std'),\n",
        "#         rot_x_mean=('rot_x', 'mean'), rot_x_std=('rot_x', 'std'),\n",
        "#         rot_y_mean=('rot_y', 'mean'), rot_y_std=('rot_y', 'std'),\n",
        "#         rot_z_mean=('rot_z', 'mean'), rot_z_std=('rot_z', 'std'),\n",
        "#     ).reset_index()\n",
        "\n",
        "#     rh_stats = stats_df[stats_df['handedness'] == 1]\n",
        "#     lh_stats = stats_df[stats_df['handedness'] == 0]\n",
        "\n",
        "#     # --- 4a. Plot ACCELEROMETER Data ---\n",
        "#     fig_acc, axes_acc = plt.subplots(3, 1, figsize=(18, 15), sharex=True)\n",
        "#     fig_acc.suptitle(f'Mean Accelerometer Signals for \"{target}\" (Aligned at Gesture Start)', fontsize=18)\n",
        "#     axes_acc[0].set_xlim(-30, 60)\n",
        "\n",
        "#     for i, axis in enumerate(['x', 'y', 'z']):\n",
        "#         ax = axes_acc[i]\n",
        "#         ax.plot(rh_stats['relative_time'], rh_stats[f'acc_{axis}_mean'], color='blue', linewidth=2, label='Mean Right-Handed')\n",
        "#         ax.fill_between(rh_stats['relative_time'], rh_stats[f'acc_{axis}_mean'] - rh_stats[f'acc_{axis}_std'], rh_stats[f'acc_{axis}_mean'] + rh_stats[f'acc_{axis}_std'], color='blue', alpha=0.1)\n",
        "#         ax.plot(lh_stats['relative_time'], -lh_stats[f'acc_{axis}_mean'], color='darkred', linestyle='--', linewidth=2, label='Mean -1 * Left-Handed (Hypothesis)')\n",
        "#         ax.fill_between(lh_stats['relative_time'], -lh_stats[f'acc_{axis}_mean'] - lh_stats[f'acc_{axis}_std'], -lh_stats[f'acc_{axis}_mean'] + lh_stats[f'acc_{axis}_std'], color='red', alpha=0.1)\n",
        "#         ax.axvline(x=0, color='black', linestyle=':', linewidth=3, label='Gesture Start (t=0)')\n",
        "#         ax.set_ylabel(f'acc_{axis}', fontsize=12)\n",
        "#         ax.grid(True, which='both', linestyle='--')\n",
        "#         ax.legend()\n",
        "#     plt.xlabel('Time Relative to Gesture Start (steps)', fontsize=14)\n",
        "#     plt.show()\n",
        "\n",
        "#     # --- 4b. Plot ROTATION Data ---\n",
        "#     fig_rot, axes_rot = plt.subplots(4, 1, figsize=(18, 20), sharex=True)\n",
        "#     fig_rot.suptitle(f'Mean Rotation (Quaternion) Signals for \"{target}\" (Aligned at Gesture Start)', fontsize=18)\n",
        "#     axes_rot[0].set_xlim(-30, 60)\n",
        "\n",
        "#     for i, axis in enumerate(['w', 'x', 'y', 'z']):\n",
        "#         ax = axes_rot[i]\n",
        "#         ax.plot(rh_stats['relative_time'], rh_stats[f'rot_{axis}_mean'], color='blue', linewidth=2, label='Mean Right-Handed')\n",
        "#         ax.fill_between(rh_stats['relative_time'], rh_stats[f'rot_{axis}_mean'] - rh_stats[f'rot_{axis}_std'], rh_stats[f'rot_{axis}_mean'] + rh_stats[f'rot_{axis}_std'], color='blue', alpha=0.1)\n",
        "#         ax.plot(lh_stats['relative_time'], -lh_stats[f'rot_{axis}_mean'], color='darkred', linestyle='--', linewidth=2, label='Mean -1 * Left-Handed (Hypothesis)')\n",
        "#         ax.fill_between(lh_stats['relative_time'], -lh_stats[f'rot_{axis}_mean'] - lh_stats[f'rot_{axis}_std'], -lh_stats[f'rot_{axis}_mean'] + lh_stats[f'rot_{axis}_std'], color='red', alpha=0.1)\n",
        "#         ax.axvline(x=0, color='black', linestyle=':', linewidth=3, label='Gesture Start (t=0)')\n",
        "#         ax.set_ylabel(f'rot_{axis}', fontsize=12)\n",
        "#         ax.grid(True, which='both', linestyle='--')\n",
        "#         ax.legend()\n",
        "#     plt.xlabel('Time Relative to Gesture Start (steps)', fontsize=14)\n",
        "#     plt.show()\n",
        "\n",
        "#     print(\"-\" * 40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ic-BSLy1d3"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaWmMuIS_5MZ"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "  TARGET = \"gesture\"\n",
        "  SUBJECT = \"subject\"\n",
        "  TRAIN_ONLY_COLS = ['sequence_type', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
        "  NUM_CLASSES = train.gesture.nunique()\n",
        "  FOLDS = 5\n",
        "  ERR = 1e-8\n",
        "\n",
        "imu_cols = [\n",
        "            \"acc_x\", \"acc_y\", \"acc_z\",\n",
        "            \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
        "            \"acc_mag\",\n",
        "\n",
        "            \"euler_roll\", \"euler_pitch\", \"euler_yaw\",\n",
        "            \"euler_total\", \"pitch_roll_ratio\", \"yaw_pitch_ratio\",\n",
        "\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "\n",
        "            \"angular_jerk_x\", \"angular_jerk_y\", \"angular_jerk_z\",\n",
        "            ]\n",
        "\n",
        "\n",
        "demo_cols = [\"adult_child\", \"age\", \"sex\", \"handedness\", \"height_cm\", \"shoulder_to_wrist_cm\", \"elbow_to_wrist_cm\"]\n",
        "\n",
        "thm_cols = [\"thm_1\", \"thm_2\",\"thm_3\",\"thm_4\",\"thm_5\"]\n",
        "tof_cols = [col for col in test.columns if col.startswith(\"tof_\")]\n",
        "\n",
        "seq_agg_cols = []\n",
        "for col in imu_cols:\n",
        "    seq_agg_cols.extend([\n",
        "        f'{col}_seq_mean',\n",
        "        f'{col}_seq_std',\n",
        "        f'{col}_seq_min',\n",
        "        ])\n",
        "\n",
        "\n",
        "position_cols = []\n",
        "for col in imu_cols:\n",
        "    position_cols.extend([\n",
        "\n",
        "        f'{col}_avg_velocity',\n",
        "\n",
        "        f'{col}_early_mean',\n",
        "        f'{col}_mid_mean',\n",
        "        f'{col}_mid2_mean',\n",
        "        f'{col}_mid3_mean',\n",
        "        f'{col}_late_mean',\n",
        "\n",
        "        f'{col}_early_std',\n",
        "        f'{col}_mid_std',\n",
        "        f'{col}_mid2_std',\n",
        "        f'{col}_mid3_std',\n",
        "        f'{col}_late_std',\n",
        "\n",
        "        f'{col}_early_velocity_mean',\n",
        "        f'{col}_mid_velocity_mean',\n",
        "        f'{col}_mid2_velocity_mean',\n",
        "        f'{col}_mid3_velocity_mean',\n",
        "        f'{col}_late_velocity_mean',\n",
        "\n",
        "        f'{col}_early_velocity_std',\n",
        "        f'{col}_mid_velocity_std',\n",
        "        f'{col}_mid2_velocity_std',\n",
        "        f'{col}_mid3_velocity_std',\n",
        "        f'{col}_late_velocity_std',\n",
        "\n",
        "        # f'{col}_early_energy',\n",
        "        # f'{col}_mid_energy',\n",
        "        # f'{col}_mid2_energy',\n",
        "        # f'{col}_mid3_energy',\n",
        "        f'{col}_late_energy',\n",
        "\n",
        "        f'{col}_very_late_mean',\n",
        "        f'{col}_very_late_std',\n",
        "\n",
        "        f'{col}_early_late_mean_ratio',\n",
        "        f'{col}_early_late_std_ratio',\n",
        "        f'{col}_early_late_energy_ratio',\n",
        "\n",
        "    ])\n",
        "\n",
        "for col in imu_cols:\n",
        "    position_cols.extend([\n",
        "        f\"{col}_late_gesture_zero_crossing_rate_std\",\n",
        "        # f\"{col}_dwell_time\",\n",
        "    ])\n",
        "\n",
        "corr_cols = []\n",
        "axis_pairs = [('acc_x', 'acc_y'), ('acc_x', 'acc_z'), ('acc_y', 'acc_z')]\n",
        "for axis1, axis2 in axis_pairs:\n",
        "    corr_cols.extend([\n",
        "        f'{axis1}_{axis2}_corr',\n",
        "    ])\n",
        "\n",
        "euler_pairs = [('euler_roll', 'euler_pitch'), ('euler_roll', 'euler_yaw'), ('euler_pitch', 'euler_yaw')]\n",
        "for axis1, axis2 in euler_pairs:\n",
        "    corr_cols.extend([\n",
        "        f'{axis1}_{axis2}_corr',\n",
        "    ])\n",
        "\n",
        "\n",
        "# FEATURES = imu_cols + seq_agg_cols + temporal_cols + position_cols + corr_cols\n",
        "FEATURES = imu_cols + seq_agg_cols + position_cols + corr_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qXHGYeF_5Ma",
        "outputId": "97dff7be-bf60-4942-b61d-b33c83a5c71b"
      },
      "outputs": [],
      "source": [
        "len(FEATURES), len(imu_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kqrDFah0pM9",
        "outputId": "bac7a50a-3c04-429d-8194-7f59559730ac"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgJjAVIXJFoU",
        "outputId": "e8aa537f-e7e4-46ba-ae13-6ec2d015c96f"
      },
      "outputs": [],
      "source": [
        "train.phase.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IhTfovKYtkl",
        "outputId": "2d35bb75-3c45-44a1-d09f-c23ce008833e"
      },
      "outputs": [],
      "source": [
        "train.sequence_type.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XofuSIjG2bfq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7nJrmq5sdv"
      },
      "source": [
        "## FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "BaGOykD_YRPn",
        "outputId": "65fc3d18-d50c-4b8b-a7cd-aba2d43aa0f7"
      },
      "outputs": [],
      "source": [
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SS49wH-CbSy"
      },
      "outputs": [],
      "source": [
        "train = train[train['subject'] != 'SUBJ_011323']\n",
        "# # # train = train[train['subject'] != 'SUBJ_045235']\n",
        "# # # train = train[train['subject'] != 'SUBJ_019262']\n",
        "train = train[train['sequence_id'] != 'SEQ_011975']\n",
        "\n",
        "# # for seq_id in missing_rot_sequences:\n",
        "# #   train = train[train['sequence_id'] != seq_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guCRwJKT2Xhu"
      },
      "outputs": [],
      "source": [
        "def noise_augmentation(df):\n",
        "    \"\"\"Add realistic sensor noise\"\"\"\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "    print(\"Adding noise\")\n",
        "    # augmented_df = df[df[\"sequence_type\"] == \"Target\"].copy()\n",
        "    augmented_df = df.copy()\n",
        "\n",
        "    # Add small amounts of Gaussian noise (realistic for sensor data)\n",
        "    for col in ['acc_x', 'acc_y', 'acc_z']:\n",
        "        noise = np.random.normal(0, 0.1, len(augmented_df))\n",
        "        augmented_df[col] += noise\n",
        "\n",
        "    augmented_df[\"sequence_id\"] = augmented_df[\"sequence_id\"].astype(str) + \"_noise\"\n",
        "\n",
        "    final_df = pd.concat([df, augmented_df], ignore_index=True)\n",
        "    print(f\"Final shape: {final_df.shape}\")\n",
        "\n",
        "    return final_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTDoFvXb_5Ma",
        "outputId": "51a6302f-e2d2-4f97-e130-8be351c0ddc7"
      },
      "outputs": [],
      "source": [
        "def cast_to_object(df):\n",
        "  df['adult_child'] = df['adult_child'].astype(\"category\")\n",
        "  df['sex'] = df['sex'].astype(\"category\")\n",
        "  df['handedness'] = df['handedness'].astype(\"category\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def aggregation_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "    agg_exprs = []\n",
        "\n",
        "    for col in imu_cols:\n",
        "        agg_exprs.extend([\n",
        "            # Basic statistics\n",
        "            pl.col(col).mean().over('sequence_id').alias(f'{col}_seq_mean'),\n",
        "            pl.col(col).std().over('sequence_id').alias(f'{col}_seq_std'),\n",
        "            pl.col(col).min().over('sequence_id').alias(f'{col}_seq_min'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(agg_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def quaternion_to_euler(w, x, y, z):\n",
        "    \"\"\"Convert quaternion to Euler angles (roll, pitch, yaw) in radians\"\"\"\n",
        "\n",
        "    sinr_cosp = 2 * (w * x + y * z)\n",
        "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
        "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
        "\n",
        "    sinp = 2 * (w * y - z * x)\n",
        "    pitch = np.where(np.abs(sinp) >= 1, np.copysign(np.pi / 2, sinp), np.arcsin(sinp))\n",
        "\n",
        "    siny_cosp = 2 * (w * z + x * y)\n",
        "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "\n",
        "    return roll, pitch, yaw\n",
        "\n",
        "def mag_features(df):\n",
        "\n",
        "    df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"]**2 + df[\"acc_y\"]**2 + df[\"acc_z\"]**2)\n",
        "    df[\"rot_mag\"] = np.sqrt(df[\"rot_x\"]**2 + df[\"rot_y\"]**2 + df[\"rot_z\"]**2)\n",
        "\n",
        "    roll, pitch, yaw = quaternion_to_euler(df[\"rot_w\"], df[\"rot_x\"], df[\"rot_y\"], df[\"rot_z\"])\n",
        "\n",
        "    df[\"euler_roll\"] = roll\n",
        "    df[\"euler_pitch\"] = pitch\n",
        "    df[\"euler_yaw\"] = yaw\n",
        "\n",
        "    # Euler angle magnitudes and combinations\n",
        "    df[\"euler_total\"] = np.sqrt(roll**2 + pitch**2 + yaw**2)\n",
        "    df[\"pitch_roll_ratio\"] = pitch / (np.abs(roll) + CONFIG.ERR)\n",
        "    df[\"yaw_pitch_ratio\"] = yaw / (np.abs(pitch) + CONFIG.ERR)\n",
        "\n",
        "    return df\n",
        "\n",
        "def rotation_matrix_features(df):\n",
        "    \"\"\"Extract features from rotation matrices - captures 3D orientation relationships\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "\n",
        "    rot_matrix_exprs = []\n",
        "\n",
        "\n",
        "    rot_matrix_exprs.extend([\n",
        "        (1 - 2*(pl.col('rot_y')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r11'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') - pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r12'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r13'),\n",
        "\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') + pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r21'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r22'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r23'),\n",
        "\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r31'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r32'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_y')**2)).alias('rot_matrix_r33'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(rot_matrix_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def angular_velocity_features(df):\n",
        "    \"\"\"Derive angular velocity from quaternion derivatives\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    angular_vel_exprs = []\n",
        "\n",
        "\n",
        "    angular_vel_exprs.extend([\n",
        "        pl.col('rot_w').diff().over('sequence_id').alias('rot_w_dot'),\n",
        "        pl.col('rot_x').diff().over('sequence_id').alias('rot_x_dot'),\n",
        "        pl.col('rot_y').diff().over('sequence_id').alias('rot_y_dot'),\n",
        "        pl.col('rot_z').diff().over('sequence_id').alias('rot_z_dot'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_vel_exprs)\n",
        "\n",
        "    angular_vel_components = []\n",
        "    angular_vel_components.extend([\n",
        "        (2 * (-pl.col('rot_x')*pl.col('rot_w_dot') + pl.col('rot_w')*pl.col('rot_x_dot') +\n",
        "              pl.col('rot_y')*pl.col('rot_z_dot') - pl.col('rot_z')*pl.col('rot_y_dot'))).alias('angular_vel_x'),\n",
        "        (2 * (-pl.col('rot_y')*pl.col('rot_w_dot') + pl.col('rot_w')*pl.col('rot_y_dot') +\n",
        "              pl.col('rot_z')*pl.col('rot_x_dot') - pl.col('rot_x')*pl.col('rot_z_dot'))).alias('angular_vel_y'),\n",
        "        (2 * (-pl.col('rot_z')*pl.col('rot_w_dot') + pl.col('rot_w')*pl.col('rot_z_dot') +\n",
        "              pl.col('rot_x')*pl.col('rot_y_dot') - pl.col('rot_y')*pl.col('rot_x_dot'))).alias('angular_vel_z'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_vel_components)\n",
        "\n",
        "    angular_vel_derived = []\n",
        "    angular_vel_derived.extend([\n",
        "        (pl.col('angular_vel_x')**2 + pl.col('angular_vel_y')**2 + pl.col('angular_vel_z')**2).sqrt().alias('angular_speed'),\n",
        "\n",
        "        pl.col('angular_vel_x').diff().over('sequence_id').alias('angular_accel_x'),\n",
        "        pl.col('angular_vel_y').diff().over('sequence_id').alias('angular_accel_y'),\n",
        "        pl.col('angular_vel_z').diff().over('sequence_id').alias('angular_accel_z'),\n",
        "\n",
        "        pl.max_horizontal([\n",
        "            pl.col('angular_vel_x').abs(),\n",
        "            pl.col('angular_vel_y').abs(),\n",
        "            pl.col('angular_vel_z').abs()\n",
        "        ]).alias('dominant_angular_vel'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_vel_derived)\n",
        "\n",
        "    angular_final = []\n",
        "    angular_final.extend([\n",
        "        (pl.col('angular_accel_x')**2 + pl.col('angular_accel_y')**2 + pl.col('angular_accel_z')**2).sqrt().alias('angular_accel_magnitude'),\n",
        "\n",
        "        pl.col('angular_accel_x').diff().over('sequence_id').alias('angular_jerk_x'),\n",
        "        pl.col('angular_accel_y').diff().over('sequence_id').alias('angular_jerk_y'),\n",
        "        pl.col('angular_accel_z').diff().over('sequence_id').alias('angular_jerk_z'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_final)\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def temporal_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "        # Normalize sequence counter to 0-1 range per sequence\n",
        "    df = df.with_columns([\n",
        "        ((pl.col('sequence_counter') - pl.col('sequence_counter').min().over('sequence_id')) /\n",
        "         (pl.col('sequence_counter').max().over('sequence_id') - pl.col('sequence_counter').min().over('sequence_id') + CONFIG.ERR))\n",
        "        .alias('normalized_position')\n",
        "    ])\n",
        "\n",
        "\n",
        "    for col in imu_cols:\n",
        "        temporal_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').alias(f'{col}_velocity'),\n",
        "\n",
        "            (pl.col(col) / (pl.col(col).shift(1).over('sequence_id').abs() + CONFIG.ERR)).alias(f'{col}_pct_vs_prev'),\n",
        "            (pl.col(col) / (pl.col(col).shift(-1).over('sequence_id').abs() + CONFIG.ERR)).alias(f'{col}_pct_vs_next'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(temporal_exprs)\n",
        "\n",
        "    acc_exprs = []\n",
        "    for col in imu_cols:\n",
        "        acc_exprs.extend([\n",
        "            pl.col(col).diff(n=4).over('sequence_id').alias(f'{col}_snap'),\n",
        "            pl.col(col).diff(n=5).over('sequence_id').alias(f'{col}_crackle'),\n",
        "            pl.col(col).diff(n=6).over('sequence_id').alias(f'{col}_pop'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(acc_exprs)\n",
        "\n",
        "    peak_exprs = []\n",
        "    for col in imu_cols:\n",
        "        peak_exprs.extend([\n",
        "            ((pl.col(f'{col}_pct_vs_prev').abs() > 1.2) &\n",
        "             (pl.col(f'{col}_pct_vs_next').abs() > 1.2)).alias(f'{col}_is_peak'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(peak_exprs)\n",
        "\n",
        "    agg_temporal_exprs = []\n",
        "    for col in imu_cols:\n",
        "        agg_temporal_exprs.extend([\n",
        "            pl.col(f'{col}_velocity').abs().mean().over('sequence_id').alias(f'{col}_avg_velocity'),\n",
        "            pl.col(f'{col}_is_peak').sum().over('sequence_id').alias(f'{col}_peak_count'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(agg_temporal_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def position_features(df):\n",
        "    \"\"\"Features based on position within sequence\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    position_exprs = []\n",
        "    for col in imu_cols:\n",
        "        position_exprs.extend([\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_early_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid2_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid3_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_late_mean'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_early_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid2_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid3_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_late_std'),\n",
        "\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_early_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_mid_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_mid2_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_mid3_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_late_velocity_mean'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_early_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_mid_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_mid2_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_mid3_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_late_velocity_std'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_early_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_mid_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_mid2_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_mid3_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_late_energy'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') >= 0.9)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_very_late_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.9)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_very_late_std'),\n",
        "\n",
        "          ])\n",
        "\n",
        "    df = df.with_columns(position_exprs)\n",
        "\n",
        "    position_ratio_exprs = []\n",
        "    for col in imu_cols:\n",
        "        position_ratio_exprs.extend([\n",
        "            (pl.col(f'{col}_early_mean') / pl.col(f'{col}_late_mean')+CONFIG.ERR).alias(f'{col}_early_late_mean_ratio'),\n",
        "            (pl.col(f'{col}_early_std') / pl.col(f'{col}_late_std')+CONFIG.ERR).alias(f'{col}_early_late_std_ratio'),\n",
        "            (pl.col(f'{col}_early_energy') / pl.col(f'{col}_late_energy')+CONFIG.ERR).alias(f'{col}_early_late_energy_ratio'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(position_ratio_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def zero_crossing_features(df):\n",
        "    \"\"\"\n",
        "    Creates and compares three different methods for identifying \"significant change\".\n",
        "\n",
        "    1.  vs_value: Change relative to the current signal value.\n",
        "    2.  vs_std: Change relative to the signal's standard deviation over the sequence.\n",
        "    3.  vs_mean: Change relative to the signal's mean over the sequence.\n",
        "\n",
        "    For each method, it generates a '{col}_flag_...' and a '{col}_rate_...' column.\n",
        "    \"\"\"\n",
        "    df_pl = pl.from_pandas(df)\n",
        "\n",
        "\n",
        "    prereq_exprs = []\n",
        "    for col in imu_cols:\n",
        "        prereq_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').abs().alias(f'{col}_diff_abs'),\n",
        "            pl.col(col).mean().over('sequence_id').alias(f'{col}_mean'),\n",
        "            pl.col(col).std().over('sequence_id').alias(f'{col}_std'),\n",
        "\n",
        "            pl.col(col).filter(pl.col('normalized_position') >= 0.6)\n",
        "              .mean().over('sequence_id').alias(f'{col}_late_mean'),\n",
        "\n",
        "            pl.col(col).filter(pl.col('normalized_position') >= 0.6)\n",
        "              .std().over('sequence_id').alias(f'{col}_late_std'),\n",
        "\n",
        "        ])\n",
        "\n",
        "    df_pl = df_pl.with_columns(prereq_exprs)\n",
        "\n",
        "    threshold_exprs = []\n",
        "    for col in imu_cols:\n",
        "        late_phase_diffs = pl.col(f'{col}_diff_abs').filter(pl.col('normalized_position') >= 0.6)\n",
        "\n",
        "        threshold_exprs.extend([\n",
        "            late_phase_diffs.mean().over('sequence_id').alias(f'{col}_mean_of_diffs_threshold'),\n",
        "            late_phase_diffs.std().over('sequence_id').alias(f'{col}_std_of_diffs_threshold'),\n",
        "        ])\n",
        "\n",
        "    df_pl = df_pl.with_columns(threshold_exprs)\n",
        "\n",
        "    feature_exprs = []\n",
        "    for col in imu_cols:\n",
        "        # # Method 1: Relative to Current Value\n",
        "        # flag_vs_value = (pl.col(f'{col}_diff_abs') > pl.col(col).abs() * 0.1).cast(pl.Int32).alias(f'{col}_flag_vs_value')\n",
        "        # zero_crossing_rate = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_value).mean().over('sequence_id').alias(f'{col}_gesture_zero_crossing_rate')\n",
        "\n",
        "        # # Method 2: Relative to Standard Deviation\n",
        "        # flag_vs_std = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_std')).cast(pl.Int32).alias(f'{col}_flag_vs_std')\n",
        "        # zero_crossing_rate = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_std).mean().over('sequence_id').alias(f'{col}_gesture_zero_crossing_rate')\n",
        "\n",
        "        # Method 3: Relative to Mean\n",
        "        # flag_vs_mean = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_mean').abs() * 0.1).cast(pl.Int32).alias(f'{col}_flag_vs_mean')\n",
        "        # zero_crossing_rate = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_mean).mean().over('sequence_id').alias(f'{col}_gesture_zero_crossing_rate')\n",
        "\n",
        "        # flag_vs_local_late_mean = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_late_mean').abs() * 0.1).cast(pl.Int32)\n",
        "        # late_zero_crossing_rate_mean = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_local_late_mean).mean().over('sequence_id').alias(f'{col}_late_gesture_zero_crossing_rate_mean')\n",
        "\n",
        "        flag_vs_local_late_std = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_late_std')).cast(pl.Int32)\n",
        "        late_zero_crossing_rate_std = pl.when(pl.col('normalized_position') >= 0.6).then(flag_vs_local_late_std).mean().over('sequence_id').alias(f'{col}_late_gesture_zero_crossing_rate_std')\n",
        "\n",
        "        flag_is_dwelling = (pl.col(f'{col}_diff_abs') < pl.col(f'{col}_late_std')).cast(pl.Int32)\n",
        "        dwell_time = pl.when(pl.col('normalized_position') >= 0.6).then(flag_is_dwelling).sum().over('sequence_id').alias(f'{col}_dwell_time')\n",
        "\n",
        "\n",
        "        # flag_vs_mean_diff = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_mean_of_diffs_threshold')).cast(pl.Int32)\n",
        "        # rate_vs_mean_diff = pl.when(pl.col('normalized_position') >= 0.6).then(flag_vs_mean_diff).mean().over('sequence_id').alias(f'{col}_rate_vs_mean_diff')\n",
        "\n",
        "        # flag_vs_std_diff = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_std_of_diffs_threshold')).cast(pl.Int32)\n",
        "        # rate_vs_std_diff = pl.when(pl.col('normalized_position') >= 0.6).then(flag_vs_std_diff).mean().over('sequence_id').alias(f'{col}_rate_vs_std_diff')\n",
        "\n",
        "        feature_exprs.extend([\n",
        "            late_zero_crossing_rate_std,\n",
        "            # dwell_time,\n",
        "        ])\n",
        "\n",
        "    df_pl = df_pl.with_columns(feature_exprs)\n",
        "\n",
        "    return df_pl.to_pandas()\n",
        "\n",
        "\n",
        "def cross_axis_correlation_features(df):\n",
        "    \"\"\"Correlation and coordination between different axes\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    corr_exprs = []\n",
        "\n",
        "    axis_pairs = [('acc_x', 'acc_y'), ('acc_x', 'acc_z'), ('acc_y', 'acc_z')]\n",
        "    for axis1, axis2 in axis_pairs:\n",
        "        corr_exprs.append(\n",
        "            pl.corr(pl.col(axis1), pl.col(axis2)).over('sequence_id').alias(f'{axis1}_{axis2}_corr')\n",
        "        )\n",
        "\n",
        "    euler_pairs = [('euler_roll', 'euler_pitch'), ('euler_roll', 'euler_yaw'), ('euler_pitch', 'euler_yaw')]\n",
        "    for axis1, axis2 in euler_pairs:\n",
        "        corr_exprs.append(\n",
        "            pl.corr(pl.col(axis1), pl.col(axis2)).over('sequence_id').alias(f'{axis1}_{axis2}_corr')\n",
        "        )\n",
        "\n",
        "    df = df.with_columns(corr_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "# print(\"CASTING OBJECT TYPES\")\n",
        "# train = cast_to_object(train)\n",
        "\n",
        "# print(\"MAG FEATURES\")\n",
        "# train = mag_features(train)\n",
        "\n",
        "# print(\"ROTATION MATRIX FEATURES\")\n",
        "# train = rotation_matrix_features(train)\n",
        "\n",
        "# print(\"ANGULAR VELOCITY FEATURES\")\n",
        "# train = angular_velocity_features(train)\n",
        "\n",
        "# print(\"AGGREGATION FEATURES\")\n",
        "# train = aggregation_features(train)\n",
        "\n",
        "# print(\"TEMPORAL FEATURES\")\n",
        "# train = temporal_features(train)\n",
        "\n",
        "# print(\"POSITION FEATURES\")\n",
        "# train = position_features(train)\n",
        "\n",
        "# print(\"ZERO CROSSING RATES\")\n",
        "# train = zero_crossing_features(train)\n",
        "\n",
        "# print(\"CORR FEATURES\")\n",
        "# train = cross_axis_correlation_features(train)\n",
        "\n",
        "def apply_feature_engineering(df):\n",
        "    print(\"  Applying feature engineering...\")\n",
        "    df = cast_to_object(df)\n",
        "    df = mag_features(df)\n",
        "    df = rotation_matrix_features(df)\n",
        "    df = angular_velocity_features(df)\n",
        "    df = aggregation_features(df)\n",
        "    df = temporal_features(df)\n",
        "    df = position_features(df)\n",
        "    df = zero_crossing_features(df)\n",
        "    df = cross_axis_correlation_features(df)\n",
        "    return df\n",
        "\n",
        "train = apply_feature_engineering(train)\n",
        "# test = apply_feature_engineering(test)\n",
        "\n",
        "for i in imu_cols:\n",
        "  FEATURES.remove(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsorwyW8qbiV",
        "outputId": "8828a146-1132-4871-b293-bbdba86f1e48"
      },
      "outputs": [],
      "source": [
        "print(train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzAAHTc6Vbyr",
        "outputId": "d44b4a4f-69ea-45ad-fcde-6bab37984c5f"
      },
      "outputs": [],
      "source": [
        "train.phase.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U5F0bZQm1xoW",
        "outputId": "99543730-7666-47b9-b1e4-072a8c9e3dfc"
      },
      "outputs": [],
      "source": [
        "train.sequence_id.iloc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yUsrq9dIQnv",
        "outputId": "9eab5744-5c3c-442d-f35f-58479b32d863"
      },
      "outputs": [],
      "source": [
        "len(FEATURES), train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ul16-2QYoT"
      },
      "source": [
        "## TOF THM FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX524iTnxA5r"
      },
      "outputs": [],
      "source": [
        "thm_cols = [\n",
        "    \"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\",\n",
        "\n",
        "    \"thm_12_diff\", \"thm_13_diff\", \"thm_14_diff\",\n",
        "    \"thm_15_diff\", \"thm_23_diff\", \"thm_24_diff\", \"thm_25_diff\",\n",
        "    \"thm_34_diff\", \"thm_35_diff\", \"thm_45_diff\",\n",
        "\n",
        "]\n",
        "\n",
        "tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
        "\n",
        "tof_diff_cols = [f\"tof_{i}{j}_mean_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]\n",
        "tof_diff_cols += [f\"tof_{i}{j}_std_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-IITROb8keh",
        "outputId": "7317c4c0-dc53-4d01-9ad7-c090ff1d9b9c"
      },
      "outputs": [],
      "source": [
        "def thermopile_features(df):\n",
        "    \"\"\"Extract features from thermopile sensors (temperature)\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    thm_diff_exprs = []\n",
        "    thm_diff_exprs.extend([\n",
        "        (pl.col('thm_1') - pl.col('thm_2')).abs().alias('thm_12_diff'),\n",
        "        (pl.col('thm_1') - pl.col('thm_3')).abs().alias('thm_13_diff'),\n",
        "        (pl.col('thm_1') - pl.col('thm_4')).abs().alias('thm_14_diff'),\n",
        "        (pl.col('thm_1') - pl.col('thm_5')).abs().alias('thm_15_diff'),\n",
        "        (pl.col('thm_2') - pl.col('thm_3')).abs().alias('thm_23_diff'),\n",
        "        (pl.col('thm_2') - pl.col('thm_4')).abs().alias('thm_24_diff'),\n",
        "        (pl.col('thm_2') - pl.col('thm_5')).abs().alias('thm_25_diff'),\n",
        "        (pl.col('thm_3') - pl.col('thm_4')).abs().alias('thm_34_diff'),\n",
        "        (pl.col('thm_3') - pl.col('thm_5')).abs().alias('thm_35_diff'),\n",
        "        (pl.col('thm_4') - pl.col('thm_5')).abs().alias('thm_45_diff'),\n",
        "\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(thm_diff_exprs)\n",
        "\n",
        "    thm_exprs = []\n",
        "    for col in thm_cols:\n",
        "        thm_exprs.extend([\n",
        "            pl.col(col).mean().over('sequence_id').alias(f'{col}_seq_mean'),\n",
        "            pl.col(col).std().over('sequence_id').alias(f'{col}_seq_std'),\n",
        "            pl.col(col).min().over('sequence_id').alias(f'{col}_seq_min'),\n",
        "            pl.col(col).max().over('sequence_id').alias(f'{col}_seq_max'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_early_temp_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid_temp_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid2_temp_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid3_temp_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_late_temp_mean'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_early_temp_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid_temp_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid2_temp_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid3_temp_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_late_temp_std'),\n",
        "\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_early_temp_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid_temp_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid2_temp_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid3_temp_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_late_temp_max'),\n",
        "\n",
        "\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col).diff().max().over('sequence_id').alias(f'{col}_late_max_heating_rate')),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col).diff().min().over('sequence_id').alias(f'{col}_late_max_cooling_rate')),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col).diff()).mean().over('sequence_id').alias(f'{col}_late_temp_rate'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col).diff()).std().over('sequence_id').alias(f'{col}_late_temp_rate_std'),\n",
        "\n",
        "            pl.col(col).diff().diff().over('sequence_id').alias(f'{col}_temp_acceleration'),\n",
        "\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(thm_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def tof_features(df):\n",
        "    \"\"\"Extract features from time-of-flight sensors (proximity/distance)\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_sensor_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        tof_sensor_exprs.extend([\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "              .list.mean().alias(f'tof_{sensor_idx}_mean_distance'),\n",
        "\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "              .list.std().alias(f'tof_{sensor_idx}_std_distance'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "def tof_cross_sensor_features(df):\n",
        "    \"\"\"Cross-sensor ToF features similar to thermopile differences\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_diff_exprs = []\n",
        "    tof_diff_exprs.extend([\n",
        "        (pl.col('tof_1_mean_distance') - pl.col('tof_2_mean_distance')).abs().alias('tof_12_mean_diff'),\n",
        "        (pl.col('tof_1_mean_distance') - pl.col('tof_3_mean_distance')).abs().alias('tof_13_mean_diff'),\n",
        "        (pl.col('tof_1_mean_distance') - pl.col('tof_4_mean_distance')).abs().alias('tof_14_mean_diff'),\n",
        "        (pl.col('tof_1_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_15_mean_diff'),\n",
        "        (pl.col('tof_2_mean_distance') - pl.col('tof_3_mean_distance')).abs().alias('tof_23_mean_diff'),\n",
        "        (pl.col('tof_2_mean_distance') - pl.col('tof_4_mean_distance')).abs().alias('tof_24_mean_diff'),\n",
        "        (pl.col('tof_2_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_25_mean_diff'),\n",
        "        (pl.col('tof_3_mean_distance') - pl.col('tof_4_mean_distance')).abs().alias('tof_34_mean_diff'),\n",
        "        (pl.col('tof_3_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_35_mean_diff'),\n",
        "        (pl.col('tof_4_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_45_mean_diff'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(tof_diff_exprs)\n",
        "\n",
        "    overall_tof_exprs = []\n",
        "    overall_tof_exprs.extend([\n",
        "        # Overall proximity signature\n",
        "        ((pl.col('tof_1_mean_distance') + pl.col('tof_2_mean_distance') + pl.col('tof_3_mean_distance') +\n",
        "          pl.col('tof_4_mean_distance') + pl.col('tof_5_mean_distance')) / 5).alias('tof_overall_mean_distance'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(overall_tof_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def tof_sequence_features(df):\n",
        "    \"\"\"Sequence-level ToF features with phase analysis\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_diff_cols = [col for col in df.columns if 'tof_' in col and '_diff' in col]\n",
        "    # tof_overall_cols = ['tof_overall_mean_distance', 'tof_closest_sensor_distance',\n",
        "    #                    'tof_furthest_sensor_distance', 'tof_proximity_spread', 'tof_spatial_variance']\n",
        "\n",
        "    seq_exprs = []\n",
        "\n",
        "    for col in ['tof_1_mean_distance', 'tof_2_mean_distance', 'tof_3_mean_distance',\n",
        "                'tof_4_mean_distance', 'tof_5_mean_distance'] + tof_diff_cols:\n",
        "        seq_exprs.extend([\n",
        "            pl.col(col).mean().over('sequence_id').alias(f'{col}_seq_mean'),\n",
        "            pl.col(col).std().over('sequence_id').alias(f'{col}_seq_std'),\n",
        "            pl.col(col).min().over('sequence_id').alias(f'{col}_seq_min'),\n",
        "            pl.col(col).max().over('sequence_id').alias(f'{col}_seq_max'),\n",
        "\n",
        "            # Phase-specific features\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_early_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid2_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid3_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_late_mean'),\n",
        "\n",
        "            # Late phase proximity dynamics (critical for BFRB)\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_early_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid2_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid3_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_late_std'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_early_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid2_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid3_max'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_late_max'),\n",
        "\n",
        "\n",
        "            # pl.when(pl.col('normalized_position') >= 0.8)\n",
        "            #   .then(pl.col(col)).min().over('sequence_id').alias(f'{col}_late_min'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(seq_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def tof_spatial_structure_features(df):\n",
        "    \"\"\"Extract spatial structure features from 8x8 ToF grids\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    spatial_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "        horizontal_edges = []\n",
        "        vertical_edges = []\n",
        "\n",
        "\n",
        "        for row in range(7):  # 7 transitions between 8 rows\n",
        "            row_start = row * 8\n",
        "            next_row_start = (row + 1) * 8\n",
        "\n",
        "            for col in range(8):\n",
        "                pixel1 = f\"tof_{sensor_idx}_v{row_start + col}\"\n",
        "                pixel2 = f\"tof_{sensor_idx}_v{next_row_start + col}\"\n",
        "\n",
        "                horizontal_edges.append(\n",
        "                    pl.when((pl.col(pixel1) != -1) & (pl.col(pixel2) != -1))\n",
        "                    .then((pl.col(pixel1) - pl.col(pixel2)).abs())\n",
        "                )\n",
        "        for row in range(8):\n",
        "            for col in range(7):  \n",
        "                pixel1 = f'tof_{sensor_idx}_v{row * 8 + col}'\n",
        "                pixel2 = f'tof_{sensor_idx}_v{row * 8 + col + 1}'\n",
        "\n",
        "                vertical_edges.append(\n",
        "                    pl.when((pl.col(pixel1) != -1) & (pl.col(pixel2) != -1))\n",
        "                    .then((pl.col(pixel1) - pl.col(pixel2)).abs())\n",
        "                )\n",
        "\n",
        "        spatial_exprs.extend([\n",
        "            pl.concat_list(horizontal_edges).list.mean().mean().over('sequence_id')\n",
        "            .alias(f'tof_{sensor_idx}_horizontal_edge_strength_seq'),\n",
        "\n",
        "            pl.concat_list(vertical_edges).list.mean().mean().over('sequence_id')\n",
        "            .alias(f'tof_{sensor_idx}_vertical_edge_strength_seq'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "            .then(pl.concat_list(horizontal_edges).list.mean())\n",
        "            .mean().over('sequence_id')\n",
        "            .alias(f'tof_{sensor_idx}_horizontal_edge_strength_late'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "            .then(pl.concat_list(vertical_edges).list.mean())\n",
        "            .mean().over('sequence_id')\n",
        "            .alias(f'tof_{sensor_idx}_vertical_edge_strength_late'),\n",
        "\n",
        "            pl.when((pl.col(f'tof_{sensor_idx}_v27') != -1) &\n",
        "                   (pl.col(f'tof_{sensor_idx}_v28') != -1) &\n",
        "                   (pl.col(f'tof_{sensor_idx}_v35') != -1) &\n",
        "                   (pl.col(f'tof_{sensor_idx}_v36') != -1))\n",
        "            .then((pl.col(f'tof_{sensor_idx}_v27') + pl.col(f'tof_{sensor_idx}_v28') +\n",
        "                  pl.col(f'tof_{sensor_idx}_v35') + pl.col(f'tof_{sensor_idx}_v36')) / 4)\n",
        "            .mean().over('sequence_id')\n",
        "            .alias(f'tof_{sensor_idx}_center_pixels_seq_mean'),\n",
        "\n",
        "            # Corner pixel activity\n",
        "            pl.when((pl.col(f'tof_{sensor_idx}_v0') != -1) &\n",
        "                   (pl.col(f'tof_{sensor_idx}_v7') != -1) &\n",
        "                   (pl.col(f'tof_{sensor_idx}_v56') != -1) &\n",
        "                   (pl.col(f'tof_{sensor_idx}_v63') != -1))\n",
        "            .then((pl.col(f'tof_{sensor_idx}_v0') + pl.col(f'tof_{sensor_idx}_v7') +\n",
        "                  pl.col(f'tof_{sensor_idx}_v56') + pl.col(f'tof_{sensor_idx}_v63')) / 4)\n",
        "            .mean().over('sequence_id')\n",
        "            .alias(f'tof_{sensor_idx}_corner_pixels_seq_mean'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(spatial_exprs)\n",
        "\n",
        "\n",
        "    advanced_tof_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "\n",
        "        center_pixels = [f'tof_{sensor_idx}_v{i}' for i in [27, 28, 35, 36]]  # 2x2 center\n",
        "        edge_pixels = [f'tof_{sensor_idx}_v{i}' for i in [0, 1, 2, 3, 4, 5, 6, 7,  # top row\n",
        "                                                          56, 57, 58, 59, 60, 61, 62, 63,  # bottom row\n",
        "                                                          8, 16, 24, 32, 40, 48,  # left column\n",
        "                                                          15, 23, 31, 39, 47, 55]]  # right column\n",
        "\n",
        "        advanced_tof_exprs.extend([\n",
        "            (pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col))\n",
        "                            for col in center_pixels]).list.mean() /\n",
        "             (pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col))\n",
        "                             for col in edge_pixels]).list.mean() + 0.001))\n",
        "            .alias(f'tof_{sensor_idx}_center_edge_ratio'),\n",
        "\n",
        "\n",
        "            (pl.concat_list([pl.when(pl.col(f'tof_{sensor_idx}_v{row*8 + col}') != -1)\n",
        "                            .then(pl.col(f'tof_{sensor_idx}_v{row*8 + col}'))\n",
        "                            for row in range(8) for col in range(4)]).list.mean() -\n",
        "             pl.concat_list([pl.when(pl.col(f'tof_{sensor_idx}_v{row*8 + col}') != -1)\n",
        "                            .then(pl.col(f'tof_{sensor_idx}_v{row*8 + col}'))\n",
        "                            for row in range(8) for col in range(4, 8)]).list.mean()).abs()\n",
        "            .alias(f'tof_{sensor_idx}_left_right_asymmetry'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(advanced_tof_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "# def cat_tof_regional_features_func(df, tof_mode=\"stats\", include_regions=True):\n",
        "#     \"\"\"\n",
        "#     Extract features from time-of-flight sensors with regional analysis\n",
        "\n",
        "#     Args:\n",
        "#         df: DataFrame with ToF data\n",
        "#         tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "#         include_regions: Whether to include regional analysis features\n",
        "#     \"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     tof_sensor_exprs = []\n",
        "\n",
        "#     # Basic statistics for each ToF sensor (5 sensors total)\n",
        "#     for sensor_idx in range(1, 6):\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         # # Basic stats (replace -1 with null for proper statistics)\n",
        "#         # tof_sensor_exprs.extend([\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.mean().alias(f'tof_{sensor_idx}_mean'),\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.std().alias(f'tof_{sensor_idx}_std'),\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.min().alias(f'tof_{sensor_idx}_min'),\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.max().alias(f'tof_{sensor_idx}_max'),\n",
        "#         # ])\n",
        "\n",
        "#         # Regional Analysis - divide 8x8 grid into regions\n",
        "#         if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "#             # Different region modes\n",
        "#             region_modes = [4] if tof_mode == \"regions\" else [4]\n",
        "\n",
        "#             for mode in region_modes:\n",
        "#                 region_size = 64 // mode  # pixels per region\n",
        "\n",
        "#                 for region_idx in range(mode):\n",
        "#                     start_pixel = region_idx * region_size\n",
        "#                     end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "#                     # Get pixel columns for this region\n",
        "#                     region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "\n",
        "#                     # Calculate regional statistics\n",
        "#                     tof_sensor_exprs.extend([\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.mean().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mean'),\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.std().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_std'),\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.min().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_min'),\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.max().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_max'),\n",
        "\n",
        "#                     ])\n",
        "\n",
        "#     df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "\n",
        "def cat_tof_regional_features_func(df, tof_mode=\"stats\", include_regions=True):\n",
        "    \"\"\"\n",
        "    Extract features from time-of-flight sensors with regional analysis across phases\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with ToF data\n",
        "        tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "        include_regions: Whether to include regional analysis features\n",
        "    \"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_sensor_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "            region_modes = [4] if tof_mode == \"regions\" else [4]\n",
        "\n",
        "            for mode in region_modes:\n",
        "                region_size = 64 // mode  \n",
        "\n",
        "                for region_idx in range(mode):\n",
        "                    start_pixel = region_idx * region_size\n",
        "                    end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "                    region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "\n",
        "                    region_values_expr = pl.concat_list([\n",
        "                        pl.when(pl.col(col) != -1).then(pl.col(col))\n",
        "                        for col in region_pixel_cols\n",
        "                    ])\n",
        "\n",
        "                    # Phase-specific regional statistics (5 phases)\n",
        "                    # Early phase (normalized_position < 0.2)\n",
        "                    # tof_sensor_exprs.extend([\n",
        "                    #     pl.when(pl.col('normalized_position') < 0.2)\n",
        "                    #       .then(region_values_expr.list.mean())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_early_mean'),\n",
        "                    #     pl.when(pl.col('normalized_position') < 0.2)\n",
        "                    #       .then(region_values_expr.list.std())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_early_std'),\n",
        "                    #     pl.when(pl.col('normalized_position') < 0.2)\n",
        "                    #       .then(region_values_expr.list.max())\n",
        "                    #       .max().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_early_max'),\n",
        "\n",
        "                    # ])\n",
        "\n",
        "                    # # Mid phase (0.2 <= normalized_position < 0.4)\n",
        "                    # tof_sensor_exprs.extend([\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "                    #       .then(region_values_expr.list.mean())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid_mean'),\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "                    #       .then(region_values_expr.list.std())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid_std'),\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "                    #       .then(region_values_expr.list.max())\n",
        "                    #       .max().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid_max'),\n",
        "                    # ])\n",
        "\n",
        "                    # # Mid2 phase (0.4 <= normalized_position < 0.6)\n",
        "                    # tof_sensor_exprs.extend([\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "                    #       .then(region_values_expr.list.mean())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid2_mean'),\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "                    #       .then(region_values_expr.list.std())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid2_std'),\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "                    #       .then(region_values_expr.list.max())\n",
        "                    #       .max().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid2_max'),\n",
        "                    # ])\n",
        "\n",
        "                    # # Mid3 phase (0.6 <= normalized_position < 0.8)\n",
        "                    # tof_sensor_exprs.extend([\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "                    #       .then(region_values_expr.list.mean())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid3_mean'),\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "                    #       .then(region_values_expr.list.std())\n",
        "                    #       .mean().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid3_std'),\n",
        "                    #     pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "                    #       .then(region_values_expr.list.max())\n",
        "                    #       .max().over('sequence_id')\n",
        "                    #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid3_max'),\n",
        "                    # ])\n",
        "\n",
        "                    tof_sensor_exprs.extend([\n",
        "                        pl.when(pl.col('normalized_position') >= 0.6)\n",
        "                          .then(region_values_expr.list.mean())\n",
        "                          .mean().over('sequence_id')\n",
        "                          .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_mean'),\n",
        "                        pl.when(pl.col('normalized_position') >= 0.6)\n",
        "                          .then(region_values_expr.list.std())\n",
        "                          .mean().over('sequence_id')\n",
        "                          .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_std'),\n",
        "                        pl.when(pl.col('normalized_position') >= 0.6)\n",
        "                          .then(region_values_expr.list.max())\n",
        "                          .max().over('sequence_id')\n",
        "                          .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_max'),\n",
        "                        pl.when(pl.col('normalized_position') >= 0.6)\n",
        "                          .then(region_values_expr.list.min())\n",
        "                          .min().over('sequence_id')\n",
        "                          .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_in'),\n",
        "\n",
        "                    ])\n",
        "\n",
        "    df = df.with_columns(tof_sensor_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def cat_advanced_tof_features(df):\n",
        "    \"\"\"Advanced time-of-flight feature engineering\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_advanced_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        tof_advanced_exprs.extend([\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[:16]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[:16]])).alias(f'tof_{sensor_idx}_contrast_q1'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[16:32]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[16:32]])).alias(f'tof_{sensor_idx}_contrast_q2'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[32:48]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[32:48]])).alias(f'tof_{sensor_idx}_contrast_q3'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[48:64]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[48:64]])).alias(f'tof_{sensor_idx}_contrast_q4'),\n",
        "\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(tof_advanced_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "train = thermopile_features(train)\n",
        "\n",
        "train = tof_features(train)\n",
        "train = tof_cross_sensor_features(train)\n",
        "train = tof_sequence_features(train)\n",
        "train = tof_spatial_structure_features(train)\n",
        "train = cat_tof_regional_features_func(train, tof_mode=\"regions\")\n",
        "# train = cat_advanced_tof_features(train)\n",
        "\n",
        "\n",
        "\n",
        "thm_feature_cols = [col for col in train.columns if 'thm_' in col and col not in thm_cols]\n",
        "# tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "\n",
        "\n",
        "FEATURES_FULL = FEATURES + thm_feature_cols + tof_feature_cols\n",
        "print(len(FEATURES_FULL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJuAJBGWQWSk"
      },
      "source": [
        "## REDUCE MEMORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOxRa7Mt_5Mb"
      },
      "outputs": [],
      "source": [
        "# def reduce_mem_usage(df, verbose=True):\n",
        "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2 # calculate current memory usage\n",
        "\n",
        "#     for col in df.columns:\n",
        "#         col_type = df[col].dtype\n",
        "#         if col_type in numerics: # check if column is numeric\n",
        "#             c_min = df[col].min()\n",
        "#             c_max = df[col].max()\n",
        "#             if str(col_type).startswith('int'): # if integer\n",
        "#                 # Check if data can be safely cast to smaller int types\n",
        "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "#                     df[col] = df[col].astype(np.int8)\n",
        "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "#                     df[col] = df[col].astype(np.int16)\n",
        "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "#                     df[col] = df[col].astype(np.int32)\n",
        "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "#                     df[col] = df[col].astype(np.int64) # Should already be this or smaller if loaded as int\n",
        "#             else: # if float\n",
        "#                 # Check if data can be safely cast to float32 (float16 often loses too much precision)\n",
        "#                 if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "#                     df[col] = df[col].astype(np.float32)\n",
        "#                 # else: # If not, keep as float64\n",
        "#                 #     df[col] = df[col].astype(np.float64) # Already this type\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     if verbose:\n",
        "#         print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "#     return df\n",
        "\n",
        "# print(\"Reducing memory for train_df:\")\n",
        "# train = reduce_mem_usage(train)\n",
        "# print(\"\\nReducing memory for test_df:\")\n",
        "# test = reduce_mem_usage(test)\n",
        "\n",
        "# print(\"\\nTrain DataFrame info after memory reduction:\")\n",
        "# train.info(memory_usage='deep')\n",
        "# print(\"\\nTest DataFrame info after memory reduction:\")\n",
        "# test.info(memory_usage='deep')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJYVNzGqBS63"
      },
      "source": [
        "## METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQRvcrT_5Mb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hierarchical macro F1 metric for the CMI 2025 Challenge.\n",
        "\n",
        "This script defines a single entry point `score(solution, submission, row_id_column_name)`\n",
        "that the Kaggle metrics orchestrator will call.\n",
        "It performs validation on submission IDs and computes a combined binary & multiclass F1 score.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.target_gestures = [\n",
        "            'Above ear - pull hair',\n",
        "            'Cheek - pinch skin',\n",
        "            'Eyebrow - pull hair',\n",
        "            'Eyelash - pull hair',\n",
        "            'Forehead - pull hairline',\n",
        "            'Forehead - scratch',\n",
        "            'Neck - pinch skin',\n",
        "            'Neck - scratch',\n",
        "        ]\n",
        "        self.non_target_gestures = [\n",
        "            'Write name on leg',\n",
        "            'Wave hello',\n",
        "            'Glasses on/off',\n",
        "            'Text on phone',\n",
        "            'Write name in air',\n",
        "            'Feel around in tray and pull out an object',\n",
        "            'Scratch knee/leg skin',\n",
        "            'Pull air toward your face',\n",
        "            'Drink from bottle/cup',\n",
        "            'Pinch knee/leg skin'\n",
        "        ]\n",
        "        self.all_classes = self.target_gestures + self.non_target_gestures\n",
        "\n",
        "    def calculate_hierarchical_f1(\n",
        "        self,\n",
        "        sol: pd.DataFrame,\n",
        "        sub: pd.DataFrame\n",
        "    ) -> float:\n",
        "\n",
        "        # Validate gestures\n",
        "        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n",
        "        if invalid_types:\n",
        "            raise ParticipantVisibleError(\n",
        "                f\"Invalid gesture values in submission: {invalid_types}\"\n",
        "            )\n",
        "\n",
        "        # Compute binary F1 (Target vs Non-Target)\n",
        "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
        "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
        "        f1_binary = f1_score(\n",
        "            y_true_bin,\n",
        "            y_pred_bin,\n",
        "            pos_label=True,\n",
        "            zero_division=0,\n",
        "            average='binary'\n",
        "        )\n",
        "\n",
        "        # Build multi-class labels for gestures\n",
        "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "\n",
        "        # Compute macro F1 over all gesture classes\n",
        "        f1_macro = f1_score(\n",
        "            y_true_mc,\n",
        "            y_pred_mc,\n",
        "            average='macro',\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f'f1_binary score: {f1_binary}')\n",
        "        print(f'f1_macro score: {f1_macro}')\n",
        "\n",
        "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
        "\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute hierarchical macro F1 for the CMI 2025 challenge.\n",
        "\n",
        "    Expected input:\n",
        "      - solution and submission as pandas.DataFrame\n",
        "      - Column 'sequence_id': unique identifier for each sequence\n",
        "      - 'gesture': one of the eight target gestures or \"Non-Target\"\n",
        "\n",
        "    This metric averages:\n",
        "    1. Binary F1 on SequenceType (Target vs Non-Target)\n",
        "    2. Macro F1 on gesture (mapping non-targets to \"Non-Target\")\n",
        "\n",
        "    Raises ParticipantVisibleError for invalid submissions,\n",
        "    including invalid SequenceType or gesture values.\n",
        "\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> row_id_column_name = \"id\"\n",
        "    >>> solution = pd.DataFrame({'id': range(4), 'gesture': ['Eyebrow - pull hair']*4})\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Forehead - pull hairline']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.5\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Text on phone']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.0\n",
        "    >>> score(solution, solution, row_id_column_name=row_id_column_name)\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    # Validate required columns\n",
        "    for col in (row_id_column_name, 'gesture'):\n",
        "        if col not in solution.columns:\n",
        "            raise ParticipantVisibleError(f\"Solution file missing required column: '{col}'\")\n",
        "        if col not in submission.columns:\n",
        "            raise ParticipantVisibleError(f\"Submission file missing required column: '{col}'\")\n",
        "\n",
        "    metric = CompetitionMetric()\n",
        "    return metric.calculate_hierarchical_f1(solution, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQg5lnUubyRZ"
      },
      "source": [
        "## MIXUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be7atd_Zbw1n"
      },
      "outputs": [],
      "source": [
        "# def feature_mixup_for_catboost(X_train, y_train, alpha=0.3, mix_prob=0.5):\n",
        "#     \"\"\"Apply mixup to engineered features for CatBoost\"\"\"\n",
        "#     X_mixed = []\n",
        "#     y_mixed = []\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     for i in range(len(X_train)):\n",
        "#         if np.random.rand() < mix_prob:\n",
        "#             # Apply mixup\n",
        "#             j = np.random.randint(0, len(X_train))\n",
        "#             lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "#             # Mix features (this works for statistical features)\n",
        "#             x_mix = lam * X_train.iloc[i] + (1 - lam) * X_train.iloc[j]\n",
        "\n",
        "#             # For labels, use the dominant class (hard decision)\n",
        "#             if lam > 0.5:\n",
        "#                 y_mix = y_train.iloc[i]\n",
        "#             else:\n",
        "#                 y_mix = y_train.iloc[j]\n",
        "\n",
        "#             X_mixed.append(x_mix)\n",
        "#             y_mixed.append(y_mix)\n",
        "#         else:\n",
        "#             # Keep original\n",
        "#             X_mixed.append(X_train.iloc[i])\n",
        "#             y_mixed.append(y_train.iloc[i])\n",
        "\n",
        "    # return pd.DataFrame(X_mixed), pd.Series(y_mixed)\n",
        "\n",
        "def feature_mixup_for_catboost(X_train, y_train, alpha=0.3, mix_prob=1.0):\n",
        "    \"\"\"\n",
        "    Applies feature space mixup by creating new synthetic rows and adding them\n",
        "    to the original dataset.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Original feature data.\n",
        "        y_train (pd.Series): Original label data.\n",
        "        alpha (float): Beta distribution parameter for Mixup.\n",
        "        augmentation_factor (float): The fraction of new data to generate.\n",
        "                                     0.5 means increase dataset size by 50%.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    num_original_samples = len(X_train)\n",
        "    num_new_samples = int(num_original_samples * mix_prob)\n",
        "\n",
        "    if num_new_samples == 0:\n",
        "        return X_train, y_train\n",
        "\n",
        "    print(f\"Original samples: {num_original_samples}. Adding {num_new_samples} mixed samples.\")\n",
        "\n",
        "    indices1 = np.random.randint(0, num_original_samples, num_new_samples)\n",
        "    indices2 = np.random.randint(0, num_original_samples, num_new_samples)\n",
        "\n",
        "    lams = np.random.beta(alpha, alpha, size=num_new_samples)\n",
        "\n",
        "\n",
        "    X1 = X_train.iloc[indices1].values\n",
        "    X2 = X_train.iloc[indices2].values\n",
        "    X_new = lams[:, np.newaxis] * X1 + (1 - lams)[:, np.newaxis] * X2\n",
        "\n",
        "    y1 = y_train.iloc[indices1].values\n",
        "    y2 = y_train.iloc[indices2].values\n",
        "    y_new = np.where(lams > 0.5, y1, y2)\n",
        "\n",
        "\n",
        "    X_augmented_df = pd.DataFrame(X_new, columns=X_train.columns)\n",
        "    y_augmented_series = pd.Series(y_new)\n",
        "\n",
        "\n",
        "    X_final = pd.concat([X_train, X_augmented_df], ignore_index=True)\n",
        "    y_final = pd.concat([y_train, y_augmented_series], ignore_index=True)\n",
        "\n",
        "    return X_final, y_final\n",
        "\n",
        "\n",
        "# def feature_mixup_for_catboost(X_train, y_train, sequence_ids, alpha=0.3, mix_prob=0.5):\n",
        "#     \"\"\"\n",
        "#     Fast vectorized version of sequence-level mixup.\n",
        "\n",
        "#     Args:\n",
        "#         X_train (pd.DataFrame): Original feature data.\n",
        "#         y_train (pd.Series): Original label data.\n",
        "#         sequence_ids (pd.Series): Sequence ID for each row.\n",
        "#         alpha (float): Beta distribution parameter for Mixup.\n",
        "#         mix_prob (float): The fraction of new sequences to generate.\n",
        "#     \"\"\"\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     # Get unique sequences and their representative indices\n",
        "#     unique_sequences = sequence_ids.unique()\n",
        "#     num_original_sequences = len(unique_sequences)\n",
        "#     num_new_sequences = int(num_original_sequences * mix_prob)\n",
        "\n",
        "#     if num_new_sequences == 0:\n",
        "#         return X_train, y_train\n",
        "\n",
        "#     print(f\"Original sequences: {num_original_sequences}. Adding {num_new_sequences} mixed sequences.\")\n",
        "\n",
        "#     # Create mapping from sequence to first occurrence index (vectorized)\n",
        "#     first_occurrence_mask = ~sequence_ids.duplicated()\n",
        "#     representative_data = X_train[first_occurrence_mask].copy()\n",
        "#     representative_labels = y_train[first_occurrence_mask].copy()\n",
        "#     representative_sequences = sequence_ids[first_occurrence_mask].copy()\n",
        "\n",
        "#     # Create sequence to row count mapping (vectorized)\n",
        "#     sequence_counts = sequence_ids.value_counts()\n",
        "\n",
        "#     # Generate random pairs and mixing weights (all vectorized)\n",
        "#     seq_indices1 = np.random.randint(0, num_original_sequences, num_new_sequences)\n",
        "#     seq_indices2 = np.random.randint(0, num_original_sequences, num_new_sequences)\n",
        "#     lams = np.random.beta(alpha, alpha, size=num_new_sequences)\n",
        "\n",
        "#     # Get the actual sequence IDs for selected indices\n",
        "#     selected_seqs1 = representative_sequences.iloc[seq_indices1].values\n",
        "#     selected_seqs2 = representative_sequences.iloc[seq_indices2].values\n",
        "\n",
        "#     # Mix features (fully vectorized)\n",
        "#     X1 = representative_data.iloc[seq_indices1].values\n",
        "#     X2 = representative_data.iloc[seq_indices2].values\n",
        "#     X_mixed_base = lams[:, np.newaxis] * X1 + (1 - lams)[:, np.newaxis] * X2\n",
        "\n",
        "#     # Mix labels (vectorized hard assignment)\n",
        "#     y1 = representative_labels.iloc[seq_indices1].values\n",
        "#     y2 = representative_labels.iloc[seq_indices2].values\n",
        "#     y_mixed_base = np.where(lams > 0.5, y1, y2)\n",
        "\n",
        "#     # Calculate row counts for mixed sequences (vectorized)\n",
        "#     counts1 = sequence_counts[selected_seqs1].values\n",
        "#     counts2 = sequence_counts[selected_seqs2].values\n",
        "#     mixed_counts = ((counts1 + counts2) / 2).astype(int)\n",
        "\n",
        "#     # Calculate total rows needed\n",
        "#     total_new_rows = mixed_counts.sum()\n",
        "\n",
        "#     # Pre-allocate arrays for maximum speed\n",
        "#     X_new = np.empty((total_new_rows, X_train.shape[1]), dtype=X_train.dtypes.iloc[0])\n",
        "#     y_new = np.empty(total_new_rows, dtype=y_train.dtype)\n",
        "\n",
        "#     # Fill arrays using vectorized operations with repeat\n",
        "#     current_idx = 0\n",
        "#     for i, count in enumerate(mixed_counts):\n",
        "#         end_idx = current_idx + count\n",
        "#         # Use numpy repeat for each mixed sample\n",
        "#         X_new[current_idx:end_idx] = np.repeat(X_mixed_base[i:i+1], count, axis=0)\n",
        "#         y_new[current_idx:end_idx] = y_mixed_base[i]\n",
        "#         current_idx = end_idx\n",
        "\n",
        "#     # Convert to pandas (single operation)\n",
        "#     X_new_df = pd.DataFrame(X_new, columns=X_train.columns)\n",
        "#     y_new_series = pd.Series(y_new)\n",
        "\n",
        "#     # Concatenate (single operation)\n",
        "#     X_final = pd.concat([X_train, X_new_df], ignore_index=True)\n",
        "#     y_final = pd.concat([y_train, y_new_series], ignore_index=True)\n",
        "\n",
        "#     return X_final, y_final\n",
        "\n",
        "def generate_mixed_samples(df, features, target, n_mixed=1000):\n",
        "    \"\"\"Generate mixed samples for CatBoost training\"\"\"\n",
        "    mixed_samples = []\n",
        "\n",
        "    for _ in range(n_mixed):\n",
        "        idx1, idx2 = np.random.choice(len(df), 2, replace=False)\n",
        "        lam = np.random.beta(0.3, 0.3)\n",
        "\n",
        "\n",
        "        mixed_features = lam * df.iloc[idx1][features] + (1 - lam) * df.iloc[idx2][features]\n",
        "        mixed_label = df.iloc[idx1][target] if lam > 0.5 else df.iloc[idx2][target]\n",
        "\n",
        "        mixed_sample = mixed_features.copy()\n",
        "        mixed_sample[target] = mixed_label\n",
        "        mixed_sample['sequence_id'] = f\"mixed_{len(mixed_samples)}\"\n",
        "\n",
        "        mixed_samples.append(mixed_sample)\n",
        "\n",
        "    return pd.DataFrame(mixed_samples)\n",
        "\n",
        "\n",
        "# def mix_time_series_sequences(seq1, seq2, lam):\n",
        "#     \"\"\"Mix two time series sequences\"\"\"\n",
        "#     # Ensure same length\n",
        "#     min_len = min(len(seq1), len(seq2))\n",
        "#     seq1_cut = seq1[:min_len]\n",
        "#     seq2_cut = seq2[:min_len]\n",
        "\n",
        "#     # Linear interpolation\n",
        "#     mixed_seq = lam * seq1_cut + (1 - lam) * seq2_cut\n",
        "\n",
        "#     return mixed_seq\n",
        "\n",
        "# def create_mixed_sequences(df, n_mixed=500):\n",
        "#     \"\"\"Create mixed sequences, then extract features\"\"\"\n",
        "#     mixed_data = []\n",
        "\n",
        "#     sequence_ids = df['sequence_id'].unique()\n",
        "\n",
        "#     for _ in range(n_mixed):\n",
        "#         # Sample two sequences\n",
        "#         seq_id1, seq_id2 = np.random.choice(sequence_ids, 2, replace=False)\n",
        "#         seq1_data = df[df['sequence_id'] == seq_id1]\n",
        "#         seq2_data = df[df['sequence_id'] == seq_id2]\n",
        "\n",
        "#         lam = np.random.beta(0.3, 0.3)\n",
        "\n",
        "#         # Mix sensor readings\n",
        "#         mixed_features = {}\n",
        "#         for col in ['acc_x', 'acc_y', 'acc_z', 'euler_roll', 'euler_pitch', 'euler_yaw']:\n",
        "#             if col in df.columns:\n",
        "#                 mixed_col = lam * seq1_data[col].values + (1 - lam) * seq2_data[col].values\n",
        "#                 mixed_features[col] = mixed_col\n",
        "\n",
        "#         # Extract features from mixed sequence\n",
        "#         mixed_seq_features = extract_sequence_features(mixed_features)\n",
        "\n",
        "#         # Choose dominant label\n",
        "#         label = seq1_data['gesture'].iloc[0] if lam > 0.5 else seq2_data['gesture'].iloc[0]\n",
        "#         mixed_seq_features['gesture'] = label\n",
        "\n",
        "#         mixed_data.append(mixed_seq_features)\n",
        "\n",
        "#     return pd.DataFrame(mixed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY0xfze8BWf_"
      },
      "source": [
        "## CATBOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVCof-hT_5Mc",
        "outputId": "18d92f36-f34a-488a-f2e1-135c902ca289"
      },
      "outputs": [],
      "source": [
        "import catboost\n",
        "from catboost import CatBoostClassifier\n",
        "print(catboost.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX9mXU3heCYj"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "cat_params = {\n",
        "    'objective': 'MultiClass',\n",
        "    'eval_metric': 'MultiClass',\n",
        "    'learning_rate': 0.03,\n",
        "    'depth': 7,\n",
        "    'random_seed': 42,\n",
        "    'iterations': 1000,\n",
        "    'task_type': 'GPU',\n",
        "    # 'reg_lambda': 5,\n",
        "    'verbose': 50,\n",
        "}\n",
        "\n",
        "le = LabelEncoder()\n",
        "train['gesture'] = le.fit_transform(train['gesture'])\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "metric_calculator = CompetitionMetric()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Iq3yxoGslbh"
      },
      "outputs": [],
      "source": [
        "# train = train.groupby('sequence_id').last().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Grc406_eiTxv",
        "outputId": "d9a664c2-c6ff-4994-85d4-432227f94e46"
      },
      "outputs": [],
      "source": [
        "X_train = train[FEATURES_FULL]\n",
        "y_train = train[CONFIG.TARGET]\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VL3rJFY5Wrn",
        "outputId": "d7b2bc03-2e81-4a48-db5f-6d5cb3a28a7a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import uuid\n",
        "run_id  = uuid.uuid4()\n",
        "\n",
        "os.makedirs('models_full', exist_ok=True)\n",
        "n_splits=5\n",
        "\n",
        "t_d = train_demographics\n",
        "skf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(\n",
        "        skf.split(t_d, t_d[['adult_child', 'handedness', 'sex']])\n",
        "    ):\n",
        "    t_d.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "t_d['fold'] = t_d['fold'].astype(int)\n",
        "print(\"Demographics fold distribution:\\n\", t_d['fold'].value_counts(), \"\\n\")\n",
        "\n",
        "train = train.merge(t_d[['subject', 'fold']], on='subject', how='left')\n",
        "\n",
        "oof_preds_cat              = np.zeros(len(train), dtype=int)\n",
        "oof_preds_proba_cat        = np.zeros((len(train), CONFIG.NUM_CLASSES))\n",
        "oof_scores                 = []\n",
        "all_fold_importances       = []\n",
        "\n",
        "metric_calculator = CompetitionMetric()\n",
        "\n",
        "\n",
        "for fold in range(n_splits):\n",
        "    print(f\"{'#'*10} Fold {fold+1} {'#'*10}\")\n",
        "\n",
        "    train_idx = train.index[train['fold'] != fold].tolist()\n",
        "    valid_idx = train.index[train['fold'] == fold].tolist()\n",
        "\n",
        "    X_train_orig = train.loc[train_idx, FEATURES_FULL].copy()\n",
        "    y_train_orig = train.loc[train_idx, CONFIG.TARGET].copy()\n",
        "    sequence_ids = train.loc[train_idx, 'sequence_id'].copy()\n",
        "\n",
        "    # X_train = train.loc[train_idx, FEATURES].copy()\n",
        "    # y_train = train.loc[train_idx, CONFIG.TARGET].copy()\n",
        "\n",
        "    print(f\"Shape before Mixup: {X_train_orig.shape}\")\n",
        "    print(f\"Valid shape before Mixup: {len(valid_idx)}\")\n",
        "\n",
        "    X_train, y_train = feature_mixup_for_catboost(X_train_orig,\n",
        "                                                  y_train_orig,\n",
        "                                                  alpha=0.3, mix_prob=1.0)\n",
        "\n",
        "    print(f\"Shape after Mixup: {X_train.shape}\")\n",
        "\n",
        "    X_valid = train.loc[valid_idx, FEATURES_FULL].copy() #\n",
        "    y_valid = train.loc[valid_idx, CONFIG.TARGET].copy() #\n",
        "\n",
        "    print(f\"  X_train shape: {X_train.shape}, X_valid shape: {X_valid.shape}\")\n",
        "\n",
        "    model = CatBoostClassifier(**cat_params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        use_best_model=True,\n",
        "        # cat_features=cat_features,\n",
        "        verbose=50\n",
        "    )\n",
        "    model.save_model(f\"/content/drive/MyDrive/cmi2025/models/cat_fold_{fold+1}_{run_id}.cbm\")\n",
        "    model.save_model(f\"models_full/cat_fold_{fold+1}.cbm\")\n",
        "\n",
        "\n",
        "    all_fold_importances.append(model.get_feature_importance())\n",
        "\n",
        "    fold_proba = model.predict_proba(X_valid)\n",
        "    oof_preds_proba_cat[valid_idx] = fold_proba\n",
        "    fold_preds = np.argmax(fold_proba, axis=1)\n",
        "\n",
        "    y_valid_orig = le.inverse_transform(y_valid)\n",
        "    preds_orig   = le.inverse_transform(fold_preds)\n",
        "\n",
        "    temp_sol_df = pd.DataFrame({\"gesture\": y_valid_orig})\n",
        "    temp_sub_df = pd.DataFrame({\"gesture\": preds_orig})\n",
        "    fold_score  = metric_calculator.calculate_hierarchical_f1(temp_sol_df, temp_sub_df)\n",
        "\n",
        "    oof_scores.append(fold_score)\n",
        "    print(f\"  Fold {fold+1} Score: {fold_score}\\n\")\n",
        "\n",
        "print(f\"Mean OOF Score: {np.mean(oof_scores):.4f}\")\n",
        "print(f\"Std  OOF Score: {np.std(oof_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmWZved8UYGK",
        "outputId": "ec9448d7-c734-4ccc-c956-a0fb40d7c1d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"Mean OOF Score: {np.mean(oof_scores):.4f}\")\n",
        "print(f\"Std  OOF Score: {np.std(oof_scores):.4f}\\n\")\n",
        "\n",
        "original_labels = le.inverse_transform(train[CONFIG.TARGET])\n",
        "oof_preds_encoded = np.argmax(oof_preds_proba_cat, axis=1)\n",
        "oof_preds_original = le.inverse_transform(oof_preds_encoded)\n",
        "\n",
        "sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "sub_df = pd.DataFrame({\"gesture\": oof_preds_original})\n",
        "\n",
        "np.save('oof_preds_cat.npy', oof_preds_original)\n",
        "np.save('oof_preds_proba_cat.npy', oof_preds_proba_cat)\n",
        "\n",
        "overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "print(f\"Overall OOF Score: {overall_oof:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "## full features:\n",
        "\n",
        "# Mean OOF Score: 0.8251\n",
        "# Std  OOF Score: 0.0057\n",
        "\n",
        "# f1_binary score: 0.9799055112614349\n",
        "# f1_macro score: 0.670842812666158\n",
        "# Overall OOF Score: 0.8254\n",
        "\n",
        "\n",
        "## full features + tof 4 features:\n",
        "# Mean OOF Score: 0.8235\n",
        "# Std  OOF Score: 0.0058\n",
        "\n",
        "# f1_binary score: 0.9796848455254779\n",
        "# f1_macro score: 0.668053497970177\n",
        "# Overall OOF Score: 0.8239\n",
        "\n",
        "\n",
        "## full features + tof 4 features normalize edilmi pozisyona gre:\n",
        "\n",
        "# Mean OOF Score: 0.8258\n",
        "# Std  OOF Score: 0.0078\n",
        "\n",
        "# f1_binary score: 0.9798178957684467\n",
        "# f1_macro score: 0.672360790621561\n",
        "# Overall OOF Score: 0.8261\n",
        "\n",
        "## full features + tof 4 features normalize edilmi pozisyona gre sadece 0.6dan sonras:\n",
        "\n",
        "# Mean OOF Score: 0.8269\n",
        "# Std  OOF Score: 0.0070\n",
        "\n",
        "# f1_binary score: 0.979738011560022\n",
        "# f1_macro score: 0.6742888403172628\n",
        "# Overall OOF Score: 0.8270\n",
        "\n",
        "\n",
        "## full features + tof 4 features normalize edilmi pozisyona gre sadece 0.6dan sonras:\n",
        "# Mean OOF Score: 0.8286\n",
        "# Std  OOF Score: 0.0125\n",
        "\n",
        "# f1_binary score: 0.9817535833918712\n",
        "# f1_macro score: 0.6763128311048723\n",
        "# Overall OOF Score: 0.8290\n",
        "\n",
        "## full features + tof 4 features normalize edilmi pozisyona gre sadece 0.6dan sonras ve active pixels feature:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6JaMUFLNY1Q"
      },
      "source": [
        "## FEATURE IMPORTANCES ETC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ApCv2fuoBcCn",
        "outputId": "e2d64c7a-7984-409d-9a4c-17541e3f3f4d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "importances_matrix = np.vstack(all_fold_importances)  # shape = (n_folds, n_features)\n",
        "mean_importances = np.mean(importances_matrix, axis=0)\n",
        "std_importances  = np.std(importances_matrix, axis=0)\n",
        "\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    \"feature\": FEATURES,\n",
        "    \"mean_importance\": mean_importances,\n",
        "    \"std_importance\": std_importances\n",
        "})\n",
        "feat_imp_df = feat_imp_df.sort_values(by=\"mean_importance\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "features = feat_imp_df[\"feature\"].tolist()\n",
        "means    = feat_imp_df[\"mean_importance\"].values\n",
        "stds     = feat_imp_df[\"std_importance\"].values\n",
        "\n",
        "y_pos = list(range(len(features)))[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, max(6, len(features) * 0.3)))\n",
        "plt.barh(\n",
        "    y=y_pos,\n",
        "    width=means[::-1],\n",
        "    xerr=stds[::-1],\n",
        "    align=\"center\",\n",
        "    ecolor=\"black\",\n",
        "    capsize=4\n",
        ")\n",
        "plt.yticks(y_pos, [features[i] for i in reversed(range(len(features)))])\n",
        "plt.xlabel(\"Mean feature importance\")\n",
        "plt.title(\"Feature importances (mean  std across folds)\")\n",
        "plt.gca().invert_yaxis()  # highest mean at the top\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u5gKCtS5C3LK",
        "outputId": "5de4c141-da75-4218-b414-0a3bee658f06"
      },
      "outputs": [],
      "source": [
        "feat_imp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apXIyLPf544S",
        "outputId": "1e5f617d-6280-44f2-b331-5dc187eb255f"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import json\n",
        "import os\n",
        "\n",
        "log_path = \"/content/drive/MyDrive/cmi2025/models/experiments_log.csv\"\n",
        "\n",
        "fold_columns = {f\"fold_{i+1}_oof\": oof_scores[i] for i in range(len(oof_scores))}\n",
        "\n",
        "summary_columns = {\n",
        "    \"mean_oof_score\":     np.mean(oof_scores),\n",
        "    \"std_oof_score\":      np.std(oof_scores),\n",
        "    \"overall_oof_score\":  overall_oof,\n",
        "    \"cv_method\": skf,\n",
        "    \"no_features\": len(FEATURES),\n",
        "    # \"cat_params\": cat_params\n",
        "}\n",
        "\n",
        "features_column = {\"features_used\": json.dumps(FEATURES)}\n",
        "\n",
        "\n",
        "cat_params_column = {\"cat_params\": cat_params}\n",
        "\n",
        "row_dict = {\n",
        "    \"run_id\": run_id,\n",
        "    **fold_columns,\n",
        "    **summary_columns,\n",
        "    **features_column,\n",
        "    **cat_params_column\n",
        "}\n",
        "\n",
        "if os.path.isfile(log_path):\n",
        "    log_df = pd.read_csv(log_path)\n",
        "    log_df = pd.concat([log_df, pd.DataFrame([row_dict])], ignore_index=True)\n",
        "else:\n",
        "    log_df = pd.DataFrame([row_dict])\n",
        "\n",
        "log_df.to_csv(log_path, index=False)\n",
        "print(f\"Appended run {run_id} to {log_path}\")\n",
        "\n",
        "np.save('/content/drive/MyDrive/cmi2025/models/oof_preds_original_labels.npy', oof_preds_original)\n",
        "np.save('/content/drive/MyDrive/cmi2025/models/oof_preds_cat.npy', oof_preds_cat)\n",
        "target_gestures = [\n",
        "    'Above ear - pull hair',\n",
        "    'Cheek - pinch skin',\n",
        "    'Eyebrow - pull hair',\n",
        "    'Eyelash - pull hair',\n",
        "    'Forehead - pull hairline',\n",
        "    'Forehead - scratch',\n",
        "    'Neck - pinch skin',\n",
        "    'Neck - scratch',\n",
        "]\n",
        "\n",
        "non_target_gestures = [\n",
        "    'Write name on leg',\n",
        "    'Wave hello',\n",
        "    'Glasses on/off',\n",
        "    'Text on phone',\n",
        "    'Write name in air',\n",
        "    'Feel around in tray and pull out an object',\n",
        "    'Scratch knee/leg skin',\n",
        "    'Pull air toward your face',\n",
        "    'Drink from bottle/cup',\n",
        "    'Pinch knee/leg skin'\n",
        "]\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'run_id': run_id,\n",
        "    'target_encoded': train[CONFIG.TARGET],\n",
        "    'target_original': original_labels,\n",
        "    'pred_encoded': oof_preds_cat,\n",
        "    'pred_original': oof_preds_original,\n",
        "    'subject': train[CONFIG.SUBJECT],\n",
        "    'sequence_id': train['sequence_id'],\n",
        "    'correct_prediction': (original_labels == oof_preds_original)\n",
        "})\n",
        "\n",
        "results_df['target_category'] = results_df['target_original'].apply(\n",
        "    lambda x: 'target' if x in target_gestures else 'non-target'\n",
        ")\n",
        "results_df['pred_category'] = results_df['pred_original'].apply(\n",
        "    lambda x: 'target' if x in target_gestures else 'non-target'\n",
        ")\n",
        "\n",
        "results_df.to_csv('/content/drive/MyDrive/cmi2025/models/results.csv', index=False)\n",
        "print(results_df.head())\n",
        "sequence_results_df = results_df.groupby('sequence_id').last().reset_index()\n",
        "sequence_results_df.to_csv('/content/drive/MyDrive/cmi2025/models/sequence_results.csv', index=False)\n",
        "print(sequence_results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "vG5mDudZ67-Q",
        "outputId": "4e0ad6ce-4cc8-4baf-c074-14406ee768ed"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_models():\n",
        "    models = []\n",
        "    for i in range(CONFIG.FOLDS):\n",
        "        model = CatBoostClassifier()\n",
        "        model.load_model(f'/content/models_full/cat_fold_{i+1}.pkl')\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "imu_models = load_models()\n",
        "\n",
        "imu_models[0].plot_tree(tree_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZnBTqN_WTAk"
      },
      "source": [
        "## FEATURE REMOVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "dsZnVjPmgoMl",
        "outputId": "779c0f76-006c-4cf0-fd5c-5aead8baabac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "metric_calculator = CompetitionMetric()\n",
        "\n",
        "\n",
        "# Feature groups to test (removing one at a time)\n",
        "feature_groups_to_test = [\n",
        "        '_avg_velocity',\n",
        "\n",
        "        '_early_mean',\n",
        "        '_mid_mean',\n",
        "        '_mid2_mean',\n",
        "        '_mid3_mean',\n",
        "        '_late_mean',\n",
        "\n",
        "        '_early_std',\n",
        "        '_mid_std',\n",
        "        '_mid2_std',\n",
        "        '_mid3_std',\n",
        "        '_late_std',\n",
        "\n",
        "        '_early_velocity_mean',\n",
        "        '_mid_velocity_mean',\n",
        "        '_mid2_velocity_mean',\n",
        "        '_mid3_velocity_mean',\n",
        "        '_late_velocity_mean',\n",
        "\n",
        "        '_early_velocity_std',\n",
        "        '_mid_velocity_std',\n",
        "        '_mid2_velocity_std',\n",
        "        '_mid3_velocity_std',\n",
        "        '_late_velocity_std',\n",
        "\n",
        "        # f'{col}_early_energy',\n",
        "        # f'{col}_mid_energy',\n",
        "        # f'{col}_mid2_energy',\n",
        "        # f'{col}_mid3_energy',\n",
        "        '_late_energy',\n",
        "\n",
        "        # f'{col}_early_gesture_oscillations', #\n",
        "        # f'{col}_mid_gesture_oscillations', #\n",
        "        # f'{col}_mid2_gesture_oscillations', #\n",
        "        # f'{col}_mid3_gesture_oscillations', #\n",
        "        # f'{col}_late_gesture_oscillations', #\n",
        "\n",
        "        '_early_late_mean_ratio',\n",
        "        '_early_late_std_ratio',\n",
        "        '_early_late_energy_ratio',\n",
        "]\n",
        "\n",
        "def get_features_excluding_group(feature_suffix, original_features, imu_cols):\n",
        "    \"\"\"\n",
        "    Get feature list excluding all features that end with the given suffix\n",
        "    \"\"\"\n",
        "    excluded_features = []\n",
        "    for col in imu_cols:\n",
        "        excluded_features.append(f'{col}{feature_suffix}')\n",
        "\n",
        "    # Filter out the excluded features\n",
        "    filtered_features = [f for f in original_features if f not in excluded_features]\n",
        "    return filtered_features\n",
        "\n",
        "def train_and_evaluate_fold(feature_list, train_data, config, cat_params, le, metric_calculator):\n",
        "    \"\"\"\n",
        "    Train model with given feature list and return OOF score\n",
        "    \"\"\"\n",
        "    sgkf = StratifiedGroupKFold(n_splits=config.FOLDS, shuffle=True, random_state=42)\n",
        "    groups = train_data[config.SUBJECT]\n",
        "\n",
        "    oof_preds_cat = np.zeros(len(train_data), dtype=int)\n",
        "    oof_scores = []\n",
        "\n",
        "    for i, (train_idx, valid_idx) in enumerate(sgkf.split(train_data, train_data[config.TARGET], groups)):\n",
        "\n",
        "        # TRAIN\n",
        "        X_train = train_data.loc[train_idx, feature_list].copy()\n",
        "        y_train = train_data.loc[train_idx, config.TARGET].copy()\n",
        "\n",
        "        # VALID\n",
        "        X_valid = train_data.loc[valid_idx, feature_list].copy()\n",
        "        y_valid = train_data.loc[valid_idx, config.TARGET].copy()\n",
        "\n",
        "        y_valid_original_labels = le.inverse_transform(y_valid)\n",
        "\n",
        "        model = CatBoostClassifier(**cat_params)\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_valid, y_valid)],\n",
        "            use_best_model=True,\n",
        "            verbose=0  # Silent training for ablation study\n",
        "        )\n",
        "\n",
        "        fold_preds_proba = model.predict_proba(X_valid)\n",
        "        fold_valid_preds_labels_encoded = np.argmax(fold_preds_proba, axis=1)\n",
        "        oof_preds_cat[valid_idx] = fold_valid_preds_labels_encoded\n",
        "\n",
        "        fold_valid_preds_original_labels = le.inverse_transform(fold_valid_preds_labels_encoded)\n",
        "\n",
        "        temp_sol_df = pd.DataFrame({\"gesture\": y_valid_original_labels})\n",
        "        temp_sub_df = pd.DataFrame({\"gesture\": fold_valid_preds_original_labels})\n",
        "\n",
        "        fold_score = metric_calculator.calculate_hierarchical_f1(temp_sol_df, temp_sub_df)\n",
        "        oof_scores.append(fold_score)\n",
        "\n",
        "    # Calculate overall OOF score\n",
        "    original_labels = le.inverse_transform(train_data[CONFIG.TARGET])\n",
        "    oof_preds_original_labels = le.inverse_transform(oof_preds_cat)\n",
        "\n",
        "    sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "    sub_df = pd.DataFrame({\"gesture\": oof_preds_original_labels})\n",
        "\n",
        "    overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "\n",
        "    return overall_oof, np.mean(oof_scores), np.std(oof_scores)\n",
        "\n",
        "# # Run feature ablation study\n",
        "# print(\"Starting Feature Ablation Study...\")\n",
        "# print(\"=\"*50)\n",
        "\n",
        "# # Get baseline score (with all features)\n",
        "# print(\"Calculating baseline score with all features...\")\n",
        "# baseline_oof, baseline_mean, baseline_std = train_and_evaluate_fold(\n",
        "#     FEATURES, train, CONFIG, cat_params, le, metric_calculator\n",
        "# )\n",
        "\n",
        "# print(f\"Baseline Overall OOF Score: {baseline_oof:.6f}\")\n",
        "# print(f\"Baseline Mean Fold Score: {baseline_mean:.6f}  {baseline_std:.6f}\")\n",
        "# print(\"=\"*50)\n",
        "\n",
        "# # Results storage\n",
        "results = []\n",
        "# results.append({\n",
        "#     'feature_group': 'BASELINE_ALL_FEATURES',\n",
        "#     'num_features': len(FEATURES),\n",
        "#     'overall_oof': baseline_oof,\n",
        "#     'mean_fold_score': baseline_mean,\n",
        "#     'std_fold_score': baseline_std,\n",
        "#     'score_drop': 0.0,\n",
        "#     'removed_features': 0\n",
        "# })\n",
        "\n",
        "baseline_oof = 0.7491\n",
        "\n",
        "# Test removing each feature group\n",
        "for feature_suffix in feature_groups_to_test:\n",
        "    print(f\"Testing removal of features ending with: {feature_suffix}\")\n",
        "\n",
        "    # Get features excluding this group\n",
        "    filtered_features = get_features_excluding_group(feature_suffix, FEATURES, imu_cols)\n",
        "\n",
        "    # Calculate how many features were removed\n",
        "    removed_count = len(FEATURES) - len(filtered_features)\n",
        "\n",
        "    print(f\"Removed {removed_count} features, using {len(filtered_features)} features\")\n",
        "\n",
        "    # Train and evaluate\n",
        "    overall_oof, mean_score, std_score = train_and_evaluate_fold(\n",
        "        filtered_features, train, CONFIG, cat_params, le, metric_calculator\n",
        "    )\n",
        "\n",
        "    score_drop = baseline_oof - overall_oof\n",
        "\n",
        "    print(f\"Overall OOF Score: {overall_oof:.6f}\")\n",
        "    print(f\"Score Drop: {score_drop:.6f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'feature_group': feature_suffix,\n",
        "        'num_features': len(filtered_features),\n",
        "        'overall_oof': overall_oof,\n",
        "        'mean_fold_score': mean_score,\n",
        "        'std_fold_score': std_score,\n",
        "        'score_drop': score_drop,\n",
        "        'removed_features': removed_count\n",
        "    })\n",
        "\n",
        "# Save results to file\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"feature_ablation_results_{timestamp}.txt\"\n",
        "\n",
        "with open(filename, 'w') as f:\n",
        "    f.write(\"Feature Ablation Study Results\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\")\n",
        "    f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Total Features in Baseline: {len(FEATURES)}\\n\")\n",
        "    f.write(f\"Baseline Overall OOF Score: {baseline_oof:.6f}\\n\\n\")\n",
        "\n",
        "    f.write(\"Results Summary:\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "    f.write(f\"{'Feature Group':<25} {'Num Features':<12} {'OOF Score':<12} {'Score Drop':<12} {'Removed':<8}\\n\")\n",
        "    f.write(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "    for result in results:\n",
        "        f.write(f\"{result['feature_group']:<25} {result['num_features']:<12} \"\n",
        "                f\"{result['overall_oof']:<12.6f} {result['score_drop']:<12.6f} {result['removed_features']:<8}\\n\")\n",
        "\n",
        "    f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    f.write(\"Detailed Results:\\n\\n\")\n",
        "\n",
        "    for result in results:\n",
        "        f.write(f\"Feature Group: {result['feature_group']}\\n\")\n",
        "        f.write(f\"  Number of Features Used: {result['num_features']}\\n\")\n",
        "        f.write(f\"  Features Removed: {result['removed_features']}\\n\")\n",
        "        f.write(f\"  Overall OOF Score: {result['overall_oof']:.6f}\\n\")\n",
        "        f.write(f\"  Mean Fold Score: {result['mean_fold_score']:.6f}  {result['std_fold_score']:.6f}\\n\")\n",
        "        f.write(f\"  Score Drop from Baseline: {result['score_drop']:.6f}\\n\")\n",
        "        if result['score_drop'] > 0:\n",
        "            f.write(f\"  Performance Impact: NEGATIVE (score decreased)\\n\")\n",
        "        elif result['score_drop'] < 0:\n",
        "            f.write(f\"  Performance Impact: POSITIVE (score improved)\\n\")\n",
        "        else:\n",
        "            f.write(f\"  Performance Impact: NEUTRAL\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Create summary DataFrame and display\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('score_drop', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE ABLATION STUDY COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Results saved to: {filename}\")\n",
        "print(\"\\nSummary (sorted by score drop):\")\n",
        "print(results_df[['feature_group', 'overall_oof', 'score_drop', 'removed_features']].to_string(index=False))\n",
        "\n",
        "# Identify most impactful features\n",
        "print(f\"\\nMost impactful feature groups (highest score drop when removed):\")\n",
        "top_impactful = results_df[results_df['feature_group'] != 'BASELINE_ALL_FEATURES'].head(3)\n",
        "for _, row in top_impactful.iterrows():\n",
        "    print(f\"  {row['feature_group']}: -{row['score_drop']:.6f} score drop\")\n",
        "\n",
        "print(f\"\\nLeast impactful feature groups (lowest score drop when removed):\")\n",
        "least_impactful = results_df[results_df['feature_group'] != 'BASELINE_ALL_FEATURES'].tail(3)\n",
        "for _, row in least_impactful.iterrows():\n",
        "    print(f\"  {row['feature_group']}: -{row['score_drop']:.6f} score drop\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksgQ4tPAWWs9"
      },
      "source": [
        "## FEATURE ADDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9CqZdIyVPUA"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "cat_params = {\n",
        "    'objective': 'MultiClass',\n",
        "    'eval_metric': 'MultiClass',\n",
        "    'learning_rate': 0.03,\n",
        "    'depth': 7,\n",
        "    'random_seed': 42,\n",
        "    'iterations': 1000,\n",
        "    'task_type': 'GPU',\n",
        "    # 'reg_lambda': 10,\n",
        "    'verbose': 50,\n",
        "}\n",
        "\n",
        "le = LabelEncoder()\n",
        "train['gesture'] = le.fit_transform(train['gesture'])\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "metric_calculator = CompetitionMetric()\n",
        "\n",
        "\n",
        "feature_suffixes_to_add = [\n",
        "        '_seq_mean',\n",
        "        '_seq_std',\n",
        "        '_seq_min',\n",
        "        '_seq_max',\n",
        "        '_seq_q10',\n",
        "        '_seq_q90',\n",
        "        '_seq_median',\n",
        "        '_seq_iqr',\n",
        "        '_seq_range',\n",
        "\n",
        "        '_avg_velocity',\n",
        "        '_max_velocity',\n",
        "        '_avg_acceleration',\n",
        "        '_peak_count',\n",
        "\n",
        "        '_early_mean',\n",
        "        '_mid_mean',\n",
        "        '_mid2_mean',\n",
        "        '_mid3_mean',\n",
        "        '_late_mean',\n",
        "\n",
        "        '_early_std',\n",
        "        '_mid_std',\n",
        "        '_mid2_std',\n",
        "        '_mid3_std',\n",
        "        '_late_std',\n",
        "\n",
        "        '_early_late_diff',\n",
        "\n",
        "        '_very_late_mean',\n",
        "        '_very_late_std',\n",
        "        '_very_late_max',\n",
        "        '_very_late_min',\n",
        "\n",
        "\n",
        "        '_late_intensity',\n",
        "        '_late_peak_count',\n",
        "        '_late_accel_mean',\n",
        "        '_late_accel_max',\n",
        "        '_late_jerk_mean',\n",
        "        # f'{col}_late_jerk_max', # kt\n",
        "\n",
        "        '_final_velocity',\n",
        "        '_deceleration_pattern',\n",
        "        # f'{col}_completion_smoothness',\n",
        "        '_late_energy',\n",
        "        # f'{col}_termination_ratio', # kt\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhU2UTU-goJH"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from catboost import CatBoostClassifier\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Assumptions: ---\n",
        "# `train`               : pd.DataFrame, with your features + CONFIG.SUBJECT + CONFIG.TARGET\n",
        "# `train_demographics`  : pd.DataFrame, with columns [CONFIG.SUBJECT, 'adult_child', 'handedness', 'sex']\n",
        "# `imu_cols`            : List[str], your raw IMU column names\n",
        "# `feature_suffixes_to_add`: List[str], as defined previously\n",
        "# `CONFIG`              : object with attributes FOLDS (int), SUBJECT (str), TARGET (str), NUM_CLASSES (int)\n",
        "# `cat_params`          : dict, your CatBoost training parameters\n",
        "# `le`                  : fitted LabelEncoder on CONFIG.TARGET\n",
        "# `metric_calculator`   : object with method calculate_hierarchical_f1(sol_df, sub_df)\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 0. Prepare run ID and output directory\n",
        "run_id = uuid.uuid4()\n",
        "models_dir = \"models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Precompute Multilabel Stratified K-Folds on demographics\n",
        "t_d = train_demographics.copy()\n",
        "skf = MultilabelStratifiedKFold(\n",
        "    n_splits=CONFIG.FOLDS,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(\n",
        "    skf.split(\n",
        "        t_d,\n",
        "        t_d[['adult_child', 'handedness', 'sex']]\n",
        "    )\n",
        "):\n",
        "    t_d.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "t_d['fold'] = t_d['fold'].astype(int)\n",
        "print(\"Demographics fold distribution:\\n\", t_d['fold'].value_counts(), \"\\n\")\n",
        "\n",
        "# 2. Merge fold assignments into main train set\n",
        "train = train.merge(\n",
        "    t_d[[CONFIG.SUBJECT, 'fold']],\n",
        "    on=CONFIG.SUBJECT,\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Helper function: trains & evaluates across precomputed folds\n",
        "def train_and_evaluate_fold(\n",
        "    feature_list: list,\n",
        "    train_data: pd.DataFrame,\n",
        "    config,\n",
        "    cat_params: dict,\n",
        "    le,\n",
        "    metric_calculator\n",
        "):\n",
        "    unique_features = sorted(set(feature_list), key=feature_list.index)\n",
        "    oof_preds = np.zeros(len(train_data), dtype=int)\n",
        "\n",
        "    for fold in range(config.FOLDS):\n",
        "        # split by precomputed fold column\n",
        "        train_idx = train_data.index[train_data['fold'] != fold].tolist()\n",
        "        valid_idx = train_data.index[train_data['fold'] == fold].tolist()\n",
        "\n",
        "        X_tr = train_data.loc[train_idx, unique_features].reset_index(drop=True)\n",
        "        y_tr = train_data.loc[train_idx, config.TARGET].reset_index(drop=True)\n",
        "        X_va = train_data.loc[valid_idx, unique_features].reset_index(drop=True)\n",
        "        y_va = train_data.loc[valid_idx, config.TARGET].reset_index(drop=True)\n",
        "\n",
        "        model = CatBoostClassifier(**cat_params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            use_best_model=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # save each fold model (optional)\n",
        "        model_path = os.path.join(\n",
        "            models_dir,\n",
        "            f\"cat_fold_{fold+1}_{run_id}.pkl\"\n",
        "        )\n",
        "        model.save_model(model_path)\n",
        "\n",
        "        proba = model.predict_proba(X_va)\n",
        "        oof_preds[valid_idx] = np.argmax(proba, axis=1)\n",
        "\n",
        "    # invert labels and compute overall OOF score\n",
        "    y_true_orig = le.inverse_transform(train_data[config.TARGET].to_numpy())\n",
        "    y_pred_orig = le.inverse_transform(oof_preds)\n",
        "    sol_df = pd.DataFrame({\"gesture\": y_true_orig})\n",
        "    sub_df = pd.DataFrame({\"gesture\": y_pred_orig})\n",
        "    overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "\n",
        "    return overall_oof, None, None\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Forward Selection Loop\n",
        "SCORE_IMPROVEMENT_THRESHOLD = 0.009\n",
        "\n",
        "print(\"Starting Threshold-based Forward Feature Selection...\")\n",
        "print(f\"Minimum required improvement: {SCORE_IMPROVEMENT_THRESHOLD}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Baseline score with raw IMU features\n",
        "current_features = imu_cols.copy()\n",
        "baseline_oof, _, _ = train_and_evaluate_fold(\n",
        "    current_features, train, CONFIG, cat_params, le, metric_calculator\n",
        ")\n",
        "print(f\"Baseline OOF ({len(current_features)} features): {baseline_oof:.6f}\")\n",
        "previous_oof = baseline_oof\n",
        "\n",
        "# Tracking\n",
        "all_results = []\n",
        "final_selected_features = current_features.copy()\n",
        "\n",
        "# Iteratively test each suffix\n",
        "for suffix in feature_suffixes_to_add:\n",
        "    print(f\"\\n--- Testing feature suffix: '{suffix}' ---\")\n",
        "    if suffix in [\n",
        "        'normalized_position',\n",
        "        'late_acc_rot_coordination',\n",
        "        'late_total_motion',\n",
        "        'late_total_motion_std',\n",
        "        'late_acc_acc_rot_ratio'\n",
        "    ]:\n",
        "        new_feats = [suffix]\n",
        "    else:\n",
        "        new_feats = [f\"{col}{suffix}\" for col in imu_cols]\n",
        "\n",
        "    candidate_features = final_selected_features + new_feats\n",
        "    candidate_features = [\n",
        "        f for f in candidate_features\n",
        "        if f in train.columns\n",
        "    ]\n",
        "\n",
        "    new_oof, _, _ = train_and_evaluate_fold(\n",
        "        candidate_features, train, CONFIG, cat_params, le, metric_calculator\n",
        "    )\n",
        "    gain = new_oof - previous_oof\n",
        "    added = len(candidate_features) - len(final_selected_features)\n",
        "\n",
        "    print(\n",
        "        f\"Tested {len(candidate_features)} features  \"\n",
        "        f\"Prev OOF: {previous_oof:.6f}, New OOF: {new_oof:.6f}, \"\n",
        "        f\"Gain: {gain:+.6f}\"\n",
        "    )\n",
        "\n",
        "    if gain > SCORE_IMPROVEMENT_THRESHOLD:\n",
        "        decision = \"KEPT\"\n",
        "        final_selected_features = candidate_features\n",
        "        previous_oof = new_oof\n",
        "        print(f\" KEEP '{suffix}' (gain {gain:.6f})\")\n",
        "    else:\n",
        "        decision = \"DISCARDED\"\n",
        "        print(f\" DISCARD '{suffix}' (gain {gain:.6f})\")\n",
        "\n",
        "    all_results.append({\n",
        "        'suffix': suffix,\n",
        "        'decision': decision,\n",
        "        'gain': gain,\n",
        "        'new_oof': new_oof,\n",
        "        'features_added': added\n",
        "    })\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Summary & Save\n",
        "results_df = pd.DataFrame(all_results)\n",
        "kept = results_df[results_df.decision == \"KEPT\"]\n",
        "discarded = results_df[results_df.decision == \"DISCARDED\"]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Forward Selection Complete\")\n",
        "print(f\"Final OOF Score: {previous_oof:.6f}\")\n",
        "print(f\"Total features selected: {len(final_selected_features)}\\n\")\n",
        "\n",
        "print(\"Kept suffixes:\")\n",
        "print(kept[['suffix','gain']].to_string(index=False) if not kept.empty else \"  None\")\n",
        "\n",
        "print(\"\\nDiscarded suffixes:\")\n",
        "print(discarded[['suffix','gain']].to_string(index=False) if not discarded.empty else \"  None\")\n",
        "\n",
        "# Save final feature list\n",
        "with open('final_feature_list.txt', 'w') as f:\n",
        "    for feat in final_selected_features:\n",
        "        f.write(feat + \"\\n\")\n",
        "print(\"\\nSaved final feature list to 'final_feature_list.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdec984wgoG5"
      },
      "outputs": [],
      "source": [
        "# avg_acceleration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryf_jecZgoEX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJorgJyx5vEp"
      },
      "outputs": [],
      "source": [
        "feat_imp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYsY6tR6_5Me"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WI0x997JyzZm",
        "KJYVNzGqBS63",
        "YQg5lnUubyRZ"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e33772371384c5a94ea080fcac7bd6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa38b9a8fa2749bea474c97b395e3657",
              "IPY_MODEL_c602d20290cb47948e9bb4ef32b1dd94",
              "IPY_MODEL_48090fab89be459890e851cb18d2a0fe",
              "IPY_MODEL_d494bffc569747708e63197f71e45d2e",
              "IPY_MODEL_5d64969ed4954d278d7b3abdcac196f5"
            ],
            "layout": "IPY_MODEL_ea86cc6d22524aba8334d6e1dbc350e7"
          }
        },
        "3efe2783c5c54a3a96086febb10c1ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48090fab89be459890e851cb18d2a0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_61544e61a6084a74ace8c9edbbb5ee06",
            "placeholder": "",
            "style": "IPY_MODEL_8cb062c5ea3642d7965e6495ff2dde3d",
            "value": ""
          }
        },
        "5d64969ed4954d278d7b3abdcac196f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3c9f57f9bda47c592f7a9c126897b07",
            "placeholder": "",
            "style": "IPY_MODEL_990250e2c3c4463db96cf61f9d718ab6",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "61544e61a6084a74ace8c9edbbb5ee06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68b1ceb7439d4660b0881734c8576e63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7136bbe018af4c6aa15ea70d8375566d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb062c5ea3642d7965e6495ff2dde3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9285236320d44b3f9c82a8a173e7fbf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "990250e2c3c4463db96cf61f9d718ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e62f6d73aa746ad9d1ed267bd8e3dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a3c9f57f9bda47c592f7a9c126897b07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd4a123d22149308d2e93d7384d4688": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c602d20290cb47948e9bb4ef32b1dd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7136bbe018af4c6aa15ea70d8375566d",
            "placeholder": "",
            "style": "IPY_MODEL_3efe2783c5c54a3a96086febb10c1ddf",
            "value": ""
          }
        },
        "d494bffc569747708e63197f71e45d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_68b1ceb7439d4660b0881734c8576e63",
            "style": "IPY_MODEL_9e62f6d73aa746ad9d1ed267bd8e3dda",
            "tooltip": ""
          }
        },
        "ea86cc6d22524aba8334d6e1dbc350e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fa38b9a8fa2749bea474c97b395e3657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9285236320d44b3f9c82a8a173e7fbf3",
            "placeholder": "",
            "style": "IPY_MODEL_acd4a123d22149308d2e93d7384d4688",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
