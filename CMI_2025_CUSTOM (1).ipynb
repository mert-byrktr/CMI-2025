{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHnAaP-MW4qA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPH1bbFZz7V",
        "outputId": "6f0cccf9-3da0-4629-a3d3-cd06fe6f3e39"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "3ec87fb5da7b483695ab416d5604e781",
            "e370dfa97ff54278bed31a65d1914143",
            "ae57f0fc0658442eb063b38fe1df31e3",
            "7b60751304b446b28f244d0ecb2b8201",
            "e8e959e315024639878963dba3b9fa50",
            "6813aea3f91e43eeb8a7a62e8808d25a",
            "d3cfa2bfd304499d8686858f84ae31f9",
            "3aa75bca288242b594b125867a468e65",
            "c61a599bcfb0426b8edb40aae7338948",
            "7b66850a912449398d5c52d820aa4e2c",
            "2e9ddbf05105439e9427027ee6ce0d65",
            "34f6f6615a664528b372c4e395ad3146",
            "029f13b4f7a441a5ac38b97b11f66e59",
            "c3e05620b2194a5a83ffc1cb176d72d0",
            "32ddb1a9862e464bb6688fe08d9d2090",
            "87428f8be1c945539482aed50461d496",
            "113f8ca2d8ea45cda08293ee4af3dc20"
          ]
        },
        "id": "ka_oHwl5_5MN",
        "outputId": "bfde17bb-3a70-4b9e-e99d-589de88eff20"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_g6C36S_5MR",
        "outputId": "21bb37f3-c2bc-4d51-c390-077d572c2164"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "cmi_detect_behavior_with_sensor_data_path = kagglehub.competition_download('cmi-detect-behavior-with-sensor-data')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7jDg6tfTvdp"
      },
      "outputs": [],
      "source": [
        "!pip install iterative-stratification==0.1.7 -qq\n",
        "!pip install polars==1.21.0 -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rYRqCy_5MT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import polars as pl\n",
        "import sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 2000)\n",
        "pd.set_option('display.max_rows', 2000)\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8MqxrueedN6",
        "outputId": "82a34810-50db-4a4c-bc83-dd95e47b94bd"
      },
      "outputs": [],
      "source": [
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(pl.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(joblib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "gY6LM36r_5MW",
        "outputId": "39f871cc-4d73-4246-e546-4ffc50637bb2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "train = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train.csv')\n",
        "test = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test.csv')\n",
        "train_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train_demographics.csv')\n",
        "test_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test_demographics.csv')\n",
        "\n",
        "train = train.to_pandas()\n",
        "test = test.to_pandas()\n",
        "train_demographics = train_demographics.to_pandas()\n",
        "test_demographics = test_demographics.to_pandas()\n",
        "\n",
        "train = pd.merge(train, train_demographics, on='subject', how='left')\n",
        "test = pd.merge(test, test_demographics, on='subject', how='left')\n",
        "\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDn9xqTCegC0"
      },
      "outputs": [],
      "source": [
        "# # Calculate average y acceleration per subject\n",
        "# def find_wrong_side_subjects(df):\n",
        "#     \"\"\"Find subjects who wore device on wrong wrist (negative avg y acceleration)\"\"\"\n",
        "\n",
        "#     subject_acc_y_stats = df.groupby('subject')['rot_x'].agg([\n",
        "#         'mean', 'std', 'count'\n",
        "#     ]).reset_index()\n",
        "\n",
        "#     subject_acc_y_stats.columns = ['subject', 'thm_3_mean', 'acc_z_std', 'sample_count']\n",
        "\n",
        "#     print(\"Y Acceleration Statistics by Subject:\")\n",
        "#     print(subject_acc_y_stats.sort_values('thm_3_mean'))\n",
        "\n",
        "#     # Identify subjects with negative average y acceleration\n",
        "#     wrong_side_subjects = subject_acc_y_stats[subject_acc_y_stats['thm_3_mean'] < 0]\n",
        "#     normal_subjects = subject_acc_y_stats[subject_acc_y_stats['thm_3_mean'] > 0]\n",
        "\n",
        "#     print(f\"\\nSubjects with NEGATIVE average y acceleration (wrong side): {len(wrong_side_subjects)}\")\n",
        "#     print(wrong_side_subjects[['subject', 'thm_3_mean']])\n",
        "\n",
        "#     print(f\"\\nSubjects with POSITIVE average y acceleration (correct side): {len(normal_subjects)}\")\n",
        "\n",
        "#     return wrong_side_subjects['subject'].tolist(), normal_subjects['subject'].tolist()\n",
        "\n",
        "# # Find the problematic subjects\n",
        "# wrong_side_subjects, normal_subjects = find_wrong_side_subjects(train)\n",
        "\n",
        "# # Detailed analysis of the wrong-side subjects\n",
        "# if wrong_side_subjects:\n",
        "#     print(f\"\\nDetailed analysis of wrong-side subjects:\")\n",
        "#     for subject in wrong_side_subjects:\n",
        "#         subject_data = train[train['subject'] == subject]\n",
        "#         sequences = subject_data['sequence_id'].nunique()\n",
        "#         gestures = subject_data['gesture'].nunique() if 'gesture' in subject_data.columns else 'Unknown'\n",
        "\n",
        "#         print(f\"\\nSubject: {subject}\")\n",
        "#         print(f\"  Sequences: {sequences}\")\n",
        "#         print(f\"  Unique gestures: {gestures}\")\n",
        "#         print(f\"  Avg acc_y: {subject_data['acc_y'].mean():.4f}\")\n",
        "#         print(f\"  Avg acc_x: {subject_data['acc_x'].mean():.4f}\")\n",
        "#         print(f\"  Avg acc_z: {subject_data['acc_z'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0ITj3X7UMz_",
        "outputId": "4fde7875-1876-4359-f4a4-2462851d5a35"
      },
      "outputs": [],
      "source": [
        "def correct_wrong_side_subjects(df):\n",
        "    \"\"\"Correct systematic measurement error from wrong-side device placement\"\"\"\n",
        "\n",
        "    subject_acc_y = df.groupby('subject')['acc_y'].mean()\n",
        "    wrong_side_subjects = subject_acc_y[subject_acc_y < 0].index.tolist()\n",
        "\n",
        "    print(f\"Wrong-side subjects identified: {wrong_side_subjects}\")\n",
        "    print(f\"Original avg acc_y: {df[df['subject'].isin(wrong_side_subjects)]['acc_y'].mean():.4f}\")\n",
        "\n",
        "    df_corrected = df.copy()\n",
        "    wrong_side_mask = df_corrected['subject'].isin(wrong_side_subjects)\n",
        "    df_corrected.loc[wrong_side_mask, 'acc_y'] = -df_corrected.loc[wrong_side_mask, 'acc_y']\n",
        "    df_corrected.loc[wrong_side_mask, 'acc_x'] = -df_corrected.loc[wrong_side_mask, 'acc_x']\n",
        "\n",
        "\n",
        "    print(f\"Corrected avg acc_y: {df_corrected[df_corrected['subject'].isin(wrong_side_subjects)]['acc_y'].mean():.4f}\")\n",
        "\n",
        "    return df_corrected, wrong_side_subjects\n",
        "\n",
        "train, wrong_side_subjects = correct_wrong_side_subjects(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNq1yWH1MU9s"
      },
      "outputs": [],
      "source": [
        "# # Check for missing quaternion data\n",
        "# def find_missing_quaternion_sequences(df):\n",
        "#     \"\"\"Find sequences with significant missing quaternion data\"\"\"\n",
        "\n",
        "#     rot_cols = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
        "\n",
        "#     # Calculate missing percentage per sequence\n",
        "#     seq_missing_rot = []\n",
        "\n",
        "#     for seq_id in df['sequence_id'].unique():\n",
        "#         seq_data = df[df['sequence_id'] == seq_id]\n",
        "\n",
        "#         # Count missing values across all rot columns\n",
        "#         total_rot_values = len(seq_data) * len(rot_cols)\n",
        "#         missing_rot_values = seq_data[rot_cols].isnull().sum().sum()\n",
        "\n",
        "#         missing_pct = (missing_rot_values / total_rot_values) * 100\n",
        "\n",
        "#         seq_missing_rot.append({\n",
        "#             'sequence_id': seq_id,\n",
        "#             'total_timesteps': len(seq_data),\n",
        "#             'missing_rot_values': missing_rot_values,\n",
        "#             'missing_percentage': missing_pct,\n",
        "#             'subject': seq_data['subject'].iloc[0],\n",
        "#             'gesture': seq_data['gesture'].iloc[0] if 'gesture' in seq_data.columns else None\n",
        "#         })\n",
        "\n",
        "#     # Convert to DataFrame for analysis\n",
        "#     missing_df = pd.DataFrame(seq_missing_rot)\n",
        "\n",
        "#     return missing_df\n",
        "\n",
        "# # Find missing quaternion sequences\n",
        "# missing_rot_analysis = find_missing_quaternion_sequences(train)\n",
        "\n",
        "# # Filter sequences with significant missing rot data (>50% missing)\n",
        "# problematic_rot_sequences = missing_rot_analysis[missing_rot_analysis['missing_percentage'] > 50]\n",
        "\n",
        "# print(f\"Total sequences analyzed: {len(missing_rot_analysis)}\")\n",
        "# print(f\"Sequences with >50% missing rot data: {len(problematic_rot_sequences)}\")\n",
        "# print(f\"Sequences with >90% missing rot data: {len(missing_rot_analysis[missing_rot_analysis['missing_percentage'] > 90])}\")\n",
        "# print()\n",
        "\n",
        "# # Show the most problematic sequences\n",
        "# print(\"Top 10 sequences with most missing rot data:\")\n",
        "# print(problematic_rot_sequences.nlargest(50, 'missing_percentage')[['sequence_id', 'subject', 'gesture', 'missing_percentage', 'total_timesteps']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k07J4kCUODxr"
      },
      "outputs": [],
      "source": [
        "# # # Show the most problematic sequences\n",
        "# # print(\"Top 10 sequences with most missing rot data:\")\n",
        "# # print(problematic_rot_sequences.nlargest(200, 'missing_percentage')[['sequence_id', 'subject', 'gesture', 'missing_percentage', 'total_timesteps']])\n",
        "# missing_rot_sequences = problematic_rot_sequences['sequence_id'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ic-BSLy1d3"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaWmMuIS_5MZ"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "  TARGET = \"gesture\"\n",
        "  SUBJECT = \"subject\"\n",
        "  TRAIN_ONLY_COLS = ['sequence_type', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
        "  NUM_CLASSES = train.gesture.nunique()\n",
        "  FOLDS = 5\n",
        "  ERR = 1e-8\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "imu_cols = [\n",
        "            \"acc_x\", \"acc_y\", \"acc_z\",\n",
        "            \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
        "            \"acc_mag\",\n",
        "\n",
        "            \"euler_roll\", \"euler_pitch\", \"euler_yaw\",\n",
        "            \"euler_total\", \"pitch_roll_ratio\", \"yaw_pitch_ratio\",\n",
        "\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "\n",
        "            \"angular_jerk_x\", \"angular_jerk_y\", \"angular_jerk_z\",\n",
        "            ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7nJrmq5sdv"
      },
      "source": [
        "## FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D3m98J9LXEw",
        "outputId": "0549a641-8a7c-4764-c36b-0137c874e6a0"
      },
      "outputs": [],
      "source": [
        "print(train.subject.nunique())\n",
        "print(train.sequence_id.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXkapfIo9Pew"
      },
      "outputs": [],
      "source": [
        "train = train[train['subject'] != 'SUBJ_011323']\n",
        "# # # train = train[train['subject'] != 'SUBJ_045235']\n",
        "# # # train = train[train['subject'] != 'SUBJ_019262']\n",
        "train = train[train['sequence_id'] != 'SEQ_011975']\n",
        "\n",
        "# # for seq_id in missing_rot_sequences:\n",
        "# #   train = train[train['sequence_id'] != seq_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7acPAQBDLhrC",
        "outputId": "3016f3bb-2d94-4814-95a8-72f17f4aeaab"
      },
      "outputs": [],
      "source": [
        "print(train.subject.nunique())\n",
        "print(train.sequence_id.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTDoFvXb_5Ma",
        "outputId": "700e5956-3e98-405a-e6f3-f7eecfaa1bf2"
      },
      "outputs": [],
      "source": [
        "def cast_to_object(df):\n",
        "  df['adult_child'] = df['adult_child'].astype(\"category\")\n",
        "  df['sex'] = df['sex'].astype(\"category\")\n",
        "  df['handedness'] = df['handedness'].astype(\"category\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "\n",
        "    if isinstance(acc_data, pd.DataFrame):\n",
        "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    else:\n",
        "        acc_values = acc_data\n",
        "\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = acc_values.shape[0]\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "\n",
        "    gravity_world = np.array([0, 0, 9.81])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "             linear_accel[i, :] = acc_values[i, :]\n",
        "\n",
        "    return linear_accel\n",
        "\n",
        "\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_vel = np.zeros((num_samples, 3))\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q_t = quat_values[i]\n",
        "        q_t_plus_dt = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    return angular_vel\n",
        "\n",
        "def calculate_angular_distance(rot_data):\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_dist = np.zeros(num_samples)\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q1 = quat_values[i]\n",
        "        q2 = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
        "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
        "            angular_dist[i] = 0 \n",
        "            continue\n",
        "        try:\n",
        "            r1 = R.from_quat(q1)\n",
        "            r2 = R.from_quat(q2)\n",
        "\n",
        "            relative_rotation = r1.inv() * r2\n",
        "\n",
        "\n",
        "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
        "            angular_dist[i] = angle\n",
        "        except ValueError:\n",
        "            angular_dist[i] = 0 \n",
        "            pass\n",
        "\n",
        "    return angular_dist\n",
        "\n",
        "def calc_angular_velocity(df):\n",
        "    res = calculate_angular_velocity_from_quat( df[['rot_x', 'rot_y', 'rot_z', 'rot_w']] )\n",
        "    res = pd.DataFrame(res, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=df.index)\n",
        "    return res\n",
        "\n",
        "angular_velocity_df = train.groupby('sequence_id').apply(calc_angular_velocity, include_groups=False)\n",
        "angular_velocity_df = angular_velocity_df.droplevel('sequence_id')\n",
        "train = train.join(angular_velocity_df)\n",
        "\n",
        "def calc_angular_distance(df):\n",
        "    res = calculate_angular_distance(df[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
        "    res = pd.DataFrame(res, columns=['angular_distance'], index=df.index)\n",
        "    return res\n",
        "\n",
        "angular_distance_df = train.groupby('sequence_id').apply(calc_angular_distance, include_groups=False)\n",
        "angular_distance_df = angular_distance_df.droplevel('sequence_id')\n",
        "train = train.join(angular_distance_df)\n",
        "\n",
        "def quaternion_to_euler(w, x, y, z):\n",
        "    \"\"\"Convert quaternion to Euler angles\"\"\"\n",
        "    sinr_cosp = 2 * (w * x + y * z)\n",
        "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
        "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
        "\n",
        "    sinp = 2 * (w * y - z * x)\n",
        "    pitch = np.where(np.abs(sinp) >= 1, np.copysign(np.pi / 2, sinp), np.arcsin(sinp))\n",
        "\n",
        "    siny_cosp = 2 * (w * z + x * y)\n",
        "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "\n",
        "    return roll, pitch, yaw\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mag_features(df):\n",
        "\n",
        "    df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"]**2 + df[\"acc_y\"]**2 + df[\"acc_z\"]**2)\n",
        "    df[\"rot_mag\"] = np.sqrt(df[\"rot_x\"]**2 + df[\"rot_y\"]**2 + df[\"rot_z\"]**2)\n",
        "\n",
        "    linear_acc = remove_gravity_from_acc(\n",
        "            df[['acc_x', 'acc_y', 'acc_z']],\n",
        "            df[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "        )\n",
        "    df['linear_acc_x'] = linear_acc[:, 0]\n",
        "    df['linear_acc_y'] = linear_acc[:, 1]\n",
        "    df['linear_acc_z'] = linear_acc[:, 2]\n",
        "\n",
        "    if 'linear_acc_x' in df.columns:\n",
        "        df[\"linear_acc_mag\"] = np.sqrt(df[\"linear_acc_x\"]**2 + df[\"linear_acc_y\"]**2 + df[\"linear_acc_z\"]**2)\n",
        "\n",
        "    if 'angular_vel_x' in df.columns:\n",
        "        df[\"angular_vel_mag\"] = np.sqrt(df[\"angular_vel_x\"]**2 + df[\"angular_vel_y\"]**2 + df[\"angular_vel_z\"]**2)\n",
        "\n",
        "    roll, pitch, yaw = quaternion_to_euler(df[\"rot_w\"], df[\"rot_x\"], df[\"rot_y\"], df[\"rot_z\"])\n",
        "    df[\"euler_roll\"] = roll\n",
        "    df[\"euler_pitch\"] = pitch\n",
        "    df[\"euler_yaw\"] = yaw\n",
        "    df[\"euler_mag\"] = np.sqrt(roll**2 + pitch**2 + yaw**2)\n",
        "\n",
        "    df['acc_angular_sync'] = df['acc_mag'] * df['angular_vel_mag']  # Hand-wrist coordination\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def temporal_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "    df = df.with_columns([\n",
        "        ((pl.col('sequence_counter') - pl.col('sequence_counter').min().over('sequence_id')) /\n",
        "         (pl.col('sequence_counter').max().over('sequence_id') - pl.col('sequence_counter').min().over('sequence_id') + CONFIG.ERR))\n",
        "        .alias('normalized_position')\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        (pl.col('normalized_position') * 2 * np.pi).sin().alias('position_sin'),\n",
        "        (pl.col('normalized_position') * 2 * np.pi).cos().alias('position_cos'),\n",
        "        (pl.col('normalized_position') * 4 * np.pi).sin().alias('position_sin_2h'),  # Second harmonic\n",
        "        (pl.col('normalized_position') * 4 * np.pi).cos().alias('position_cos_2h'),\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "    # Core IMU + derived columns\n",
        "    core_cols = [\"acc_x\", \"acc_y\", \"acc_z\", \"acc_mag\", \"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\", \"rot_mag\"]\n",
        "    if 'linear_acc_x' in df.columns:\n",
        "        core_cols.extend([\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\", \"linear_acc_mag\"])\n",
        "    if 'angular_vel_x' in df.columns:\n",
        "        core_cols.extend([\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\", \"angular_vel_mag\"])\n",
        "    if 'euler_roll' in df.columns:\n",
        "        core_cols.extend([\"euler_roll\", \"euler_pitch\", \"euler_yaw\", \"euler_mag\"])\n",
        "\n",
        "\n",
        "    core_cols.append('acc_angular_sync')\n",
        "    core_cols.extend([\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "    ])\n",
        "\n",
        "    for col in core_cols:\n",
        "        temporal_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').alias(f'{col}_vel'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(temporal_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "def rolling_window_features(df, windows=[3, 5, 10]):\n",
        "    \"\"\"Add rolling window features that CNNs can learn from\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    rolling_exprs = []\n",
        "\n",
        "    key_cols = [\"acc_mag\", \"acc_x\", \"acc_y\", \"acc_z\", \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\", \"acc_mag_vel\", \"acc_x_vel\", \"acc_y_vel\", \"acc_z_vel\",\n",
        "                \"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\", \"angular_vel_mag_vel\", \"angular_vel_x_vel\", \"angular_vel_y_vel\",\n",
        "                \"angular_vel_z_vel\",\n",
        "                \"angular_vel_distance\", \"angular_vel_mag\",\n",
        "                \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "                \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "                \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\"]\n",
        "\n",
        "    for window in windows:\n",
        "        for col in key_cols:\n",
        "            if col in df.columns:\n",
        "                rolling_exprs.extend([\n",
        "                    pl.col(col).rolling_mean(window).over('sequence_id').alias(f'{col}_roll_mean_{window}'),\n",
        "                    pl.col(col).rolling_std(window).over('sequence_id').alias(f'{col}_roll_std_{window}'),\n",
        "                    pl.col(col).rolling_max(window).over('sequence_id').alias(f'{col}_roll_max_{window}'),\n",
        "                    pl.col(col).rolling_min(window).over('sequence_id').alias(f'{col}_roll_min_{window}'),\n",
        "                    pl.col(col).rolling_sum(window).over('sequence_id').alias(f'{col}_roll_sum_{window}'),\n",
        "\n",
        "                ])\n",
        "\n",
        "    df = df.with_columns(rolling_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def rotation_matrix_features(df):\n",
        "    \"\"\"Extract features from rotation matrices - captures 3D orientation relationships\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "\n",
        "    rot_matrix_exprs = []\n",
        "\n",
        "    rot_matrix_exprs.extend([\n",
        "        (1 - 2*(pl.col('rot_y')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r11'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') - pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r12'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r13'),\n",
        "\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') + pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r21'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r22'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r23'),\n",
        "\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r31'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r32'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_y')**2)).alias('rot_matrix_r33'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(rot_matrix_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def apply_feature_engineering(df):\n",
        "    print(\"  Applying feature engineering...\")\n",
        "    df = cast_to_object(df)\n",
        "    df = mag_features(df)\n",
        "\n",
        "    df = rotation_matrix_features(df)\n",
        "\n",
        "    df = temporal_features(df)\n",
        "    df = rolling_window_features(df)\n",
        "    print(\"  Feature engineering complete.\")\n",
        "    return df\n",
        "\n",
        "def handle_missing_quaternions(df):\n",
        "    \"\"\"Handle missing quaternion data smartly\"\"\"\n",
        "    rot_cols = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
        "\n",
        "    print(\"Handling missing quaternion data...\")\n",
        "    sequences_fixed = 0\n",
        "\n",
        "    for seq_id in df['sequence_id'].unique():\n",
        "        seq_mask = df['sequence_id'] == seq_id\n",
        "        seq_data = df.loc[seq_mask]\n",
        "\n",
        "        if seq_data[rot_cols].isnull().all().all():\n",
        "            df.loc[seq_mask, 'rot_w'] = 0.0  # Identity quaternion\n",
        "            df.loc[seq_mask, 'rot_x'] = 0.0\n",
        "            df.loc[seq_mask, 'rot_y'] = 0.0\n",
        "            df.loc[seq_mask, 'rot_z'] = 0.0\n",
        "\n",
        "            df.loc[seq_mask, 'rot_missing_flag'] = 1\n",
        "            sequences_fixed += 1\n",
        "        else:\n",
        "            df.loc[seq_mask, 'rot_missing_flag'] = 0\n",
        "\n",
        "    print(f\"Fixed {sequences_fixed} sequences with missing quaternion data\")\n",
        "    return df\n",
        "\n",
        "# train = handle_missing_quaternions(train)\n",
        "train = apply_feature_engineering(train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ul16-2QYoT"
      },
      "source": [
        "## TOF THM FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX524iTnxA5r"
      },
      "outputs": [],
      "source": [
        "thm_cols = [\n",
        "    \"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\",\n",
        "\n",
        "    # \"thm_12_diff\", \"thm_13_diff\", \"thm_14_diff\",\n",
        "    # \"thm_15_diff\", \"thm_23_diff\", \"thm_24_diff\", \"thm_25_diff\",\n",
        "    # \"thm_34_diff\", \"thm_35_diff\", \"thm_45_diff\",\n",
        "\n",
        "]\n",
        "\n",
        "tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
        "\n",
        "tof_diff_cols = [f\"tof_{i}{j}_mean_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]\n",
        "tof_diff_cols += [f\"tof_{i}{j}_std_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRYrkG8sbfb7"
      },
      "outputs": [],
      "source": [
        "# train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-IITROb8keh"
      },
      "outputs": [],
      "source": [
        "def thm_features_func(df):\n",
        "    \"\"\"Extract features from thermopile sensors\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    thm_sensor_exprs = []\n",
        "    for col in [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]:\n",
        "        if col in df.columns:\n",
        "            thm_sensor_exprs.extend([\n",
        "                pl.col(col).diff().over('sequence_id').alias(f'{col}_vel'),\n",
        "                pl.col(col).diff().diff().over('sequence_id').alias(f'{col}_accel'),\n",
        "                (pl.col(col) - pl.col(col).mean().over('sequence_id')).alias(f'{col}_relative'),\n",
        "            ])\n",
        "\n",
        "    df = df.with_columns(thm_sensor_exprs)\n",
        "\n",
        "    thm_sensor_exprs_2 = []\n",
        "    thm_sensor_exprs_2.extend([\n",
        "        ((pl.col('thm_1') + pl.col('thm_2') + pl.col('thm_3') + pl.col('thm_4')) / (pl.col('thm_5') + 1e-8)).alias('thm_side_ratio'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(thm_sensor_exprs_2)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "from scipy import ndimage, signal\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def advanced_tof_features(df):\n",
        "    \"\"\"Advanced time-of-flight feature engineering\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_advanced_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        tof_advanced_exprs.extend([\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[:16]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[:16]])).alias(f'tof_{sensor_idx}_contrast_q1'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[16:32]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[16:32]])).alias(f'tof_{sensor_idx}_contrast_q2'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[32:48]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[32:48]])).alias(f'tof_{sensor_idx}_contrast_q3'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[48:64]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[48:64]])).alias(f'tof_{sensor_idx}_contrast_q4'),\n",
        "\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "            .list.var().alias(f'tof_{sensor_idx}_uniformity'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(tof_advanced_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def tof_features_func(df):\n",
        "    \"\"\"Extract features from time-of-flight sensors (proximity/distance)\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_sensor_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        tof_sensor_exprs.extend([\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "              .list.mean().alias(f'tof_{sensor_idx}_mean_distance'),\n",
        "\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "              .list.std().alias(f'tof_{sensor_idx}_std_distance'),\n",
        "        ])\n",
        "\n",
        "        tof_sensor_exprs.extend([\n",
        "            pl.sum_horizontal([pl.when(pl.col(col) != -1).then(1).otherwise(0) for col in pixel_cols])\n",
        "              .alias(f'tof_{sensor_idx}_active_pixels'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "\n",
        "def tof_regional_features_func(df, tof_mode=\"stats\", include_regions=True):\n",
        "    \"\"\"\n",
        "    Extract features from time-of-flight sensors with regional analysis\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with ToF data\n",
        "        tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "        include_regions: Whether to include regional analysis features\n",
        "    \"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_sensor_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        # # Basic stats (replace -1 with null for proper statistics)\n",
        "        # tof_sensor_exprs.extend([\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.mean().alias(f'tof_{sensor_idx}_mean'),\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.std().alias(f'tof_{sensor_idx}_std'),\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.min().alias(f'tof_{sensor_idx}_min'),\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.max().alias(f'tof_{sensor_idx}_max'),\n",
        "        # ])\n",
        "\n",
        "        if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "            region_modes = [4] if tof_mode == \"regions\" else [4]\n",
        "\n",
        "            for mode in region_modes:\n",
        "                region_size = 64 // mode  \n",
        "\n",
        "                for region_idx in range(mode):\n",
        "                    start_pixel = region_idx * region_size\n",
        "                    end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "                    region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "\n",
        "                    tof_sensor_exprs.extend([\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.mean().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_mean'),\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.std().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_std'),\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.min().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_min'),\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.max().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_max'),\n",
        "\n",
        "                    ])\n",
        "\n",
        "    df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "\n",
        "def tof_center_of_mass_features(df):\n",
        "    \"\"\"Fast vectorized center of mass calculation\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_com_exprs = []\n",
        "    tof_weight_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        x_coords = [i % 8 for i in range(64)]  \n",
        "        y_coords = [i // 8 for i in range(64)]\n",
        "\n",
        "        tof_weight_exprs.extend([\n",
        "            pl.sum_horizontal([\n",
        "                pl.when(pl.col(col) != -1).then(pl.col(col)).otherwise(0)\n",
        "                for col in pixel_cols\n",
        "            ]).alias(f'tof_{sensor_idx}_total_weight'),\n",
        "\n",
        "        ])\n",
        "\n",
        "        df = df.with_columns(tof_weight_exprs)\n",
        "\n",
        "        tof_com_exprs.extend([\n",
        "            (pl.sum_horizontal([\n",
        "                pl.when(pl.col(pixel_cols[i]) != -1)\n",
        "                .then(pl.col(pixel_cols[i]) * x_coords[i])\n",
        "                .otherwise(0)\n",
        "                for i in range(64)\n",
        "            ]) / (pl.col(f'tof_{sensor_idx}_total_weight') + 1e-8)).alias(f'tof_{sensor_idx}_com_x'),\n",
        "\n",
        "            (pl.sum_horizontal([\n",
        "                pl.when(pl.col(pixel_cols[i]) != -1)\n",
        "                .then(pl.col(pixel_cols[i]) * y_coords[i])\n",
        "                .otherwise(0)\n",
        "                for i in range(64)\n",
        "            ]) / (pl.col(f'tof_{sensor_idx}_total_weight') + 1e-8)).alias(f'tof_{sensor_idx}_com_y'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(tof_com_exprs)\n",
        "\n",
        "    derived_exprs = []\n",
        "    for sensor_idx in range(1, 6):\n",
        "        derived_exprs.extend([\n",
        "            # Distance from center (3.5, 3.5)\n",
        "            ((pl.col(f'tof_{sensor_idx}_com_x') - 3.5)**2 +\n",
        "             (pl.col(f'tof_{sensor_idx}_com_y') - 3.5)**2).sqrt()\n",
        "            .alias(f'tof_{sensor_idx}_center_distance'),\n",
        "\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(derived_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "train = thm_features_func(train)\n",
        "\n",
        "train = tof_features_func(train)\n",
        "train = tof_regional_features_func(train, tof_mode=\"regions\")\n",
        "train = advanced_tof_features(train)\n",
        "train = tof_center_of_mass_features(train)\n",
        "\n",
        "\n",
        "\n",
        "# thm_feature_cols = [col for col in train.columns if 'thm_' in col and col not in thm_cols]\n",
        "# # tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "# tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "\n",
        "\n",
        "# FEATURES_FULL = FEATURES + thm_feature_cols + tof_feature_cols\n",
        "# print(len(FEATURES_FULL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJuAJBGWQWSk"
      },
      "source": [
        "## REDUCE MEMORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOxRa7Mt_5Mb"
      },
      "outputs": [],
      "source": [
        "# def reduce_mem_usage(df, verbose=True):\n",
        "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2 # calculate current memory usage\n",
        "\n",
        "#     for col in df.columns:\n",
        "#         col_type = df[col].dtype\n",
        "#         if col_type in numerics: # check if column is numeric\n",
        "#             c_min = df[col].min()\n",
        "#             c_max = df[col].max()\n",
        "#             if str(col_type).startswith('int'): # if integer\n",
        "#                 # Check if data can be safely cast to smaller int types\n",
        "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "#                     df[col] = df[col].astype(np.int8)\n",
        "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "#                     df[col] = df[col].astype(np.int16)\n",
        "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "#                     df[col] = df[col].astype(np.int32)\n",
        "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "#                     df[col] = df[col].astype(np.int64) # Should already be this or smaller if loaded as int\n",
        "#             else: # if float\n",
        "#                 # Check if data can be safely cast to float32 (float16 often loses too much precision)\n",
        "#                 if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "#                     df[col] = df[col].astype(np.float32)\n",
        "#                 # else: # If not, keep as float64\n",
        "#                 #     df[col] = df[col].astype(np.float64) # Already this type\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     if verbose:\n",
        "#         print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "#     return df\n",
        "\n",
        "# print(\"Reducing memory for train_df:\")\n",
        "# train = reduce_mem_usage(train)\n",
        "# print(\"\\nReducing memory for test_df:\")\n",
        "# test = reduce_mem_usage(test)\n",
        "\n",
        "# print(\"\\nTrain DataFrame info after memory reduction:\")\n",
        "# train.info(memory_usage='deep')\n",
        "# print(\"\\nTest DataFrame info after memory reduction:\")\n",
        "# test.info(memory_usage='deep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO30PQozOlbg"
      },
      "outputs": [],
      "source": [
        "# numeric_df = train[FEATURES]\n",
        "# corr = numeric_df.corr(method = 'pearson')\n",
        "# corr = corr.abs()\n",
        "# # corr.style.background_gradient(cmap='inferno')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdESwz1wQHwM"
      },
      "outputs": [],
      "source": [
        "# upper_tri_mask = np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
        "# upper_tri = corr.where(upper_tri_mask)\n",
        "# highly_correlated_series = upper_tri.stack()\n",
        "# strong_pairs = highly_correlated_series[highly_correlated_series > 0.90]\n",
        "# strong_pairs_df = strong_pairs.reset_index()\n",
        "# strong_pairs_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "# strong_pairs_df_sorted = strong_pairs_df.sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
        "# print(f\"Found {len(strong_pairs_df_sorted)} pairs of features with correlation > 0.90\")\n",
        "# print(\"-\" * 50)\n",
        "# print(strong_pairs_df_sorted.head(200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJYVNzGqBS63"
      },
      "source": [
        "## METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQRvcrT_5Mb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hierarchical macro F1 metric for the CMI 2025 Challenge.\n",
        "\n",
        "This script defines a single entry point `score(solution, submission, row_id_column_name)`\n",
        "that the Kaggle metrics orchestrator will call.\n",
        "It performs validation on submission IDs and computes a combined binary & multiclass F1 score.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.target_gestures = [\n",
        "            'Above ear - pull hair',\n",
        "            'Cheek - pinch skin',\n",
        "            'Eyebrow - pull hair',\n",
        "            'Eyelash - pull hair',\n",
        "            'Forehead - pull hairline',\n",
        "            'Forehead - scratch',\n",
        "            'Neck - pinch skin',\n",
        "            'Neck - scratch',\n",
        "        ]\n",
        "        self.non_target_gestures = [\n",
        "            'Write name on leg',\n",
        "            'Wave hello',\n",
        "            'Glasses on/off',\n",
        "            'Text on phone',\n",
        "            'Write name in air',\n",
        "            'Feel around in tray and pull out an object',\n",
        "            'Scratch knee/leg skin',\n",
        "            'Pull air toward your face',\n",
        "            'Drink from bottle/cup',\n",
        "            'Pinch knee/leg skin'\n",
        "        ]\n",
        "        self.all_classes = self.target_gestures + self.non_target_gestures\n",
        "\n",
        "    def calculate_hierarchical_f1(\n",
        "        self,\n",
        "        sol: pd.DataFrame,\n",
        "        sub: pd.DataFrame\n",
        "    ) -> float:\n",
        "\n",
        "        # Validate gestures\n",
        "        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n",
        "        if invalid_types:\n",
        "            raise ParticipantVisibleError(\n",
        "                f\"Invalid gesture values in submission: {invalid_types}\"\n",
        "            )\n",
        "\n",
        "        # Compute binary F1 (Target vs Non-Target)\n",
        "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
        "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
        "        f1_binary = f1_score(\n",
        "            y_true_bin,\n",
        "            y_pred_bin,\n",
        "            pos_label=True,\n",
        "            zero_division=0,\n",
        "            average='binary'\n",
        "        )\n",
        "\n",
        "        # Build multi-class labels for gestures\n",
        "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "\n",
        "        # Compute macro F1 over all gesture classes\n",
        "        f1_macro = f1_score(\n",
        "            y_true_mc,\n",
        "            y_pred_mc,\n",
        "            average='macro',\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f'f1_binary score: {f1_binary}')\n",
        "        print(f'f1_macro score: {f1_macro}')\n",
        "\n",
        "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
        "\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute hierarchical macro F1 for the CMI 2025 challenge.\n",
        "\n",
        "    Expected input:\n",
        "      - solution and submission as pandas.DataFrame\n",
        "      - Column 'sequence_id': unique identifier for each sequence\n",
        "      - 'gesture': one of the eight target gestures or \"Non-Target\"\n",
        "\n",
        "    This metric averages:\n",
        "    1. Binary F1 on SequenceType (Target vs Non-Target)\n",
        "    2. Macro F1 on gesture (mapping non-targets to \"Non-Target\")\n",
        "\n",
        "    Raises ParticipantVisibleError for invalid submissions,\n",
        "    including invalid SequenceType or gesture values.\n",
        "\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> row_id_column_name = \"id\"\n",
        "    >>> solution = pd.DataFrame({'id': range(4), 'gesture': ['Eyebrow - pull hair']*4})\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Forehead - pull hairline']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.5\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Text on phone']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.0\n",
        "    >>> score(solution, solution, row_id_column_name=row_id_column_name)\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    # Validate required columns\n",
        "    for col in (row_id_column_name, 'gesture'):\n",
        "        if col not in solution.columns:\n",
        "            raise ParticipantVisibleError(f\"Solution file missing required column: '{col}'\")\n",
        "        if col not in submission.columns:\n",
        "            raise ParticipantVisibleError(f\"Submission file missing required column: '{col}'\")\n",
        "\n",
        "    metric = CompetitionMetric()\n",
        "    return metric.calculate_hierarchical_f1(solution, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOrm302jDZ6P"
      },
      "source": [
        "## CNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpze6U9aCrS-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import uuid\n",
        "import random\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "metric_calculator = CompetitionMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xTMV2YDUFep",
        "outputId": "6529a432-ccaa-4648-cf25-505f02168aab"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.experimental.numpy.random.seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print(f\"Seeds set to {seed} for reproducibility\")\n",
        "\n",
        "set_seed(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XqhN_lyh2UK"
      },
      "outputs": [],
      "source": [
        "class SWACallback(callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Stochastic Weight Averaging callback for Keras\n",
        "    \"\"\"\n",
        "    def __init__(self, start_epoch=10, swa_freq=5, verbose=1):\n",
        "        super().__init__()\n",
        "        self.start_epoch = start_epoch\n",
        "        self.swa_freq = swa_freq\n",
        "        self.verbose = verbose\n",
        "        self.swa_weights = None\n",
        "        self.n_models = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch >= self.start_epoch and (epoch - self.start_epoch) % self.swa_freq == 0:\n",
        "            current_weights = self.model.get_weights()\n",
        "\n",
        "            if self.swa_weights is None:\n",
        "                self.swa_weights = [w.copy() for w in current_weights]\n",
        "                self.n_models = 1\n",
        "            else:\n",
        "                self.n_models += 1\n",
        "                for i in range(len(self.swa_weights)):\n",
        "                    self.swa_weights[i] = (\n",
        "                        (self.n_models - 1) * self.swa_weights[i] + current_weights[i]\n",
        "                    ) / self.n_models\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"SWA: Updated weights at epoch {epoch + 1} (n_models: {self.n_models})\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.swa_weights is not None:\n",
        "            self.model.set_weights(self.swa_weights)\n",
        "            if self.verbose:\n",
        "                print(f\"SWA: Applied averaged weights from {self.n_models} models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nfyJxvfbOfC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "class MixupSequence(Sequence):\n",
        "    def __init__(self, X, y, batch_size=32, alpha=0.3, mixup_prob=0.5, mixup_seed=42):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.mixup_prob = mixup_prob\n",
        "        self.indices = np.arange(len(X))\n",
        "        self.seed = mixup_seed\n",
        "\n",
        "        self.total_batches = int(np.ceil(len(X) / batch_size))\n",
        "        self.rng = np.random.RandomState(mixup_seed)\n",
        "        self.reset_random_states()\n",
        "\n",
        "    def reset_random_states(self):\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "        self.mixup_decisions = self.rng.rand(self.total_batches)\n",
        "        self.permutation_seeds = self.rng.randint(0, 10000, self.total_batches)\n",
        "        self.lambda_seeds = self.rng.randint(0, 10000, self.total_batches)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_batches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch = self.X[batch_indices]\n",
        "        y_batch = self.y[batch_indices]\n",
        "\n",
        "        if self.mixup_decisions[idx] < self.mixup_prob:\n",
        "            X_batch, y_batch = self._mixup_batch_reproducible(X_batch, y_batch, idx)\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def _mixup_batch_reproducible(self, X_batch, y_batch, batch_idx):\n",
        "        batch_size = len(X_batch)\n",
        "\n",
        "        perm_rng = np.random.RandomState(self.permutation_seeds[batch_idx])\n",
        "        lambda_rng = np.random.RandomState(self.lambda_seeds[batch_idx])\n",
        "        indices = perm_rng.permutation(batch_size)\n",
        "        lam = lambda_rng.beta(self.alpha, self.alpha, batch_size)\n",
        "\n",
        "        X_mixed = lam[:, np.newaxis, np.newaxis] * X_batch + (1 - lam[:, np.newaxis, np.newaxis]) * X_batch[indices]\n",
        "        y_mixed = lam[:, np.newaxis] * y_batch + (1 - lam[:, np.newaxis]) * y_batch[indices]\n",
        "\n",
        "        return X_mixed, y_mixed\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        shuffle_rng = np.random.RandomState(self.seed + 1000)\n",
        "        shuffle_rng.shuffle(self.indices)\n",
        "\n",
        "        self.seed += 1\n",
        "        self.reset_random_states()\n",
        "\n",
        "\n",
        "\n",
        "def fit_with_swa_and_mixup(self, X, y, validation_data=None, epochs=100, batch_size=32,\n",
        "                          alpha=0.3, mixup_prob=0.5, swa_start=20, swa_freq=5, verbose=1, patience=35):\n",
        "    \"\"\"Train the model with both SWA and mixup data generator\"\"\"\n",
        "\n",
        "    X_scaled = self.scale_features(X, fit=True)\n",
        "    y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "\n",
        "    train_generator = MixupSequence(\n",
        "        X_scaled, y_cat,\n",
        "        batch_size=batch_size,\n",
        "        alpha=alpha,\n",
        "        mixup_prob=mixup_prob,\n",
        "        mixup_seed = 42,\n",
        "    )\n",
        "\n",
        "    val_data = None\n",
        "    if validation_data is not None:\n",
        "        X_val, y_val = validation_data\n",
        "        X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "        y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "        val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "    self.swa_callback = SWACallback(\n",
        "        start_epoch=swa_start,\n",
        "        swa_freq=swa_freq,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    callback_list = [\n",
        "        self.swa_callback,  \n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            patience=patience,  \n",
        "            restore_best_weights=False,  \n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            factor=0.7,  \n",
        "            patience=10,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = self.model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_data,\n",
        "        epochs=epochs,\n",
        "        callbacks=callback_list,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def add_swa_mixup_to_cnn_class():\n",
        "    CNN1DModelWithSWA.fit_with_swa_and_mixup = fit_with_swa_and_mixup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu_svyANzJSK"
      },
      "outputs": [],
      "source": [
        "class CNN1DModelWithSWA:\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=1e-3, full_sensors=False):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.full_sensors = full_sensors\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def create_branch(self, inputs, name_prefix, kernel_sizes, filters):\n",
        "        \"\"\"Create a specialized branch\"\"\"\n",
        "        x = inputs\n",
        "\n",
        "        for i, (k_size, f) in enumerate(zip(kernel_sizes, filters)):\n",
        "            x = layers.Conv1D(f, k_size, padding='same', activation='relu',\n",
        "                            name=f'{name_prefix}_conv_{i}')(x)\n",
        "            x = layers.BatchNormalization(name=f'{name_prefix}_bn_{i}')(x)\n",
        "            if i < len(kernel_sizes) - 1:  # No pooling on last layer\n",
        "                x = layers.MaxPooling1D(2, name=f'{name_prefix}_pool_{i}')(x)\n",
        "            x = layers.Dropout(0.2, name=f'{name_prefix}_dropout_{i}')(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Multi-branch ensemble architecture\"\"\"\n",
        "        inputs = layers.Input(shape=self.input_shape)\n",
        "\n",
        "        branch1 = self.create_branch(\n",
        "            inputs, 'short_term',\n",
        "            kernel_sizes=[3, 3, 3],\n",
        "            filters=[64, 128, 256]\n",
        "        )\n",
        "\n",
        "        branch2 = self.create_branch(\n",
        "            inputs, 'medium_term',\n",
        "            kernel_sizes=[7, 5, 3],\n",
        "            filters=[64, 128, 256]\n",
        "        )\n",
        "\n",
        "        branch3 = self.create_branch(\n",
        "            inputs, 'long_term',\n",
        "            kernel_sizes=[15, 11, 7],\n",
        "            filters=[64, 128, 256]\n",
        "        )\n",
        "\n",
        "        combined = layers.Concatenate()([branch1, branch2, branch3])\n",
        "\n",
        "        x = layers.Conv1D(512, 3, padding='same', activation='relu')(combined) # 512\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.GlobalMaxPooling1D()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = models.Model(inputs, outputs)\n",
        "        if self.full_sensors:\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.AdamW(learning_rate=self.learning_rate),\n",
        "                #   optimizer=tf.keras.optimizers.AdamW(\n",
        "                #     learning_rate=self.learning_rate,\n",
        "                #     weight_decay=1e-5,\n",
        "                #     beta_1=0.9,\n",
        "                #     beta_2=0.999,\n",
        "                #     epsilon=1e-8,\n",
        "                #     clipnorm=1.0  # Gradient clipping\n",
        "                # ),\n",
        "                loss='categorical_crossentropy',\n",
        "                # loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "                #     from_logits=False,\n",
        "                #     label_smoothing=0.1,\n",
        "                #     axis=-1,\n",
        "                #     reduction='sum_over_batch_size',\n",
        "                #     name='categorical_crossentropy'\n",
        "                # ),\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.AdamW(learning_rate=self.learning_rate),\n",
        "                #   optimizer=tf.keras.optimizers.AdamW(\n",
        "                #     learning_rate=self.learning_rate,\n",
        "                #     weight_decay=1e-5,\n",
        "                #     beta_1=0.9,\n",
        "                #     beta_2=0.999,\n",
        "                #     epsilon=1e-8,\n",
        "                #     clipnorm=1.0  # Gradient clipping\n",
        "                # ),\n",
        "                # loss='categorical_crossentropy',\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "                    from_logits=False,\n",
        "                    label_smoothing=0.1,\n",
        "                    axis=-1,\n",
        "                    reduction='sum_over_batch_size',\n",
        "                    name='categorical_crossentropy'\n",
        "                ),\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def prepare_sequences(self, df, features, target_col=None, sequence_length=None):\n",
        "        \"\"\"Fast sequence preparation using numpy operations\"\"\"\n",
        "        df_sorted = df.sort_values(['sequence_id', 'sequence_counter']).reset_index(drop=True)\n",
        "\n",
        "        seq_changes = df_sorted['sequence_id'].ne(df_sorted['sequence_id'].shift()).cumsum() - 1\n",
        "        unique_seqs, seq_starts = np.unique(seq_changes, return_index=True)\n",
        "        seq_ends = np.append(seq_starts[1:], len(df_sorted))\n",
        "        seq_lengths = seq_ends - seq_starts\n",
        "\n",
        "        if sequence_length is None:\n",
        "            sequence_length = seq_lengths.max()\n",
        "\n",
        "        num_sequences = len(seq_starts)\n",
        "        sequences = np.zeros((num_sequences, sequence_length, len(features)), dtype=np.float32)\n",
        "        feature_matrix = df_sorted[features].values.astype(np.float32)\n",
        "        for i, (start, end) in enumerate(zip(seq_starts, seq_ends)):\n",
        "            seq_len = end - start\n",
        "            actual_len = min(seq_len, sequence_length)\n",
        "            sequences[i, :actual_len] = feature_matrix[start:start + actual_len]\n",
        "\n",
        "        if target_col is not None:\n",
        "            targets = df_sorted.iloc[seq_starts][target_col].values\n",
        "            return sequences, targets\n",
        "        else:\n",
        "            return sequences\n",
        "\n",
        "    def fit(self, X, y, validation_data=None, epochs=100, batch_size=32, verbose=1):\n",
        "        \"\"\"Standard training without SWA (for compatibility)\"\"\"\n",
        "        return self.fit_with_swa(\n",
        "            X, y, validation_data, epochs, batch_size,\n",
        "            swa_start=max(10, epochs//4),\n",
        "            swa_freq=5,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        # return self.fit_with_ema(\n",
        "        #     X, y, validation_data, epochs, batch_size,\n",
        "        #     ema_start=0,\n",
        "        #     verbose=verbose\n",
        "        # )\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities\"\"\"\n",
        "        X_scaled = self.scale_features(X, fit=False)\n",
        "        return self.model.predict(X_scaled)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict classes\"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)\n",
        "\n",
        "    def scale_features(self, X, fit=False):\n",
        "        \"\"\"Scale features across time and feature dimensions\"\"\"\n",
        "        original_shape = X.shape\n",
        "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "\n",
        "        if np.any(np.isnan(X_reshaped)) or np.any(np.isinf(X_reshaped)):\n",
        "            print(f\"WARNING: Found NaN/Inf values in input data!\")\n",
        "            print(f\"NaN count: {np.sum(np.isnan(X_reshaped))}\")\n",
        "            print(f\"Inf count: {np.sum(np.isinf(X_reshaped))}\")\n",
        "            X_reshaped = np.nan_to_num(X_reshaped, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if fit:\n",
        "            X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "        else:\n",
        "            X_scaled = self.scaler.transform(X_reshaped)\n",
        "\n",
        "        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n",
        "            print(f\"WARNING: Found NaN/Inf values in scaled data!\")\n",
        "            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        return X_scaled.reshape(original_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEHIW5K7ouZa"
      },
      "source": [
        "## TRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yYp1PIm3Pw1S",
        "outputId": "55ce2f5f-3cdd-4958-e05e-226deb797a68"
      },
      "outputs": [],
      "source": [
        "train_demographics.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37naYx5ih2OP"
      },
      "outputs": [],
      "source": [
        "# Updated cross-validation function with SWA\n",
        "def train_cnn_cross_validation_with_swa(train_df, features, target_col, demographics_df,\n",
        "                                       n_splits=5, label_encoder=None, aggregation_method=None, mixup_prob=1.0, patience=35,\n",
        "                                       swa_start=40, full_sensors=False):\n",
        "    \"\"\"Modified cross-validation function for CNN with SWA\"\"\"\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "    import joblib\n",
        "    import uuid\n",
        "\n",
        "    run_id = uuid.uuid4()\n",
        "    os.makedirs('models_cnn_swa', exist_ok=True)\n",
        "\n",
        "    skf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(\n",
        "            skf.split(demographics_df, demographics_df[['adult_child', 'handedness', 'sex']])\n",
        "        ):\n",
        "        demographics_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "\n",
        "        train_demo = demographics_df.iloc[tr_idx]\n",
        "        val_demo = demographics_df.iloc[val_idx]\n",
        "\n",
        "        print(f\"\\nFold {fold+1}:\")\n",
        "        print(f\"  Train - Adult: {train_demo['adult_child'].mean():.3f}, Right-handed: {train_demo['handedness'].mean():.3f}\")\n",
        "        print(f\"  Val   - Adult: {val_demo['adult_child'].mean():.3f}, Right-handed: {val_demo['handedness'].mean():.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "    demographics_df['fold'] = demographics_df['fold'].astype(int)\n",
        "\n",
        "    train_df = train_df.merge(demographics_df[['subject', 'fold']], on='subject', how='left')\n",
        "\n",
        "    if label_encoder is None:\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        unique_labels = train_df.groupby('sequence_id')[target_col].first().unique()\n",
        "        le.fit(unique_labels)\n",
        "        print(f\"Created label encoder with classes: {le.classes_}\")\n",
        "    else:\n",
        "        le = label_encoder\n",
        "        print(f\"Using provided label encoder with classes: {le.classes_}\")\n",
        "    joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    oof_preds = np.zeros(len(train_df.groupby('sequence_id').first()), dtype=int)\n",
        "    oof_proba = np.zeros((len(train_df.groupby('sequence_id').first()), num_classes))\n",
        "    oof_scores = []\n",
        "\n",
        "    seq_info = train_df.groupby('sequence_id').first()[['fold']].reset_index()\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        print(f\"{'#'*10} Fold {fold+1} with SWA {'#'*10}\")\n",
        "\n",
        "        train_sequences = seq_info[seq_info['fold'] != fold]['sequence_id'].values\n",
        "        valid_sequences = seq_info[seq_info['fold'] == fold]['sequence_id'].values\n",
        "\n",
        "        train_fold_df = train_df[train_df['sequence_id'].isin(train_sequences)]\n",
        "        valid_fold_df = train_df[train_df['sequence_id'].isin(valid_sequences)]\n",
        "\n",
        "        ##################################################### SUBJECT KALDIRMA #######################################\n",
        "        # train_fold_df = train_fold_df[train_fold_df['subject'] != 'SUBJ_011323']\n",
        "        # train_fold_df = train_fold_df[train_fold_df['sequence_id'] != 'SEQ_011975']\n",
        "        # train_fold_df = train_fold_df[train_fold_df['subject'] != 'SUBJ_045235']\n",
        "        # train_fold_df = train_fold_df[train_fold_df['subject'] != 'SUBJ_019262']\n",
        "\n",
        "        print(f\"  Train sequences: {len(train_sequences)}, Valid sequences: {len(valid_sequences)}\")\n",
        "\n",
        "        sequence_lengths = train_fold_df.groupby('sequence_id').size()\n",
        "        max_seq_length = int(sequence_lengths.quantile(0.99))\n",
        "        # max_seq_length = int(sequence_lengths.quantile(0.95))\n",
        "        # max_seq_length = sequence_lengths.max()\n",
        "\n",
        "        # if fold == 0:\n",
        "        #     max_seq_length = sequence_lengths.max()\n",
        "\n",
        "        # elif fold == 1:\n",
        "        #     max_seq_length = int(sequence_lengths.quantile(0.99))\n",
        "\n",
        "        # elif fold == 2:\n",
        "        #     max_seq_length = int(sequence_lengths.quantile(0.95))\n",
        "\n",
        "        # elif fold == 3:\n",
        "        #     max_seq_length = sequence_lengths.max()\n",
        "\n",
        "        # elif fold == 4:\n",
        "        #     max_seq_length = sequence_lengths.max()\n",
        "\n",
        "        joblib.dump(max_seq_length, f\"models_cnn_swa/seq_length_fold_{fold+1}.pkl\")\n",
        "\n",
        "        cnn_model = CNN1DModelWithSWA(\n",
        "            input_shape=(max_seq_length, len(features)),\n",
        "            num_classes=num_classes,\n",
        "            learning_rate=1e-3,\n",
        "            full_sensors=full_sensors\n",
        "        )\n",
        "        cnn_model.build_model()\n",
        "\n",
        "        print(\"Initialized CNN model with SWA\")\n",
        "\n",
        "        X_train, y_train_str = cnn_model.prepare_sequences(\n",
        "            train_fold_df, features, target_col, sequence_length=max_seq_length\n",
        "        )\n",
        "        X_valid, y_valid_str = cnn_model.prepare_sequences(\n",
        "            valid_fold_df, features, target_col, sequence_length=max_seq_length\n",
        "        )\n",
        "\n",
        "        y_train = le.transform(y_train_str)\n",
        "        y_valid = le.transform(y_valid_str)\n",
        "\n",
        "        print(f\"  X_train shape: {X_train.shape}, X_valid shape: {X_valid.shape}\")\n",
        "        print(f\"  y_train shape: {y_train.shape}, y_valid shape: {y_valid.shape}\")\n",
        "\n",
        "        if aggregation_method == \"swa\":\n",
        "            # Use standard SWA\n",
        "            # history = cnn_model.fit_with_swa(\n",
        "            #     X_train, y_train,\n",
        "            #     validation_data=(X_valid, y_valid),\n",
        "            #     epochs=100,\n",
        "            #     batch_size=32,\n",
        "            #     swa_start=20,\n",
        "            #     swa_freq=5,\n",
        "            #     verbose=2\n",
        "            # )\n",
        "\n",
        "            history = cnn_model.fit_with_swa_and_mixup(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_valid, y_valid),\n",
        "                epochs=150,\n",
        "                batch_size=32,\n",
        "                alpha=0.3,\n",
        "                mixup_prob=mixup_prob,\n",
        "                swa_start=swa_start,\n",
        "                swa_freq=5,\n",
        "                verbose=2,\n",
        "                patience=patience,\n",
        "              )\n",
        "\n",
        "        cnn_model.model.save(f\"models_cnn_swa/cnn_swa_fold_{fold+1}.h5\")\n",
        "        joblib.dump(cnn_model.scaler, f\"models_cnn_swa/scaler_swa_fold_{fold+1}.pkl\")\n",
        "\n",
        "        fold_proba = cnn_model.predict_proba(X_valid)\n",
        "        fold_preds = np.argmax(fold_proba, axis=1)\n",
        "\n",
        "        valid_indices = seq_info[seq_info['fold'] == fold].index\n",
        "        oof_proba[valid_indices] = fold_proba\n",
        "        oof_preds[valid_indices] = fold_preds\n",
        "\n",
        "        y_valid_orig = le.inverse_transform(y_valid)\n",
        "        preds_orig = le.inverse_transform(fold_preds)\n",
        "\n",
        "        temp_sol_df = pd.DataFrame({\"gesture\": y_valid_orig})\n",
        "        temp_sub_df = pd.DataFrame({\"gesture\": preds_orig})\n",
        "        fold_score = metric_calculator.calculate_hierarchical_f1(temp_sol_df, temp_sub_df)\n",
        "\n",
        "        oof_scores.append(fold_score)\n",
        "        print(f\"  Fold {fold+1} SWA Score: {fold_score:.4f}\\n\")\n",
        "\n",
        "    print(f\"Mean OOF Score with SWA: {np.mean(oof_scores):.4f}\")\n",
        "    print(f\"Std  OOF Score with SWA: {np.std(oof_scores):.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL OOF CALCULATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    seq_level_df = train_df.groupby('sequence_id')[target_col].last().reset_index()\n",
        "    seq_level_df = seq_level_df.merge(seq_info[['sequence_id']], on='sequence_id', how='inner')\n",
        "    seq_level_df = seq_level_df.sort_values('sequence_id').reset_index(drop=True)\n",
        "\n",
        "    print(seq_level_df.head(100))\n",
        "\n",
        "    if isinstance(seq_level_df[target_col].iloc[0], (int, np.integer)):\n",
        "        original_labels = le.inverse_transform(seq_level_df[target_col])\n",
        "    else:\n",
        "        original_labels = seq_level_df[target_col].values\n",
        "\n",
        "    oof_preds_encoded = np.argmax(oof_proba, axis=1)\n",
        "    oof_preds_original = le.inverse_transform(oof_preds_encoded)\n",
        "\n",
        "    sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "    sub_df = pd.DataFrame({\"gesture\": oof_preds_original})\n",
        "\n",
        "    print(f\"Ground truth shape: {sol_df.shape}\")\n",
        "    print(f\"Predictions shape: {sub_df.shape}\")\n",
        "    print(f\"Unique ground truth gestures: {len(sol_df['gesture'].unique())}\")\n",
        "    print(f\"Unique predicted gestures: {len(sub_df['gesture'].unique())}\")\n",
        "\n",
        "    np.save('oof_preds_cnn_custom.npy', oof_preds_original)\n",
        "    np.save('oof_proba_cnn_custom.npy', oof_proba)\n",
        "    print(\"Saved OOF predictions to 'oof_preds_cnn_custom.npy' and 'oof_proba_cnn_custom.npy'\")\n",
        "\n",
        "    overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "    print(f\"\\nOverall CNN OOF Score: {overall_oof:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return oof_preds, oof_proba, oof_scores, overall_oof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "ePeIRVU3U3-r",
        "outputId": "f60fdf4e-c38a-4ca5-c50f-a86126cd6a23"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP2MCockc9HQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxiRGrXYtNeK",
        "outputId": "faaaddc8-8667-4df9-b87a-bbf05adf46ff"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "add_swa_mixup_to_cnn_class()\n",
        "\n",
        "velocity_features = []\n",
        "rolling_features = []\n",
        "\n",
        "\n",
        "thm_vel_features = []\n",
        "thm_roll_features = []\n",
        "tof_vel_features = []\n",
        "tof_regional_features = []\n",
        "tof_advanced_features = []\n",
        "\n",
        "\n",
        "\n",
        "imu_features = [\n",
        "                'acc_x', 'acc_y', 'acc_z',\n",
        "                'acc_mag',\n",
        "                'rot_w', 'rot_x', 'rot_y', 'rot_z',\n",
        "                'rot_mag',\n",
        "                'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
        "                'angular_distance',\n",
        "                ]\n",
        "\n",
        "thm_features = [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]\n",
        "\n",
        "\n",
        "tof_features = [\n",
        "    # \"tof_1_mean_distance\",\n",
        "    # \"tof_2_mean_distance\",\n",
        "    # \"tof_3_mean_distance\", \"tof_4_mean_distance\", \"tof_5_mean_distance\",\n",
        "\n",
        "    'tof_1_active_pixels', 'tof_2_active_pixels', 'tof_3_active_pixels', 'tof_4_active_pixels',\n",
        "    'tof_5_active_pixels',\n",
        "\n",
        "    'tof_1_contrast_q1', 'tof_1_contrast_q2', 'tof_1_contrast_q3', 'tof_1_contrast_q4',\n",
        "    'tof_2_contrast_q1', 'tof_2_contrast_q2', 'tof_2_contrast_q3', 'tof_2_contrast_q4',\n",
        "    'tof_3_contrast_q1', 'tof_3_contrast_q2', 'tof_3_contrast_q3', 'tof_3_contrast_q4',\n",
        "    'tof_4_contrast_q1', 'tof_4_contrast_q2', 'tof_4_contrast_q3', 'tof_4_contrast_q4',\n",
        "\n",
        "    'tof_1_com_x', 'tof_1_com_y', 'tof_2_com_x', 'tof_2_com_y', 'tof_3_com_x', 'tof_3_com_y', 'tof_4_com_x', 'tof_4_com_y', 'tof_5_com_x', 'tof_5_com_y',\n",
        "    'tof_1_center_distance', 'tof_2_center_distance', 'tof_3_center_distance', 'tof_4_center_distance', 'tof_5_center_distance',\n",
        "]\n",
        "\n",
        "for col in ['acc_x', 'acc_y', 'acc_z', 'acc_mag']:\n",
        "        velocity_features.append(f\"{col}_vel\")\n",
        "\n",
        "for col in ['angular_vel_x', 'angular_vel_y', 'angular_vel_z']:\n",
        "        velocity_features.append(f\"{col}_vel\")\n",
        "\n",
        "# for col in ['rot_w', 'rot_x', 'rot_y', 'rot_z']:\n",
        "#         velocity_features.append(f\"{col}_vel\")\n",
        "\n",
        "\n",
        "for window in [3, 5, 10]:\n",
        "  for col in [\"acc_mag\", \"acc_x\", \"acc_y\", \"acc_z\", \"angular_vel_mag\",\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\",\n",
        "              # 'rot_w', 'rot_x', 'rot_y', 'rot_z'\n",
        "              ]:\n",
        "\n",
        "      rolling_features.extend([\n",
        "          f\"{col}_roll_std_{window}\",\n",
        "          f\"{col}_roll_min_{window}\",\n",
        "      ])\n",
        "\n",
        "\n",
        "for col in [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]:\n",
        "    thm_vel_features.append(f\"{col}_vel\")\n",
        "    thm_vel_features.append(f\"{col}_relative\")\n",
        "\n",
        "\n",
        "\n",
        "for mode in [4]: # 4, 8\n",
        "        for sensor_idx in range(1, 6):\n",
        "            for region_idx in range(mode):\n",
        "                tof_regional_features.extend([\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_mean',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_std',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_min',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_max',\n",
        "                ])\n",
        "\n",
        "\n",
        "features = imu_features + velocity_features + rolling_features + thm_features + thm_vel_features + tof_features + tof_regional_features\n",
        "\n",
        "\n",
        "\n",
        "oof_preds, oof_proba, oof_scores, overall_oof = train_cnn_cross_validation_with_swa(\n",
        "    train_df=train,\n",
        "    features=features,  # or FEATURES\n",
        "    target_col='gesture',\n",
        "    demographics_df=train_demographics,\n",
        "    n_splits=5,\n",
        "    label_encoder=None,\n",
        "    aggregation_method=\"swa\",\n",
        "    mixup_prob = 1.0,\n",
        "    patience=50,\n",
        "    swa_start=40,\n",
        "    full_sensors=True,\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Individual Fold Scores: {[f'{score:.4f}' for score in oof_scores]}\")\n",
        "print(f\"Mean Fold Score: {np.mean(oof_scores):.4f}  {np.std(oof_scores):.4f}\")\n",
        "print(f\"Overall OOF Score: {overall_oof:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2F4sRKKZl9k"
      },
      "outputs": [],
      "source": [
        "## rot featurelar nan ise 0'ladk:\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7993', '0.7947', '0.7821', '0.7839', '0.7842']\n",
        "# Mean Fold Score: 0.7889  0.0069\n",
        "# Overall OOF Score: 0.7895\n",
        "\n",
        "## bu yntem ve seq id kaldrma cortlad\n",
        "\n",
        "## bu yntem ve patience=45\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8006', '0.7976', '0.7855', '0.7830', '0.7880']\n",
        "# Mean Fold Score: 0.7909  0.0069\n",
        "# Overall OOF Score: 0.7913\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, TM VERDE SUBJ_011323 YOK, SEQ_011975 YOK, NEW FEATURES\n",
        "\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8401', '0.8322', '0.8511', '0.8427', '0.8594']\n",
        "# Mean Fold Score: 0.8451  0.0094\n",
        "# Overall OOF Score: 0.8452\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, TM VERDE SUBJ_011323 YOK, SEQ_011975 YOK, NEW FEATURES (active pixels etc.)\n",
        "\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8429', '0.8366', '0.8467', '0.8523', '0.8601']\n",
        "# Mean Fold Score: 0.8477  0.0080\n",
        "# Overall OOF Score: 0.8478"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuOMznuGpf-3"
      },
      "outputs": [],
      "source": [
        "## MU ONLY: 0.7895 LBDE 0.833 VERD\n",
        "\n",
        "# ## MU ONLY TM VERDE SUBJ_011323 YOK, SEQ_011975 YOK\n",
        "# Individual Fold Scores: ['0.7897', '0.7949', '0.7868', '0.7792', '0.7895'] # Overall OOF Score: 0.7891\n",
        "\n",
        "# ## MU ONLY TM VERDE SUBJ_011323 YOK, SEQ_011975 YOK ve ROT NULL 50 TANES YOK\n",
        "# Individual Fold Scores: ['0.7895', '0.7916', '0.7842', '0.7891', '0.7893'] # Overall OOF Score: 0.7894  0.0024 lb: 0.830\n",
        "\n",
        "# ## MU ONLY TM VERDE SUBJ_011323 YOK, ve ROT NULL 50 TANES YOK\n",
        "# Individual Fold Scores: ['0.7929', '0.7949', '0.7829', '0.7895', '0.7832']\n",
        "# Overall OOF Score: 0.7896 LBDE 0.832 VERD\n",
        "\n",
        "\n",
        "# ## MU ONLY TM VERDE SADECE ROT NULL 50 TANES YOK:\n",
        "# Individual Fold Scores: ['0.7976', '0.7819', '0.7814', '0.7928', '0.7835']\n",
        "# Overall OOF Score: 0.7877\n",
        "\n",
        "# ## MU ONLY TM VERDE  ROT NULL 50 TANES YOK, SEQ_011975 YOK:\n",
        "# Individual Fold Scores: ['0.7867', '0.7970', '0.7817', '0.7768', '0.7810'] # Overall OOF Score: 0.7853  0.0069\n",
        "\n",
        "\n",
        "\n",
        "# ## MU ONLY TM VERDE SADECE ROT NULL 50 TANES N missing flag: 0.7882\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## TOF=4, TOF MEAN YOK, PATIENCE 35, LABEL SMOOTH YOK, CV: 0.835\n",
        "## TOF=4, TOF MEAN YOK, PATIENCE 35, LABEL SMOOTH 0.1, CV: 0.8334\n",
        "## TOF=4, TOF MEAN YOK, PATIENCE 50, LABEL SMOOTH YOK, CV:\n",
        "\n",
        "## TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK:\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8447', '0.8311', '0.8521', '0.8147', '0.8504']\n",
        "# Mean Fold Score: 0.8386  0.0140\n",
        "# Overall OOF Score: 0.8387\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, SUBJ_011323 YOK:\n",
        "## SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8329', '0.8324', '0.8472', '0.8340', '0.8509']\n",
        "# Mean Fold Score: 0.8395  0.0079\n",
        "# Overall OOF Score: 0.8397\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, TM VERDE SUBJ_011323 YOK, SEQ_011975 YOK:\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8451', '0.8277', '0.8516', '0.8429', '0.8456']\n",
        "# Mean Fold Score: 0.8426  0.0080\n",
        "# Overall OOF Score: 0.8425 bunu yanllkla kaggle verisetinde 0.8451 yazmm herhalde\n",
        "\n",
        "\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, SADECE TRAINDE SUBJ_011323 YOK, SEQ_011975 YOK:\n",
        "\n",
        "# Overall OOF Score: 0.8359\n",
        "\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, SADECE TRAINDE SUBJ_045235, SUBJ_019262 YOK:\n",
        "\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8395', '0.8331', '0.8499', '0.8118', '0.8441']\n",
        "# Mean Fold Score: 0.8357  0.0132\n",
        "# Overall OOF Score: 0.8361\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, SADECE TRAINDE SUBJ_011323 YOK, SEQ_011975 SUBJ_045235, SUBJ_019262 YOK: BUNU LBDE DENEEEEEEEEEEEEEEEEEEEEEE\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8364', '0.8319', '0.8463', '0.8041', '0.8383']\n",
        "# Mean Fold Score: 0.8314  0.0144\n",
        "# Overall OOF Score: 0.8317\n",
        "\n",
        "# TOF=4, TOF MEAN YOK, PATIENCE 50, CONTRAST VAR, LABEL SMOOTH YOK, TM VERDE SUBJ_011323 YOK, SEQ_011975 SUBJ_045235, SUBJ_019262 YOK: BUNU LBDE DENEEEEEEEEEEEEEEEEEEEEEE\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.8364', '0.8319', '0.8463', '0.8542', '0.8559']\n",
        "# Mean Fold Score: 0.8449  0.0095\n",
        "# Overall OOF Score: 0.8447\n",
        "\n",
        "\n",
        "## TOF DETR,\n",
        "## TOF MEAN EKLE\n",
        "## YENI FEATURELARI DENE\n",
        "# PATIENCE ARTIR\n",
        "# CONTRAST FEATURELAR EKLE\n",
        "# imu onlyde 0.05 label smoothing dene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUYt1R9KT46C"
      },
      "source": [
        "## SEQUENCE 0.99, PATIENCE 35, LABEL SMOOTH=0.1:\n",
        "* 0.7993, 0.7947, 0.7821,  OOF: 0.7895 LB: 0.797\n",
        "\n",
        "## SEQUENCE 0.99, PATIENCE 50, LABEL SMOOTH=0.1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFypSsIPIxB1"
      },
      "outputs": [],
      "source": [
        "### HER FOLD ICIN AYRI SEQ LENGTH DENE\n",
        "### SUBMITLE BAK\n",
        "### DER MMARELRE BAK\n",
        "\n",
        "\n",
        "### FARKLI MXUP DENE THM TOF N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90wOIOwzFPHj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# sequence 0.9:\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7914', '0.7861', '0.7778', '0.7847', '0.7756']\n",
        "# Mean Fold Score: 0.7831  0.0057\n",
        "# Overall OOF Score: 0.7838\n",
        "\n",
        "\n",
        "# sequence 0.95\n",
        "\n",
        "#                     ['0.7906', '0.7835', '0.7786', '0.7720', '0.7829'] oof: 0.7823  0.0061\n",
        "# label smoothing 0.1: '0.7908', '0.7866', '0.7751', '0.7823', '0.7839'], oof: 0.7842  0.0052\n",
        "# mode = 4 score: ['0.8328', '0.8239', '0.8384', '0.7989', '0.8379'] 0.8268  0.0147\n",
        "\n",
        "\n",
        "# sequence 0.98\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7919', '0.7956', '0.7778', '0.7827', '0.7835']\n",
        "# Mean Fold Score: 0.7863  0.0065\n",
        "# Overall OOF Score: 0.7872\n",
        "\n",
        "# sequence 0.99:\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7856', '0.7975', '0.7867', '0.7841', '0.7887'] *************************\n",
        "# Mean Fold Score: 0.7885  0.0047\n",
        "# Overall OOF Score: 0.7895\n",
        "\n",
        "## FULL SENSORS: SWA START=40, MODE=4\n",
        "# Individual Fold Scores: [0.8387, 0.8241, 0.8516, 0.8126, 0.8481]\n",
        "# Mean Fold Score: 0.8350  0.0147\n",
        "# Overall OOF Score: 0.8352\n",
        "\n",
        "# sequence max:\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7981', '0.7912', '0.7843', '0.7856', '0.7898']\n",
        "# Mean Fold Score: 0.7898  0.0049\n",
        "# Overall OOF Score: 0.7900\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTNGP9lR2dF2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "029f13b4f7a441a5ac38b97b11f66e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "113f8ca2d8ea45cda08293ee4af3dc20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e9ddbf05105439e9427027ee6ce0d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32ddb1a9862e464bb6688fe08d9d2090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "34f6f6615a664528b372c4e395ad3146": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa75bca288242b594b125867a468e65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec87fb5da7b483695ab416d5604e781": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e370dfa97ff54278bed31a65d1914143",
              "IPY_MODEL_ae57f0fc0658442eb063b38fe1df31e3",
              "IPY_MODEL_7b60751304b446b28f244d0ecb2b8201",
              "IPY_MODEL_e8e959e315024639878963dba3b9fa50",
              "IPY_MODEL_6813aea3f91e43eeb8a7a62e8808d25a"
            ],
            "layout": "IPY_MODEL_d3cfa2bfd304499d8686858f84ae31f9"
          }
        },
        "6813aea3f91e43eeb8a7a62e8808d25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87428f8be1c945539482aed50461d496",
            "placeholder": "",
            "style": "IPY_MODEL_113f8ca2d8ea45cda08293ee4af3dc20",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "7b60751304b446b28f244d0ecb2b8201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_34f6f6615a664528b372c4e395ad3146",
            "placeholder": "",
            "style": "IPY_MODEL_029f13b4f7a441a5ac38b97b11f66e59",
            "value": ""
          }
        },
        "7b66850a912449398d5c52d820aa4e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87428f8be1c945539482aed50461d496": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae57f0fc0658442eb063b38fe1df31e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7b66850a912449398d5c52d820aa4e2c",
            "placeholder": "",
            "style": "IPY_MODEL_2e9ddbf05105439e9427027ee6ce0d65",
            "value": ""
          }
        },
        "c3e05620b2194a5a83ffc1cb176d72d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61a599bcfb0426b8edb40aae7338948": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3cfa2bfd304499d8686858f84ae31f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e370dfa97ff54278bed31a65d1914143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa75bca288242b594b125867a468e65",
            "placeholder": "",
            "style": "IPY_MODEL_c61a599bcfb0426b8edb40aae7338948",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "e8e959e315024639878963dba3b9fa50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c3e05620b2194a5a83ffc1cb176d72d0",
            "style": "IPY_MODEL_32ddb1a9862e464bb6688fe08d9d2090",
            "tooltip": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
