{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RhGQArbXbOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPH1bbFZz7V",
        "outputId": "7f5344a7-dc4e-44be-be6b-4fb583bcef75"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "c59a8c92799043908ef9b5d122892277",
            "516295f52abc4c4d9ca67651de81e91a",
            "b0a5c5e0e65b4c1cb794b358e466c1e4",
            "18c3186e0d294ef59a8c2cf9a5ad4991",
            "c028666291d54038ade2fe51bf2f66c5",
            "6f52edc9ddc34f8181474d226b6adecc",
            "8c9a5849f0ee49459a80e0b1e54cf7e4",
            "fe78b18b0d224a42b6bbc75144d6bee9",
            "f90cdb2773a84aa7b1ae5aa4daf884ef",
            "58eb998bb7284c3fbe7d120367985b72",
            "dac10dc8c9e542cd9f087b57ab127198",
            "f332b5f88077416f9e688e9a49a4c591",
            "537d4965a0e6456b8a7e0cdff51126cf",
            "8b6b85a1fc174486a1204db5a048b52d",
            "0db3054994a2469fa6d39de78bf032e5",
            "1f3d59b133b447fdbd78453cb755297f",
            "af83b654457641ddb536897ab0760bdc"
          ]
        },
        "id": "ka_oHwl5_5MN",
        "outputId": "b26ed61c-9933-46c8-bb54-eb0291fad42b"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_g6C36S_5MR",
        "outputId": "a13a276b-fee2-4ab9-8a1e-7be5914c4e5c"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "cmi_detect_behavior_with_sensor_data_path = kagglehub.competition_download('cmi-detect-behavior-with-sensor-data')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7jDg6tfTvdp",
        "outputId": "1d455fa8-c328-4368-c2c5-09d40328035b"
      },
      "outputs": [],
      "source": [
        "!pip install iterative-stratification==0.1.7 -qq\n",
        "!pip install polars==1.21.0 -qq\n",
        "!pip install pytabkit -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqE_dw_9X9GR"
      },
      "outputs": [],
      "source": [
        "import pytabkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saGqBOlJYDzv"
      },
      "outputs": [],
      "source": [
        "from pytabkit import RealMLP_TD_Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rYRqCy_5MT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import polars as pl\n",
        "import sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 2000)\n",
        "pd.set_option('display.max_rows', 2000)\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8MqxrueedN6",
        "outputId": "ce8b21be-7c30-4613-dfff-fb4f10f991f6"
      },
      "outputs": [],
      "source": [
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(pl.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(joblib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "gY6LM36r_5MW",
        "outputId": "09ea4771-7f8c-4cce-99d6-6f4f37cbc941"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "train = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train.csv')\n",
        "test = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test.csv')\n",
        "train_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train_demographics.csv')\n",
        "test_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test_demographics.csv')\n",
        "\n",
        "train = train.to_pandas()\n",
        "test = test.to_pandas()\n",
        "train_demographics = train_demographics.to_pandas()\n",
        "test_demographics = test_demographics.to_pandas()\n",
        "\n",
        "train = pd.merge(train, train_demographics, on='subject', how='left')\n",
        "test = pd.merge(test, test_demographics, on='subject', how='left')\n",
        "\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDn9xqTCegC0"
      },
      "outputs": [],
      "source": [
        "# # Calculate average y acceleration per subject\n",
        "# def find_wrong_side_subjects(df):\n",
        "#     \"\"\"Find subjects who wore device on wrong wrist (negative avg y acceleration)\"\"\"\n",
        "\n",
        "#     subject_acc_y_stats = df.groupby('subject')['rot_x'].agg([\n",
        "#         'mean', 'std', 'count'\n",
        "#     ]).reset_index()\n",
        "\n",
        "#     subject_acc_y_stats.columns = ['subject', 'thm_3_mean', 'acc_z_std', 'sample_count']\n",
        "\n",
        "#     print(\"Y Acceleration Statistics by Subject:\")\n",
        "#     print(subject_acc_y_stats.sort_values('thm_3_mean'))\n",
        "\n",
        "#     # Identify subjects with negative average y acceleration\n",
        "#     wrong_side_subjects = subject_acc_y_stats[subject_acc_y_stats['thm_3_mean'] < 0]\n",
        "#     normal_subjects = subject_acc_y_stats[subject_acc_y_stats['thm_3_mean'] > 0]\n",
        "\n",
        "#     print(f\"\\nSubjects with NEGATIVE average y acceleration (wrong side): {len(wrong_side_subjects)}\")\n",
        "#     print(wrong_side_subjects[['subject', 'thm_3_mean']])\n",
        "\n",
        "#     print(f\"\\nSubjects with POSITIVE average y acceleration (correct side): {len(normal_subjects)}\")\n",
        "\n",
        "#     return wrong_side_subjects['subject'].tolist(), normal_subjects['subject'].tolist()\n",
        "\n",
        "# # Find the problematic subjects\n",
        "# wrong_side_subjects, normal_subjects = find_wrong_side_subjects(train)\n",
        "\n",
        "# # Detailed analysis of the wrong-side subjects\n",
        "# if wrong_side_subjects:\n",
        "#     print(f\"\\nDetailed analysis of wrong-side subjects:\")\n",
        "#     for subject in wrong_side_subjects:\n",
        "#         subject_data = train[train['subject'] == subject]\n",
        "#         sequences = subject_data['sequence_id'].nunique()\n",
        "#         gestures = subject_data['gesture'].nunique() if 'gesture' in subject_data.columns else 'Unknown'\n",
        "\n",
        "#         print(f\"\\nSubject: {subject}\")\n",
        "#         print(f\"  Sequences: {sequences}\")\n",
        "#         print(f\"  Unique gestures: {gestures}\")\n",
        "#         print(f\"  Avg acc_y: {subject_data['acc_y'].mean():.4f}\")\n",
        "#         print(f\"  Avg acc_x: {subject_data['acc_x'].mean():.4f}\")\n",
        "#         print(f\"  Avg acc_z: {subject_data['acc_z'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0ITj3X7UMz_"
      },
      "outputs": [],
      "source": [
        "# def correct_wrong_side_subjects(df):\n",
        "#     \"\"\"Correct systematic measurement error from wrong-side device placement\"\"\"\n",
        "\n",
        "#     # Identify wrong-side subjects\n",
        "#     subject_acc_y = df.groupby('subject')['acc_y'].mean()\n",
        "#     wrong_side_subjects = subject_acc_y[subject_acc_y < 0].index.tolist()\n",
        "\n",
        "#     print(f\"Wrong-side subjects identified: {wrong_side_subjects}\")\n",
        "#     print(f\"Original avg acc_y: {df[df['subject'].isin(wrong_side_subjects)]['acc_y'].mean():.4f}\")\n",
        "\n",
        "#     # Flip y-axis for wrong-side subjects\n",
        "#     df_corrected = df.copy()\n",
        "#     wrong_side_mask = df_corrected['subject'].isin(wrong_side_subjects)\n",
        "#     df_corrected.loc[wrong_side_mask, 'acc_y'] = -df_corrected.loc[wrong_side_mask, 'acc_y']\n",
        "#     df_corrected.loc[wrong_side_mask, 'acc_x'] = -df_corrected.loc[wrong_side_mask, 'acc_x']\n",
        "\n",
        "\n",
        "#     print(f\"Corrected avg acc_y: {df_corrected[df_corrected['subject'].isin(wrong_side_subjects)]['acc_y'].mean():.4f}\")\n",
        "\n",
        "#     return df_corrected, wrong_side_subjects\n",
        "\n",
        "# # Apply correction\n",
        "# train, wrong_side_subjects = correct_wrong_side_subjects(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNq1yWH1MU9s"
      },
      "outputs": [],
      "source": [
        "# # Check for missing quaternion data\n",
        "# def find_missing_quaternion_sequences(df):\n",
        "#     \"\"\"Find sequences with significant missing quaternion data\"\"\"\n",
        "\n",
        "#     rot_cols = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
        "\n",
        "#     # Calculate missing percentage per sequence\n",
        "#     seq_missing_rot = []\n",
        "\n",
        "#     for seq_id in df['sequence_id'].unique():\n",
        "#         seq_data = df[df['sequence_id'] == seq_id]\n",
        "\n",
        "#         # Count missing values across all rot columns\n",
        "#         total_rot_values = len(seq_data) * len(rot_cols)\n",
        "#         missing_rot_values = seq_data[rot_cols].isnull().sum().sum()\n",
        "\n",
        "#         missing_pct = (missing_rot_values / total_rot_values) * 100\n",
        "\n",
        "#         seq_missing_rot.append({\n",
        "#             'sequence_id': seq_id,\n",
        "#             'total_timesteps': len(seq_data),\n",
        "#             'missing_rot_values': missing_rot_values,\n",
        "#             'missing_percentage': missing_pct,\n",
        "#             'subject': seq_data['subject'].iloc[0],\n",
        "#             'gesture': seq_data['gesture'].iloc[0] if 'gesture' in seq_data.columns else None\n",
        "#         })\n",
        "\n",
        "#     # Convert to DataFrame for analysis\n",
        "#     missing_df = pd.DataFrame(seq_missing_rot)\n",
        "\n",
        "#     return missing_df\n",
        "\n",
        "# # Find missing quaternion sequences\n",
        "# missing_rot_analysis = find_missing_quaternion_sequences(train)\n",
        "\n",
        "# # Filter sequences with significant missing rot data (>50% missing)\n",
        "# problematic_rot_sequences = missing_rot_analysis[missing_rot_analysis['missing_percentage'] > 50]\n",
        "\n",
        "# print(f\"Total sequences analyzed: {len(missing_rot_analysis)}\")\n",
        "# print(f\"Sequences with >50% missing rot data: {len(problematic_rot_sequences)}\")\n",
        "# print(f\"Sequences with >90% missing rot data: {len(missing_rot_analysis[missing_rot_analysis['missing_percentage'] > 90])}\")\n",
        "# print()\n",
        "\n",
        "# # Show the most problematic sequences\n",
        "# print(\"Top 10 sequences with most missing rot data:\")\n",
        "# print(problematic_rot_sequences.nlargest(50, 'missing_percentage')[['sequence_id', 'subject', 'gesture', 'missing_percentage', 'total_timesteps']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k07J4kCUODxr"
      },
      "outputs": [],
      "source": [
        "# # # Show the most problematic sequences\n",
        "# # print(\"Top 10 sequences with most missing rot data:\")\n",
        "# # print(problematic_rot_sequences.nlargest(200, 'missing_percentage')[['sequence_id', 'subject', 'gesture', 'missing_percentage', 'total_timesteps']])\n",
        "# missing_rot_sequences = problematic_rot_sequences['sequence_id'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ic-BSLy1d3"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaWmMuIS_5MZ"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "  TARGET = \"gesture\"\n",
        "  SUBJECT = \"subject\"\n",
        "  TRAIN_ONLY_COLS = ['sequence_type', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
        "  NUM_CLASSES = train.gesture.nunique()\n",
        "  FOLDS = 5\n",
        "  ERR = 1e-8\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "imu_cols = [\n",
        "            \"acc_x\", \"acc_y\", \"acc_z\",\n",
        "            \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
        "            \"acc_mag\",\n",
        "\n",
        "            \"euler_roll\", \"euler_pitch\", \"euler_yaw\",\n",
        "            \"euler_total\", \"pitch_roll_ratio\", \"yaw_pitch_ratio\",\n",
        "\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "\n",
        "            \"angular_jerk_x\", \"angular_jerk_y\", \"angular_jerk_z\",\n",
        "            ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7nJrmq5sdv"
      },
      "source": [
        "## FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D3m98J9LXEw",
        "outputId": "f5029b32-ee86-4f1b-cd2b-32a3e39a3695"
      },
      "outputs": [],
      "source": [
        "print(train.subject.nunique())\n",
        "print(train.sequence_id.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXkapfIo9Pew"
      },
      "outputs": [],
      "source": [
        "# train = train[train['subject'] != 'SUBJ_011323']\n",
        "# # # # train = train[train['subject'] != 'SUBJ_045235']\n",
        "# # # # train = train[train['subject'] != 'SUBJ_019262']\n",
        "# train = train[train['sequence_id'] != 'SEQ_011975']\n",
        "\n",
        "# # # for seq_id in missing_rot_sequences:\n",
        "# # #   train = train[train['sequence_id'] != seq_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7acPAQBDLhrC",
        "outputId": "e6bd4d08-b99d-47d0-b7b9-c458b88d4e10"
      },
      "outputs": [],
      "source": [
        "print(train.subject.nunique())\n",
        "print(train.sequence_id.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTDoFvXb_5Ma"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "  TARGET = \"gesture\"\n",
        "  SUBJECT = \"subject\"\n",
        "  TRAIN_ONLY_COLS = ['sequence_type', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
        "  NUM_CLASSES = train.gesture.nunique()\n",
        "  FOLDS = 5\n",
        "  ERR = 1e-8\n",
        "\n",
        "imu_cols = [\n",
        "            \"acc_x\", \"acc_y\", \"acc_z\",\n",
        "            \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
        "            \"acc_mag\",\n",
        "\n",
        "            \"euler_roll\", \"euler_pitch\", \"euler_yaw\",\n",
        "            \"euler_total\", \"pitch_roll_ratio\", \"yaw_pitch_ratio\",\n",
        "\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "\n",
        "            \"angular_jerk_x\", \"angular_jerk_y\", \"angular_jerk_z\",\n",
        "            ]\n",
        "\n",
        "\n",
        "demo_cols = [\"adult_child\", \"age\", \"sex\", \"handedness\", \"height_cm\", \"shoulder_to_wrist_cm\", \"elbow_to_wrist_cm\"]\n",
        "\n",
        "thm_cols = [\"thm_1\", \"thm_2\",\"thm_3\",\"thm_4\",\"thm_5\"]\n",
        "tof_cols = [col for col in test.columns if col.startswith(\"tof_\")]\n",
        "\n",
        "seq_agg_cols = []\n",
        "for col in imu_cols:\n",
        "    seq_agg_cols.extend([\n",
        "        f'{col}_seq_mean',\n",
        "        f'{col}_seq_std',\n",
        "        f'{col}_seq_min',\n",
        "        ])\n",
        "\n",
        "\n",
        "position_cols = []\n",
        "for col in imu_cols:\n",
        "    position_cols.extend([\n",
        "\n",
        "        f'{col}_avg_velocity',\n",
        "\n",
        "        f'{col}_early_mean',\n",
        "        f'{col}_mid_mean',\n",
        "        f'{col}_mid2_mean',\n",
        "        f'{col}_mid3_mean',\n",
        "        f'{col}_late_mean',\n",
        "\n",
        "        f'{col}_early_std',\n",
        "        f'{col}_mid_std',\n",
        "        f'{col}_mid2_std',\n",
        "        f'{col}_mid3_std',\n",
        "        f'{col}_late_std',\n",
        "\n",
        "        f'{col}_early_velocity_mean',\n",
        "        f'{col}_mid_velocity_mean',\n",
        "        f'{col}_mid2_velocity_mean',\n",
        "        f'{col}_mid3_velocity_mean',\n",
        "        f'{col}_late_velocity_mean',\n",
        "\n",
        "        f'{col}_early_velocity_std',\n",
        "        f'{col}_mid_velocity_std',\n",
        "        f'{col}_mid2_velocity_std',\n",
        "        f'{col}_mid3_velocity_std',\n",
        "        f'{col}_late_velocity_std',\n",
        "\n",
        "        # f'{col}_early_energy',\n",
        "        # f'{col}_mid_energy',\n",
        "        # f'{col}_mid2_energy',\n",
        "        # f'{col}_mid3_energy',\n",
        "        f'{col}_late_energy',\n",
        "\n",
        "        f'{col}_very_late_mean',\n",
        "        f'{col}_very_late_std',\n",
        "\n",
        "        f'{col}_early_late_mean_ratio',\n",
        "        f'{col}_early_late_std_ratio',\n",
        "        f'{col}_early_late_energy_ratio',\n",
        "\n",
        "    ])\n",
        "\n",
        "for col in imu_cols:\n",
        "    position_cols.extend([\n",
        "        f\"{col}_late_gesture_zero_crossing_rate_std\",\n",
        "        # f\"{col}_dwell_time\",\n",
        "    ])\n",
        "\n",
        "corr_cols = []\n",
        "axis_pairs = [('acc_x', 'acc_y'), ('acc_x', 'acc_z'), ('acc_y', 'acc_z')]\n",
        "for axis1, axis2 in axis_pairs:\n",
        "    corr_cols.extend([\n",
        "        f'{axis1}_{axis2}_corr',\n",
        "    ])\n",
        "\n",
        "euler_pairs = [('euler_roll', 'euler_pitch'), ('euler_roll', 'euler_yaw'), ('euler_pitch', 'euler_yaw')]\n",
        "for axis1, axis2 in euler_pairs:\n",
        "    corr_cols.extend([\n",
        "        f'{axis1}_{axis2}_corr',\n",
        "    ])\n",
        "\n",
        "\n",
        "# FEATURES = imu_cols + seq_agg_cols + temporal_cols + position_cols + corr_cols\n",
        "FEATURES = imu_cols + seq_agg_cols + position_cols + corr_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFCgU15VcGPd",
        "outputId": "76c8c839-450f-4409-f78c-8c44dc804f88"
      },
      "outputs": [],
      "source": [
        "def cast_to_object(df):\n",
        "  df['adult_child'] = df['adult_child'].astype(\"category\")\n",
        "  df['sex'] = df['sex'].astype(\"category\")\n",
        "  df['handedness'] = df['handedness'].astype(\"category\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def aggregation_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "    agg_exprs = []\n",
        "\n",
        "    for col in imu_cols:\n",
        "        agg_exprs.extend([\n",
        "            # Basic statistics\n",
        "            pl.col(col).mean().over('sequence_id').alias(f'{col}_seq_mean'),\n",
        "            pl.col(col).std().over('sequence_id').alias(f'{col}_seq_std'),\n",
        "            pl.col(col).min().over('sequence_id').alias(f'{col}_seq_min'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(agg_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def quaternion_to_euler(w, x, y, z):\n",
        "    \"\"\"Convert quaternion to Euler angles (roll, pitch, yaw) in radians\"\"\"\n",
        "\n",
        "    # Roll (x-axis rotation)\n",
        "    sinr_cosp = 2 * (w * x + y * z)\n",
        "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
        "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
        "\n",
        "    # Pitch (y-axis rotation)\n",
        "    sinp = 2 * (w * y - z * x)\n",
        "    pitch = np.where(np.abs(sinp) >= 1, np.copysign(np.pi / 2, sinp), np.arcsin(sinp))\n",
        "\n",
        "    # Yaw (z-axis rotation)\n",
        "    siny_cosp = 2 * (w * z + x * y)\n",
        "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "\n",
        "    return roll, pitch, yaw\n",
        "\n",
        "def mag_features(df):\n",
        "\n",
        "    df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"]**2 + df[\"acc_y\"]**2 + df[\"acc_z\"]**2)\n",
        "    df[\"rot_mag\"] = np.sqrt(df[\"rot_x\"]**2 + df[\"rot_y\"]**2 + df[\"rot_z\"]**2)\n",
        "\n",
        "    roll, pitch, yaw = quaternion_to_euler(df[\"rot_w\"], df[\"rot_x\"], df[\"rot_y\"], df[\"rot_z\"])\n",
        "\n",
        "    df[\"euler_roll\"] = roll\n",
        "    df[\"euler_pitch\"] = pitch\n",
        "    df[\"euler_yaw\"] = yaw\n",
        "\n",
        "    # Euler angle magnitudes and combinations\n",
        "    df[\"euler_total\"] = np.sqrt(roll**2 + pitch**2 + yaw**2)\n",
        "    df[\"pitch_roll_ratio\"] = pitch / (np.abs(roll) + CONFIG.ERR)\n",
        "    df[\"yaw_pitch_ratio\"] = yaw / (np.abs(pitch) + CONFIG.ERR)\n",
        "\n",
        "    return df\n",
        "\n",
        "def rotation_matrix_features(df):\n",
        "    \"\"\"Extract features from rotation matrices - captures 3D orientation relationships\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    # Convert quaternion to rotation matrix elements\n",
        "    # Rotation matrix gives us more detailed orientation information\n",
        "    rot_matrix_exprs = []\n",
        "\n",
        "    # Rotation matrix elements (3x3 matrix from quaternion)\n",
        "    # These capture specific orientation relationships\n",
        "    rot_matrix_exprs.extend([\n",
        "        # R11, R12, R13 (first row of rotation matrix)\n",
        "        (1 - 2*(pl.col('rot_y')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r11'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') - pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r12'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r13'),\n",
        "\n",
        "        # R21, R22, R23 (second row)\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') + pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r21'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r22'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r23'),\n",
        "\n",
        "        # R31, R32, R33 (third row)\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r31'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r32'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_y')**2)).alias('rot_matrix_r33'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(rot_matrix_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "def angular_velocity_features(df):\n",
        "    \"\"\"Derive angular velocity from quaternion derivatives\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    angular_vel_exprs = []\n",
        "\n",
        "    # Angular velocity from quaternion derivatives\n",
        "    # ω = 2 * q' * q*  (where q* is quaternion conjugate)\n",
        "    angular_vel_exprs.extend([\n",
        "        # Quaternion derivatives (time derivatives)\n",
        "        pl.col('rot_w').diff().over('sequence_id').alias('rot_w_dot'),\n",
        "        pl.col('rot_x').diff().over('sequence_id').alias('rot_x_dot'),\n",
        "        pl.col('rot_y').diff().over('sequence_id').alias('rot_y_dot'),\n",
        "        pl.col('rot_z').diff().over('sequence_id').alias('rot_z_dot'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_vel_exprs)\n",
        "\n",
        "    # Angular velocity components\n",
        "    angular_vel_components = []\n",
        "    angular_vel_components.extend([\n",
        "        # Angular velocity in body frame\n",
        "        (2 * (-pl.col('rot_x')*pl.col('rot_w_dot') + pl.col('rot_w')*pl.col('rot_x_dot') +\n",
        "              pl.col('rot_y')*pl.col('rot_z_dot') - pl.col('rot_z')*pl.col('rot_y_dot'))).alias('angular_vel_x'),\n",
        "        (2 * (-pl.col('rot_y')*pl.col('rot_w_dot') + pl.col('rot_w')*pl.col('rot_y_dot') +\n",
        "              pl.col('rot_z')*pl.col('rot_x_dot') - pl.col('rot_x')*pl.col('rot_z_dot'))).alias('angular_vel_y'),\n",
        "        (2 * (-pl.col('rot_z')*pl.col('rot_w_dot') + pl.col('rot_w')*pl.col('rot_z_dot') +\n",
        "              pl.col('rot_x')*pl.col('rot_y_dot') - pl.col('rot_y')*pl.col('rot_x_dot'))).alias('angular_vel_z'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_vel_components)\n",
        "\n",
        "    # Angular velocity derived features\n",
        "    angular_vel_derived = []\n",
        "    angular_vel_derived.extend([\n",
        "        # Angular speed (magnitude)\n",
        "        (pl.col('angular_vel_x')**2 + pl.col('angular_vel_y')**2 + pl.col('angular_vel_z')**2).sqrt().alias('angular_speed'),\n",
        "\n",
        "        # Angular acceleration\n",
        "        pl.col('angular_vel_x').diff().over('sequence_id').alias('angular_accel_x'),\n",
        "        pl.col('angular_vel_y').diff().over('sequence_id').alias('angular_accel_y'),\n",
        "        pl.col('angular_vel_z').diff().over('sequence_id').alias('angular_accel_z'),\n",
        "\n",
        "        # Dominant angular velocity axis\n",
        "        pl.max_horizontal([\n",
        "            pl.col('angular_vel_x').abs(),\n",
        "            pl.col('angular_vel_y').abs(),\n",
        "            pl.col('angular_vel_z').abs()\n",
        "        ]).alias('dominant_angular_vel'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_vel_derived)\n",
        "\n",
        "    angular_final = []\n",
        "    angular_final.extend([\n",
        "        # Angular acceleration magnitude\n",
        "        (pl.col('angular_accel_x')**2 + pl.col('angular_accel_y')**2 + pl.col('angular_accel_z')**2).sqrt().alias('angular_accel_magnitude'),\n",
        "\n",
        "        # Angular jerk (rate of change of angular acceleration)\n",
        "        pl.col('angular_accel_x').diff().over('sequence_id').alias('angular_jerk_x'),\n",
        "        pl.col('angular_accel_y').diff().over('sequence_id').alias('angular_jerk_y'),\n",
        "        pl.col('angular_accel_z').diff().over('sequence_id').alias('angular_jerk_z'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(angular_final)\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def temporal_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "        # Normalize sequence counter to 0-1 range per sequence\n",
        "    df = df.with_columns([\n",
        "        ((pl.col('sequence_counter') - pl.col('sequence_counter').min().over('sequence_id')) /\n",
        "         (pl.col('sequence_counter').max().over('sequence_id') - pl.col('sequence_counter').min().over('sequence_id') + CONFIG.ERR))\n",
        "        .alias('normalized_position')\n",
        "    ])\n",
        "\n",
        "\n",
        "    for col in imu_cols:\n",
        "        temporal_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').alias(f'{col}_velocity'),\n",
        "\n",
        "            (pl.col(col) / (pl.col(col).shift(1).over('sequence_id').abs() + CONFIG.ERR)).alias(f'{col}_pct_vs_prev'),\n",
        "            (pl.col(col) / (pl.col(col).shift(-1).over('sequence_id').abs() + CONFIG.ERR)).alias(f'{col}_pct_vs_next'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(temporal_exprs)\n",
        "\n",
        "    acc_exprs = []\n",
        "    for col in imu_cols:\n",
        "        acc_exprs.extend([\n",
        "            pl.col(col).diff(n=4).over('sequence_id').alias(f'{col}_snap'),\n",
        "            pl.col(col).diff(n=5).over('sequence_id').alias(f'{col}_crackle'),\n",
        "            pl.col(col).diff(n=6).over('sequence_id').alias(f'{col}_pop'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(acc_exprs)\n",
        "\n",
        "    peak_exprs = []\n",
        "    for col in imu_cols:\n",
        "        peak_exprs.extend([\n",
        "            # Peak: current value is significantly higher than neighbors (e.g., >10% higher)\n",
        "            ((pl.col(f'{col}_pct_vs_prev').abs() > 1.2) &\n",
        "             (pl.col(f'{col}_pct_vs_next').abs() > 1.2)).alias(f'{col}_is_peak'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(peak_exprs)\n",
        "\n",
        "    agg_temporal_exprs = []\n",
        "    for col in imu_cols:\n",
        "        agg_temporal_exprs.extend([\n",
        "            pl.col(f'{col}_velocity').abs().mean().over('sequence_id').alias(f'{col}_avg_velocity'),\n",
        "            pl.col(f'{col}_is_peak').sum().over('sequence_id').alias(f'{col}_peak_count'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(agg_temporal_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def position_features(df):\n",
        "    \"\"\"Features based on position within sequence\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    position_exprs = []\n",
        "    for col in imu_cols:\n",
        "        position_exprs.extend([\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_early_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid2_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid3_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_late_mean'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_early_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid2_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid3_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_late_std'),\n",
        "\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_early_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_mid_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_mid2_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_mid3_velocity_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().mean().over('sequence_id').alias(f'{col}_late_velocity_mean'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_early_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_mid_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_mid2_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_mid3_velocity_std'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(f'{col}_velocity')).abs().std().over('sequence_id').alias(f'{col}_late_velocity_std'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') < 0.2)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_early_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_mid_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_mid2_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_mid3_energy'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.8)\n",
        "              .then(pl.col(col).pow(2)).sum().over('sequence_id').alias(f'{col}_late_energy'),\n",
        "\n",
        "            pl.when(pl.col('normalized_position') >= 0.9)\n",
        "              .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_very_late_mean'),\n",
        "            pl.when(pl.col('normalized_position') >= 0.9)\n",
        "              .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_very_late_std'),\n",
        "\n",
        "          ])\n",
        "\n",
        "    df = df.with_columns(position_exprs)\n",
        "\n",
        "    position_ratio_exprs = []\n",
        "    for col in imu_cols:\n",
        "        position_ratio_exprs.extend([\n",
        "            (pl.col(f'{col}_early_mean') / pl.col(f'{col}_late_mean')+CONFIG.ERR).alias(f'{col}_early_late_mean_ratio'),\n",
        "            (pl.col(f'{col}_early_std') / pl.col(f'{col}_late_std')+CONFIG.ERR).alias(f'{col}_early_late_std_ratio'),\n",
        "            (pl.col(f'{col}_early_energy') / pl.col(f'{col}_late_energy')+CONFIG.ERR).alias(f'{col}_early_late_energy_ratio'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(position_ratio_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def zero_crossing_features(df):\n",
        "    \"\"\"\n",
        "    Creates and compares three different methods for identifying \"significant change\".\n",
        "\n",
        "    1.  vs_value: Change relative to the current signal value.\n",
        "    2.  vs_std: Change relative to the signal's standard deviation over the sequence.\n",
        "    3.  vs_mean: Change relative to the signal's mean over the sequence.\n",
        "\n",
        "    For each method, it generates a '{col}_flag_...' and a '{col}_rate_...' column.\n",
        "    \"\"\"\n",
        "    df_pl = pl.from_pandas(df)\n",
        "\n",
        "    # --- Step 1: Calculate prerequisites in a single pass ---\n",
        "    # We need diff, mean, and std for each sequence.\n",
        "    prereq_exprs = []\n",
        "    for col in imu_cols:\n",
        "        prereq_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').abs().alias(f'{col}_diff_abs'),\n",
        "            pl.col(col).mean().over('sequence_id').alias(f'{col}_mean'),\n",
        "            pl.col(col).std().over('sequence_id').alias(f'{col}_std'),\n",
        "\n",
        "            pl.col(col).filter(pl.col('normalized_position') >= 0.6)\n",
        "              .mean().over('sequence_id').alias(f'{col}_late_mean'),\n",
        "\n",
        "            pl.col(col).filter(pl.col('normalized_position') >= 0.6)\n",
        "              .std().over('sequence_id').alias(f'{col}_late_std'),\n",
        "\n",
        "        ])\n",
        "\n",
        "    df_pl = df_pl.with_columns(prereq_exprs)\n",
        "\n",
        "    threshold_exprs = []\n",
        "    for col in imu_cols:\n",
        "        late_phase_diffs = pl.col(f'{col}_diff_abs').filter(pl.col('normalized_position') >= 0.6)\n",
        "\n",
        "        threshold_exprs.extend([\n",
        "            late_phase_diffs.mean().over('sequence_id').alias(f'{col}_mean_of_diffs_threshold'),\n",
        "            late_phase_diffs.std().over('sequence_id').alias(f'{col}_std_of_diffs_threshold'),\n",
        "        ])\n",
        "\n",
        "    df_pl = df_pl.with_columns(threshold_exprs)\n",
        "\n",
        "    feature_exprs = []\n",
        "    for col in imu_cols:\n",
        "        # # Method 1: Relative to Current Value\n",
        "        # flag_vs_value = (pl.col(f'{col}_diff_abs') > pl.col(col).abs() * 0.1).cast(pl.Int32).alias(f'{col}_flag_vs_value')\n",
        "        # zero_crossing_rate = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_value).mean().over('sequence_id').alias(f'{col}_gesture_zero_crossing_rate')\n",
        "\n",
        "        # # Method 2: Relative to Standard Deviation\n",
        "        # flag_vs_std = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_std')).cast(pl.Int32).alias(f'{col}_flag_vs_std')\n",
        "        # zero_crossing_rate = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_std).mean().over('sequence_id').alias(f'{col}_gesture_zero_crossing_rate')\n",
        "\n",
        "        # Method 3: Relative to Mean\n",
        "        # flag_vs_mean = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_mean').abs() * 0.1).cast(pl.Int32).alias(f'{col}_flag_vs_mean')\n",
        "        # zero_crossing_rate = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_mean).mean().over('sequence_id').alias(f'{col}_gesture_zero_crossing_rate')\n",
        "\n",
        "        # flag_vs_local_late_mean = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_late_mean').abs() * 0.1).cast(pl.Int32)\n",
        "        # late_zero_crossing_rate_mean = pl.when(pl.col('normalized_position') > 0.6).then(flag_vs_local_late_mean).mean().over('sequence_id').alias(f'{col}_late_gesture_zero_crossing_rate_mean')\n",
        "\n",
        "        flag_vs_local_late_std = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_late_std')).cast(pl.Int32)\n",
        "        late_zero_crossing_rate_std = pl.when(pl.col('normalized_position') >= 0.6).then(flag_vs_local_late_std).mean().over('sequence_id').alias(f'{col}_late_gesture_zero_crossing_rate_std')\n",
        "\n",
        "        flag_is_dwelling = (pl.col(f'{col}_diff_abs') < pl.col(f'{col}_late_std')).cast(pl.Int32)\n",
        "        dwell_time = pl.when(pl.col('normalized_position') >= 0.6).then(flag_is_dwelling).sum().over('sequence_id').alias(f'{col}_dwell_time')\n",
        "\n",
        "\n",
        "        # flag_vs_mean_diff = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_mean_of_diffs_threshold')).cast(pl.Int32)\n",
        "        # rate_vs_mean_diff = pl.when(pl.col('normalized_position') >= 0.6).then(flag_vs_mean_diff).mean().over('sequence_id').alias(f'{col}_rate_vs_mean_diff')\n",
        "\n",
        "        # flag_vs_std_diff = (pl.col(f'{col}_diff_abs') > pl.col(f'{col}_std_of_diffs_threshold')).cast(pl.Int32)\n",
        "        # rate_vs_std_diff = pl.when(pl.col('normalized_position') >= 0.6).then(flag_vs_std_diff).mean().over('sequence_id').alias(f'{col}_rate_vs_std_diff')\n",
        "\n",
        "        feature_exprs.extend([\n",
        "            late_zero_crossing_rate_std,\n",
        "            # dwell_time,\n",
        "        ])\n",
        "\n",
        "    df_pl = df_pl.with_columns(feature_exprs)\n",
        "\n",
        "    return df_pl.to_pandas()\n",
        "\n",
        "\n",
        "def cross_axis_correlation_features(df):\n",
        "    \"\"\"Correlation and coordination between different axes\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    # Calculate correlations between axes within each sequence\n",
        "    corr_exprs = []\n",
        "\n",
        "    # Acceleration cross-correlations\n",
        "    axis_pairs = [('acc_x', 'acc_y'), ('acc_x', 'acc_z'), ('acc_y', 'acc_z')]\n",
        "    for axis1, axis2 in axis_pairs:\n",
        "        corr_exprs.append(\n",
        "            pl.corr(pl.col(axis1), pl.col(axis2)).over('sequence_id').alias(f'{axis1}_{axis2}_corr')\n",
        "        )\n",
        "\n",
        "    euler_pairs = [('euler_roll', 'euler_pitch'), ('euler_roll', 'euler_yaw'), ('euler_pitch', 'euler_yaw')]\n",
        "    for axis1, axis2 in euler_pairs:\n",
        "        corr_exprs.append(\n",
        "            pl.corr(pl.col(axis1), pl.col(axis2)).over('sequence_id').alias(f'{axis1}_{axis2}_corr')\n",
        "        )\n",
        "\n",
        "    df = df.with_columns(corr_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "# print(\"CASTING OBJECT TYPES\")\n",
        "# train = cast_to_object(train)\n",
        "\n",
        "# print(\"MAG FEATURES\")\n",
        "# train = mag_features(train)\n",
        "\n",
        "# print(\"ROTATION MATRIX FEATURES\")\n",
        "# train = rotation_matrix_features(train)\n",
        "\n",
        "# print(\"ANGULAR VELOCITY FEATURES\")\n",
        "# train = angular_velocity_features(train)\n",
        "\n",
        "# print(\"AGGREGATION FEATURES\")\n",
        "# train = aggregation_features(train)\n",
        "\n",
        "# print(\"TEMPORAL FEATURES\")\n",
        "# train = temporal_features(train)\n",
        "\n",
        "# print(\"POSITION FEATURES\")\n",
        "# train = position_features(train)\n",
        "\n",
        "# print(\"ZERO CROSSING RATES\")\n",
        "# train = zero_crossing_features(train)\n",
        "\n",
        "# print(\"CORR FEATURES\")\n",
        "# train = cross_axis_correlation_features(train)\n",
        "\n",
        "def apply_feature_engineering(df):\n",
        "    print(\"  Applying feature engineering...\")\n",
        "    df = cast_to_object(df)\n",
        "    df = mag_features(df)\n",
        "    df = rotation_matrix_features(df)\n",
        "    df = angular_velocity_features(df)\n",
        "    df = aggregation_features(df)\n",
        "    df = temporal_features(df)\n",
        "    df = position_features(df)\n",
        "    df = zero_crossing_features(df)\n",
        "    df = cross_axis_correlation_features(df)\n",
        "    return df\n",
        "\n",
        "train = apply_feature_engineering(train)\n",
        "# test = apply_feature_engineering(test)\n",
        "\n",
        "for i in imu_cols:\n",
        "  FEATURES.remove(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ul16-2QYoT"
      },
      "source": [
        "## TOF THM FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX524iTnxA5r"
      },
      "outputs": [],
      "source": [
        "# thm_cols = [\n",
        "#     \"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\",\n",
        "\n",
        "#     \"thm_12_diff\", \"thm_13_diff\", \"thm_14_diff\",\n",
        "#     \"thm_15_diff\", \"thm_23_diff\", \"thm_24_diff\", \"thm_25_diff\",\n",
        "#     \"thm_34_diff\", \"thm_35_diff\", \"thm_45_diff\",\n",
        "\n",
        "# ]\n",
        "\n",
        "# tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
        "\n",
        "# tof_diff_cols = [f\"tof_{i}{j}_mean_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]\n",
        "# tof_diff_cols += [f\"tof_{i}{j}_std_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRYrkG8sbfb7"
      },
      "outputs": [],
      "source": [
        "# train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-IITROb8keh"
      },
      "outputs": [],
      "source": [
        "# def thermopile_features(df):\n",
        "#     \"\"\"Extract features from thermopile sensors (temperature)\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     thm_diff_exprs = []\n",
        "#     thm_diff_exprs.extend([\n",
        "#         (pl.col('thm_1') - pl.col('thm_2')).abs().alias('thm_12_diff'),\n",
        "#         (pl.col('thm_1') - pl.col('thm_3')).abs().alias('thm_13_diff'),\n",
        "#         (pl.col('thm_1') - pl.col('thm_4')).abs().alias('thm_14_diff'),\n",
        "#         (pl.col('thm_1') - pl.col('thm_5')).abs().alias('thm_15_diff'),\n",
        "#         (pl.col('thm_2') - pl.col('thm_3')).abs().alias('thm_23_diff'),\n",
        "#         (pl.col('thm_2') - pl.col('thm_4')).abs().alias('thm_24_diff'),\n",
        "#         (pl.col('thm_2') - pl.col('thm_5')).abs().alias('thm_25_diff'),\n",
        "#         (pl.col('thm_3') - pl.col('thm_4')).abs().alias('thm_34_diff'),\n",
        "#         (pl.col('thm_3') - pl.col('thm_5')).abs().alias('thm_35_diff'),\n",
        "#         (pl.col('thm_4') - pl.col('thm_5')).abs().alias('thm_45_diff'),\n",
        "\n",
        "#     ])\n",
        "\n",
        "#     df = df.with_columns(thm_diff_exprs)\n",
        "\n",
        "#     thm_exprs = []\n",
        "#     for col in thm_cols:\n",
        "#         thm_exprs.extend([\n",
        "#             pl.col(col).mean().over('sequence_id').alias(f'{col}_seq_mean'),\n",
        "#             pl.col(col).std().over('sequence_id').alias(f'{col}_seq_std'),\n",
        "#             pl.col(col).min().over('sequence_id').alias(f'{col}_seq_min'),\n",
        "#             pl.col(col).max().over('sequence_id').alias(f'{col}_seq_max'),\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') < 0.2)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_early_temp_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid_temp_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid2_temp_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid3_temp_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_late_temp_mean'),\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') < 0.2)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_early_temp_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid_temp_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid2_temp_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid3_temp_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_late_temp_std'),\n",
        "\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') < 0.2)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_early_temp_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.2 , pl.col('normalized_position') < 0.4)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid_temp_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.4 , pl.col('normalized_position') < 0.6)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid2_temp_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid3_temp_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_late_temp_max'),\n",
        "\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col).diff().max().over('sequence_id').alias(f'{col}_late_max_heating_rate')),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col).diff().min().over('sequence_id').alias(f'{col}_late_max_cooling_rate')),\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col).diff()).mean().over('sequence_id').alias(f'{col}_late_temp_rate'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col).diff()).std().over('sequence_id').alias(f'{col}_late_temp_rate_std'),\n",
        "\n",
        "#             pl.col(col).diff().diff().over('sequence_id').alias(f'{col}_temp_acceleration'),\n",
        "\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(thm_exprs)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "\n",
        "# def tof_features(df):\n",
        "#     \"\"\"Extract features from time-of-flight sensors (proximity/distance)\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     tof_sensor_exprs = []\n",
        "\n",
        "#     for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         tof_sensor_exprs.extend([\n",
        "#             pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#               .list.mean().alias(f'tof_{sensor_idx}_mean_distance'),\n",
        "\n",
        "#             pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#               .list.std().alias(f'tof_{sensor_idx}_std_distance'),\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# def tof_cross_sensor_features(df):\n",
        "#     \"\"\"Cross-sensor ToF features similar to thermopile differences\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     # ToF sensor differences (proximity asymmetry)\n",
        "#     tof_diff_exprs = []\n",
        "#     tof_diff_exprs.extend([\n",
        "#         # Sensor pair differences (proximity asymmetry patterns)\n",
        "#         (pl.col('tof_1_mean_distance') - pl.col('tof_2_mean_distance')).abs().alias('tof_12_mean_diff'),\n",
        "#         (pl.col('tof_1_mean_distance') - pl.col('tof_3_mean_distance')).abs().alias('tof_13_mean_diff'),\n",
        "#         (pl.col('tof_1_mean_distance') - pl.col('tof_4_mean_distance')).abs().alias('tof_14_mean_diff'),\n",
        "#         (pl.col('tof_1_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_15_mean_diff'),\n",
        "#         (pl.col('tof_2_mean_distance') - pl.col('tof_3_mean_distance')).abs().alias('tof_23_mean_diff'),\n",
        "#         (pl.col('tof_2_mean_distance') - pl.col('tof_4_mean_distance')).abs().alias('tof_24_mean_diff'),\n",
        "#         (pl.col('tof_2_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_25_mean_diff'),\n",
        "#         (pl.col('tof_3_mean_distance') - pl.col('tof_4_mean_distance')).abs().alias('tof_34_mean_diff'),\n",
        "#         (pl.col('tof_3_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_35_mean_diff'),\n",
        "#         (pl.col('tof_4_mean_distance') - pl.col('tof_5_mean_distance')).abs().alias('tof_45_mean_diff'),\n",
        "#     ])\n",
        "\n",
        "#     df = df.with_columns(tof_diff_exprs)\n",
        "\n",
        "#     # Overall ToF patterns\n",
        "#     overall_tof_exprs = []\n",
        "#     overall_tof_exprs.extend([\n",
        "#         # Overall proximity signature\n",
        "#         ((pl.col('tof_1_mean_distance') + pl.col('tof_2_mean_distance') + pl.col('tof_3_mean_distance') +\n",
        "#           pl.col('tof_4_mean_distance') + pl.col('tof_5_mean_distance')) / 5).alias('tof_overall_mean_distance'),\n",
        "#     ])\n",
        "\n",
        "#     df = df.with_columns(overall_tof_exprs)\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# def tof_sequence_features(df):\n",
        "#     \"\"\"Sequence-level ToF features with phase analysis\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     # Get all ToF difference columns\n",
        "#     tof_diff_cols = [col for col in df.columns if 'tof_' in col and '_diff' in col]\n",
        "#     # tof_overall_cols = ['tof_overall_mean_distance', 'tof_closest_sensor_distance',\n",
        "#     #                    'tof_furthest_sensor_distance', 'tof_proximity_spread', 'tof_spatial_variance']\n",
        "\n",
        "#     seq_exprs = []\n",
        "\n",
        "#     # Basic sequence statistics for all ToF features\n",
        "#     for col in ['tof_1_mean_distance', 'tof_2_mean_distance', 'tof_3_mean_distance',\n",
        "#                 'tof_4_mean_distance', 'tof_5_mean_distance'] + tof_diff_cols:\n",
        "#         seq_exprs.extend([\n",
        "#             pl.col(col).mean().over('sequence_id').alias(f'{col}_seq_mean'),\n",
        "#             pl.col(col).std().over('sequence_id').alias(f'{col}_seq_std'),\n",
        "#             pl.col(col).min().over('sequence_id').alias(f'{col}_seq_min'),\n",
        "#             pl.col(col).max().over('sequence_id').alias(f'{col}_seq_max'),\n",
        "\n",
        "#             # Phase-specific features\n",
        "#             pl.when(pl.col('normalized_position') < 0.2)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_early_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid2_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_mid3_mean'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col)).mean().over('sequence_id').alias(f'{col}_late_mean'),\n",
        "\n",
        "#             # Late phase proximity dynamics (critical for BFRB)\n",
        "#             pl.when(pl.col('normalized_position') < 0.2)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_early_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid2_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_mid3_std'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col)).std().over('sequence_id').alias(f'{col}_late_std'),\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') < 0.2)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_early_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid2_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_mid3_max'),\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#               .then(pl.col(col)).max().over('sequence_id').alias(f'{col}_late_max'),\n",
        "\n",
        "\n",
        "#             # pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#             #   .then(pl.col(col)).min().over('sequence_id').alias(f'{col}_late_min'),\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(seq_exprs)\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# def tof_spatial_structure_features(df):\n",
        "#     \"\"\"Extract spatial structure features from 8x8 ToF grids\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     spatial_exprs = []\n",
        "\n",
        "#     for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "#         horizontal_edges = []\n",
        "#         vertical_edges = []\n",
        "\n",
        "#         # Edge detection features\n",
        "#         # Horizontal edges (difference between adjacent rows)\n",
        "#         for row in range(7):  # 7 transitions between 8 rows\n",
        "#             row_start = row * 8\n",
        "#             next_row_start = (row + 1) * 8\n",
        "\n",
        "#             for col in range(8):\n",
        "#                 pixel1 = f\"tof_{sensor_idx}_v{row_start + col}\"\n",
        "#                 pixel2 = f\"tof_{sensor_idx}_v{next_row_start + col}\"\n",
        "\n",
        "#                 horizontal_edges.append(\n",
        "#                     pl.when((pl.col(pixel1) != -1) & (pl.col(pixel2) != -1))\n",
        "#                     .then((pl.col(pixel1) - pl.col(pixel2)).abs())\n",
        "#                 )\n",
        "#         for row in range(8):\n",
        "#             for col in range(7):  # 7 transitions between 8 columns\n",
        "#                 pixel1 = f'tof_{sensor_idx}_v{row * 8 + col}'\n",
        "#                 pixel2 = f'tof_{sensor_idx}_v{row * 8 + col + 1}'\n",
        "\n",
        "#                 vertical_edges.append(\n",
        "#                     pl.when((pl.col(pixel1) != -1) & (pl.col(pixel2) != -1))\n",
        "#                     .then((pl.col(pixel1) - pl.col(pixel2)).abs())\n",
        "#                 )\n",
        "\n",
        "#         spatial_exprs.extend([\n",
        "#             # Overall edge strength over sequence\n",
        "#             pl.concat_list(horizontal_edges).list.mean().mean().over('sequence_id')\n",
        "#             .alias(f'tof_{sensor_idx}_horizontal_edge_strength_seq'),\n",
        "\n",
        "#             pl.concat_list(vertical_edges).list.mean().mean().over('sequence_id')\n",
        "#             .alias(f'tof_{sensor_idx}_vertical_edge_strength_seq'),\n",
        "\n",
        "#             # Edge strength in different phases\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#             .then(pl.concat_list(horizontal_edges).list.mean())\n",
        "#             .mean().over('sequence_id')\n",
        "#             .alias(f'tof_{sensor_idx}_horizontal_edge_strength_late'),\n",
        "\n",
        "#             pl.when(pl.col('normalized_position') >= 0.8)\n",
        "#             .then(pl.concat_list(vertical_edges).list.mean())\n",
        "#             .mean().over('sequence_id')\n",
        "#             .alias(f'tof_{sensor_idx}_vertical_edge_strength_late'),\n",
        "\n",
        "#                         # Center vs edge pixel activity\n",
        "#             pl.when((pl.col(f'tof_{sensor_idx}_v27') != -1) &\n",
        "#                    (pl.col(f'tof_{sensor_idx}_v28') != -1) &\n",
        "#                    (pl.col(f'tof_{sensor_idx}_v35') != -1) &\n",
        "#                    (pl.col(f'tof_{sensor_idx}_v36') != -1))\n",
        "#             .then((pl.col(f'tof_{sensor_idx}_v27') + pl.col(f'tof_{sensor_idx}_v28') +\n",
        "#                   pl.col(f'tof_{sensor_idx}_v35') + pl.col(f'tof_{sensor_idx}_v36')) / 4)\n",
        "#             .mean().over('sequence_id')\n",
        "#             .alias(f'tof_{sensor_idx}_center_pixels_seq_mean'),\n",
        "\n",
        "#             # Corner pixel activity\n",
        "#             pl.when((pl.col(f'tof_{sensor_idx}_v0') != -1) &\n",
        "#                    (pl.col(f'tof_{sensor_idx}_v7') != -1) &\n",
        "#                    (pl.col(f'tof_{sensor_idx}_v56') != -1) &\n",
        "#                    (pl.col(f'tof_{sensor_idx}_v63') != -1))\n",
        "#             .then((pl.col(f'tof_{sensor_idx}_v0') + pl.col(f'tof_{sensor_idx}_v7') +\n",
        "#                   pl.col(f'tof_{sensor_idx}_v56') + pl.col(f'tof_{sensor_idx}_v63')) / 4)\n",
        "#             .mean().over('sequence_id')\n",
        "#             .alias(f'tof_{sensor_idx}_corner_pixels_seq_mean'),\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(spatial_exprs)\n",
        "\n",
        "\n",
        "#     advanced_tof_exprs = []\n",
        "\n",
        "#     # Object detection and tracking features\n",
        "#     for sensor_idx in range(1, 6):\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         # Spatial pattern recognition (specific to 8x8 grid)\n",
        "#         # Blob detection (center pixels vs edge pixels)\n",
        "#         center_pixels = [f'tof_{sensor_idx}_v{i}' for i in [27, 28, 35, 36]]  # 2x2 center\n",
        "#         edge_pixels = [f'tof_{sensor_idx}_v{i}' for i in [0, 1, 2, 3, 4, 5, 6, 7,  # top row\n",
        "#                                                           56, 57, 58, 59, 60, 61, 62, 63,  # bottom row\n",
        "#                                                           8, 16, 24, 32, 40, 48,  # left column\n",
        "#                                                           15, 23, 31, 39, 47, 55]]  # right column\n",
        "\n",
        "#         advanced_tof_exprs.extend([\n",
        "#             # Center vs edge proximity ratio\n",
        "#             (pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col))\n",
        "#                             for col in center_pixels]).list.mean() /\n",
        "#              (pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col))\n",
        "#                              for col in edge_pixels]).list.mean() + 0.001))\n",
        "#             .alias(f'tof_{sensor_idx}_center_edge_ratio'),\n",
        "\n",
        "#             # Symmetry detection (left vs right, top vs bottom)\n",
        "#             # Left vs right symmetry\n",
        "#             (pl.concat_list([pl.when(pl.col(f'tof_{sensor_idx}_v{row*8 + col}') != -1)\n",
        "#                             .then(pl.col(f'tof_{sensor_idx}_v{row*8 + col}'))\n",
        "#                             for row in range(8) for col in range(4)]).list.mean() -\n",
        "#              pl.concat_list([pl.when(pl.col(f'tof_{sensor_idx}_v{row*8 + col}') != -1)\n",
        "#                             .then(pl.col(f'tof_{sensor_idx}_v{row*8 + col}'))\n",
        "#                             for row in range(8) for col in range(4, 8)]).list.mean()).abs()\n",
        "#             .alias(f'tof_{sensor_idx}_left_right_asymmetry'),\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(advanced_tof_exprs)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# # def cat_tof_regional_features_func(df, tof_mode=\"stats\", include_regions=True):\n",
        "# #     \"\"\"\n",
        "# #     Extract features from time-of-flight sensors with regional analysis\n",
        "\n",
        "# #     Args:\n",
        "# #         df: DataFrame with ToF data\n",
        "# #         tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "# #         include_regions: Whether to include regional analysis features\n",
        "# #     \"\"\"\n",
        "# #     df = pl.from_pandas(df)\n",
        "\n",
        "# #     tof_sensor_exprs = []\n",
        "\n",
        "# #     # Basic statistics for each ToF sensor (5 sensors total)\n",
        "# #     for sensor_idx in range(1, 6):\n",
        "# #         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "# #         # # Basic stats (replace -1 with null for proper statistics)\n",
        "# #         # tof_sensor_exprs.extend([\n",
        "# #         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "# #         #       .list.mean().alias(f'tof_{sensor_idx}_mean'),\n",
        "# #         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "# #         #       .list.std().alias(f'tof_{sensor_idx}_std'),\n",
        "# #         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "# #         #       .list.min().alias(f'tof_{sensor_idx}_min'),\n",
        "# #         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "# #         #       .list.max().alias(f'tof_{sensor_idx}_max'),\n",
        "# #         # ])\n",
        "\n",
        "# #         # Regional Analysis - divide 8x8 grid into regions\n",
        "# #         if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "# #             # Different region modes\n",
        "# #             region_modes = [4] if tof_mode == \"regions\" else [4]\n",
        "\n",
        "# #             for mode in region_modes:\n",
        "# #                 region_size = 64 // mode  # pixels per region\n",
        "\n",
        "# #                 for region_idx in range(mode):\n",
        "# #                     start_pixel = region_idx * region_size\n",
        "# #                     end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "# #                     # Get pixel columns for this region\n",
        "# #                     region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "\n",
        "# #                     # Calculate regional statistics\n",
        "# #                     tof_sensor_exprs.extend([\n",
        "# #                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "# #                           .list.mean().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mean'),\n",
        "# #                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "# #                           .list.std().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_std'),\n",
        "# #                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "# #                           .list.min().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_min'),\n",
        "# #                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "# #                           .list.max().alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_max'),\n",
        "\n",
        "# #                     ])\n",
        "\n",
        "# #     df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "# #     return df.to_pandas()\n",
        "\n",
        "\n",
        "# def cat_tof_regional_features_func(df, tof_mode=\"stats\", include_regions=True):\n",
        "#     \"\"\"\n",
        "#     Extract features from time-of-flight sensors with regional analysis across phases\n",
        "\n",
        "#     Args:\n",
        "#         df: DataFrame with ToF data\n",
        "#         tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "#         include_regions: Whether to include regional analysis features\n",
        "#     \"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     tof_sensor_exprs = []\n",
        "\n",
        "#     # Basic statistics for each ToF sensor (5 sensors total)\n",
        "#     for sensor_idx in range(1, 6):\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         # Regional Analysis - divide 8x8 grid into regions with phase analysis\n",
        "#         if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "#             # Different region modes\n",
        "#             region_modes = [4] if tof_mode == \"regions\" else [4]\n",
        "\n",
        "#             for mode in region_modes:\n",
        "#                 region_size = 64 // mode  # pixels per region\n",
        "\n",
        "#                 for region_idx in range(mode):\n",
        "#                     start_pixel = region_idx * region_size\n",
        "#                     end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "#                     # Get pixel columns for this region\n",
        "#                     region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "\n",
        "#                     # Create a single expression for this region's valid values\n",
        "#                     region_values_expr = pl.concat_list([\n",
        "#                         pl.when(pl.col(col) != -1).then(pl.col(col))\n",
        "#                         for col in region_pixel_cols\n",
        "#                     ])\n",
        "\n",
        "#                     # Phase-specific regional statistics (5 phases)\n",
        "#                     # Early phase (normalized_position < 0.2)\n",
        "#                     # tof_sensor_exprs.extend([\n",
        "#                     #     pl.when(pl.col('normalized_position') < 0.2)\n",
        "#                     #       .then(region_values_expr.list.mean())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_early_mean'),\n",
        "#                     #     pl.when(pl.col('normalized_position') < 0.2)\n",
        "#                     #       .then(region_values_expr.list.std())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_early_std'),\n",
        "#                     #     pl.when(pl.col('normalized_position') < 0.2)\n",
        "#                     #       .then(region_values_expr.list.max())\n",
        "#                     #       .max().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_early_max'),\n",
        "\n",
        "#                     # ])\n",
        "\n",
        "#                     # # Mid phase (0.2 <= normalized_position < 0.4)\n",
        "#                     # tof_sensor_exprs.extend([\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "#                     #       .then(region_values_expr.list.mean())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid_mean'),\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "#                     #       .then(region_values_expr.list.std())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid_std'),\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.2, pl.col('normalized_position') < 0.4)\n",
        "#                     #       .then(region_values_expr.list.max())\n",
        "#                     #       .max().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid_max'),\n",
        "#                     # ])\n",
        "\n",
        "#                     # # Mid2 phase (0.4 <= normalized_position < 0.6)\n",
        "#                     # tof_sensor_exprs.extend([\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "#                     #       .then(region_values_expr.list.mean())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid2_mean'),\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "#                     #       .then(region_values_expr.list.std())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid2_std'),\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.4, pl.col('normalized_position') < 0.6)\n",
        "#                     #       .then(region_values_expr.list.max())\n",
        "#                     #       .max().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid2_max'),\n",
        "#                     # ])\n",
        "\n",
        "#                     # # Mid3 phase (0.6 <= normalized_position < 0.8)\n",
        "#                     # tof_sensor_exprs.extend([\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#                     #       .then(region_values_expr.list.mean())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid3_mean'),\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#                     #       .then(region_values_expr.list.std())\n",
        "#                     #       .mean().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid3_std'),\n",
        "#                     #     pl.when(pl.col('normalized_position') >= 0.6, pl.col('normalized_position') < 0.8)\n",
        "#                     #       .then(region_values_expr.list.max())\n",
        "#                     #       .max().over('sequence_id')\n",
        "#                     #       .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_mid3_max'),\n",
        "#                     # ])\n",
        "\n",
        "#                     # Late phase (normalized_position >= 0.8)\n",
        "#                     tof_sensor_exprs.extend([\n",
        "#                         pl.when(pl.col('normalized_position') >= 0.6)\n",
        "#                           .then(region_values_expr.list.mean())\n",
        "#                           .mean().over('sequence_id')\n",
        "#                           .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_mean'),\n",
        "#                         pl.when(pl.col('normalized_position') >= 0.6)\n",
        "#                           .then(region_values_expr.list.std())\n",
        "#                           .mean().over('sequence_id')\n",
        "#                           .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_std'),\n",
        "#                         pl.when(pl.col('normalized_position') >= 0.6)\n",
        "#                           .then(region_values_expr.list.max())\n",
        "#                           .max().over('sequence_id')\n",
        "#                           .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_max'),\n",
        "#                         pl.when(pl.col('normalized_position') >= 0.6)\n",
        "#                           .then(region_values_expr.list.min())\n",
        "#                           .min().over('sequence_id')\n",
        "#                           .alias(f'tof_{mode}_{sensor_idx}_region_{region_idx}_late_in'),\n",
        "\n",
        "#                     ])\n",
        "\n",
        "#     df = df.with_columns(tof_sensor_exprs)\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# def cat_advanced_tof_features(df):\n",
        "#     \"\"\"Advanced time-of-flight feature engineering\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     tof_advanced_exprs = []\n",
        "\n",
        "#     for sensor_idx in range(1, 6):\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         # 2. Texture and pattern features\n",
        "#         tof_advanced_exprs.extend([\n",
        "#             # Local contrast (difference between max and min in local regions)\n",
        "#             (pl.max_horizontal([pl.col(col) for col in pixel_cols[:16]]) -\n",
        "#              pl.min_horizontal([pl.col(col) for col in pixel_cols[:16]])).alias(f'tof_{sensor_idx}_contrast_q1'),\n",
        "\n",
        "#             (pl.max_horizontal([pl.col(col) for col in pixel_cols[16:32]]) -\n",
        "#              pl.min_horizontal([pl.col(col) for col in pixel_cols[16:32]])).alias(f'tof_{sensor_idx}_contrast_q2'),\n",
        "\n",
        "#             (pl.max_horizontal([pl.col(col) for col in pixel_cols[32:48]]) -\n",
        "#              pl.min_horizontal([pl.col(col) for col in pixel_cols[32:48]])).alias(f'tof_{sensor_idx}_contrast_q3'),\n",
        "\n",
        "#             (pl.max_horizontal([pl.col(col) for col in pixel_cols[48:64]]) -\n",
        "#              pl.min_horizontal([pl.col(col) for col in pixel_cols[48:64]])).alias(f'tof_{sensor_idx}_contrast_q4'),\n",
        "\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(tof_advanced_exprs)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "\n",
        "# train = thermopile_features(train)\n",
        "\n",
        "# train = tof_features(train)\n",
        "# train = tof_cross_sensor_features(train)\n",
        "# train = tof_sequence_features(train)\n",
        "# train = tof_spatial_structure_features(train)\n",
        "# train = cat_tof_regional_features_func(train, tof_mode=\"regions\")\n",
        "# # train = cat_advanced_tof_features(train)\n",
        "\n",
        "\n",
        "\n",
        "# thm_feature_cols = [col for col in train.columns if 'thm_' in col and col not in thm_cols]\n",
        "# # tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "# tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "\n",
        "\n",
        "# FEATURES_FULL = FEATURES + thm_feature_cols + tof_feature_cols\n",
        "# print(len(FEATURES_FULL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJuAJBGWQWSk"
      },
      "source": [
        "## REDUCE MEMORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOxRa7Mt_5Mb"
      },
      "outputs": [],
      "source": [
        "# def reduce_mem_usage(df, verbose=True):\n",
        "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2 # calculate current memory usage\n",
        "\n",
        "#     for col in df.columns:\n",
        "#         col_type = df[col].dtype\n",
        "#         if col_type in numerics: # check if column is numeric\n",
        "#             c_min = df[col].min()\n",
        "#             c_max = df[col].max()\n",
        "#             if str(col_type).startswith('int'): # if integer\n",
        "#                 # Check if data can be safely cast to smaller int types\n",
        "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "#                     df[col] = df[col].astype(np.int8)\n",
        "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "#                     df[col] = df[col].astype(np.int16)\n",
        "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "#                     df[col] = df[col].astype(np.int32)\n",
        "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "#                     df[col] = df[col].astype(np.int64) # Should already be this or smaller if loaded as int\n",
        "#             else: # if float\n",
        "#                 # Check if data can be safely cast to float32 (float16 often loses too much precision)\n",
        "#                 if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "#                     df[col] = df[col].astype(np.float32)\n",
        "#                 # else: # If not, keep as float64\n",
        "#                 #     df[col] = df[col].astype(np.float64) # Already this type\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     if verbose:\n",
        "#         print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "#     return df\n",
        "\n",
        "# print(\"Reducing memory for train_df:\")\n",
        "# train = reduce_mem_usage(train)\n",
        "# print(\"\\nReducing memory for test_df:\")\n",
        "# test = reduce_mem_usage(test)\n",
        "\n",
        "# print(\"\\nTrain DataFrame info after memory reduction:\")\n",
        "# train.info(memory_usage='deep')\n",
        "# print(\"\\nTest DataFrame info after memory reduction:\")\n",
        "# test.info(memory_usage='deep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO30PQozOlbg"
      },
      "outputs": [],
      "source": [
        "# numeric_df = train[FEATURES]\n",
        "# corr = numeric_df.corr(method = 'pearson')\n",
        "# corr = corr.abs()\n",
        "# # corr.style.background_gradient(cmap='inferno')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdESwz1wQHwM"
      },
      "outputs": [],
      "source": [
        "# upper_tri_mask = np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
        "# upper_tri = corr.where(upper_tri_mask)\n",
        "# highly_correlated_series = upper_tri.stack()\n",
        "# strong_pairs = highly_correlated_series[highly_correlated_series > 0.90]\n",
        "# strong_pairs_df = strong_pairs.reset_index()\n",
        "# strong_pairs_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "# strong_pairs_df_sorted = strong_pairs_df.sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
        "# print(f\"Found {len(strong_pairs_df_sorted)} pairs of features with correlation > 0.90\")\n",
        "# print(\"-\" * 50)\n",
        "# print(strong_pairs_df_sorted.head(200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRTgtBm4smf9"
      },
      "outputs": [],
      "source": [
        "# def feature_mixup_for_catboost(X_train, y_train, alpha=0.3, mix_prob=0.5):\n",
        "#     \"\"\"Apply mixup to engineered features for CatBoost\"\"\"\n",
        "#     X_mixed = []\n",
        "#     y_mixed = []\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     for i in range(len(X_train)):\n",
        "#         if np.random.rand() < mix_prob:\n",
        "#             # Apply mixup\n",
        "#             j = np.random.randint(0, len(X_train))\n",
        "#             lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "#             # Mix features (this works for statistical features)\n",
        "#             x_mix = lam * X_train.iloc[i] + (1 - lam) * X_train.iloc[j]\n",
        "\n",
        "#             # For labels, use the dominant class (hard decision)\n",
        "#             if lam > 0.5:\n",
        "#                 y_mix = y_train.iloc[i]\n",
        "#             else:\n",
        "#                 y_mix = y_train.iloc[j]\n",
        "\n",
        "#             X_mixed.append(x_mix)\n",
        "#             y_mixed.append(y_mix)\n",
        "#         else:\n",
        "#             # Keep original\n",
        "#             X_mixed.append(X_train.iloc[i])\n",
        "#             y_mixed.append(y_train.iloc[i])\n",
        "\n",
        "    # return pd.DataFrame(X_mixed), pd.Series(y_mixed)\n",
        "\n",
        "def feature_mixup_for_catboost(X_train, y_train, alpha=0.3, mix_prob=1.0):\n",
        "    \"\"\"\n",
        "    Applies feature space mixup by creating new synthetic rows and adding them\n",
        "    to the original dataset.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Original feature data.\n",
        "        y_train (pd.Series): Original label data.\n",
        "        alpha (float): Beta distribution parameter for Mixup.\n",
        "        augmentation_factor (float): The fraction of new data to generate.\n",
        "                                     0.5 means increase dataset size by 50%.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    num_original_samples = len(X_train)\n",
        "    num_new_samples = int(num_original_samples * mix_prob)\n",
        "\n",
        "    if num_new_samples == 0:\n",
        "        return X_train, y_train\n",
        "\n",
        "    print(f\"Original samples: {num_original_samples}. Adding {num_new_samples} mixed samples.\")\n",
        "\n",
        "    # Get random pairs of indices to mix\n",
        "    indices1 = np.random.randint(0, num_original_samples, num_new_samples)\n",
        "    indices2 = np.random.randint(0, num_original_samples, num_new_samples)\n",
        "\n",
        "    # Generate all mixing weights at once\n",
        "    lams = np.random.beta(alpha, alpha, size=num_new_samples)\n",
        "\n",
        "    # --- Use vectorized operations for speed ---\n",
        "\n",
        "    # Mix features\n",
        "    X1 = X_train.iloc[indices1].values\n",
        "    X2 = X_train.iloc[indices2].values\n",
        "    X_new = lams[:, np.newaxis] * X1 + (1 - lams)[:, np.newaxis] * X2\n",
        "\n",
        "    # Mix labels (hard assignment)\n",
        "    y1 = y_train.iloc[indices1].values\n",
        "    y2 = y_train.iloc[indices2].values\n",
        "    y_new = np.where(lams > 0.5, y1, y2)\n",
        "\n",
        "    # Create a new DataFrame for the augmented data\n",
        "    X_augmented_df = pd.DataFrame(X_new, columns=X_train.columns)\n",
        "    y_augmented_series = pd.Series(y_new)\n",
        "\n",
        "    # Concatenate the original and the new data\n",
        "    X_final = pd.concat([X_train, X_augmented_df], ignore_index=True)\n",
        "    y_final = pd.concat([y_train, y_augmented_series], ignore_index=True)\n",
        "\n",
        "    return X_final, y_final\n",
        "\n",
        "\n",
        "# def feature_mixup_for_catboost(X_train, y_train, sequence_ids, alpha=0.3, mix_prob=0.5):\n",
        "#     \"\"\"\n",
        "#     Fast vectorized version of sequence-level mixup.\n",
        "\n",
        "#     Args:\n",
        "#         X_train (pd.DataFrame): Original feature data.\n",
        "#         y_train (pd.Series): Original label data.\n",
        "#         sequence_ids (pd.Series): Sequence ID for each row.\n",
        "#         alpha (float): Beta distribution parameter for Mixup.\n",
        "#         mix_prob (float): The fraction of new sequences to generate.\n",
        "#     \"\"\"\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     # Get unique sequences and their representative indices\n",
        "#     unique_sequences = sequence_ids.unique()\n",
        "#     num_original_sequences = len(unique_sequences)\n",
        "#     num_new_sequences = int(num_original_sequences * mix_prob)\n",
        "\n",
        "#     if num_new_sequences == 0:\n",
        "#         return X_train, y_train\n",
        "\n",
        "#     print(f\"Original sequences: {num_original_sequences}. Adding {num_new_sequences} mixed sequences.\")\n",
        "\n",
        "#     # Create mapping from sequence to first occurrence index (vectorized)\n",
        "#     first_occurrence_mask = ~sequence_ids.duplicated()\n",
        "#     representative_data = X_train[first_occurrence_mask].copy()\n",
        "#     representative_labels = y_train[first_occurrence_mask].copy()\n",
        "#     representative_sequences = sequence_ids[first_occurrence_mask].copy()\n",
        "\n",
        "#     # Create sequence to row count mapping (vectorized)\n",
        "#     sequence_counts = sequence_ids.value_counts()\n",
        "\n",
        "#     # Generate random pairs and mixing weights (all vectorized)\n",
        "#     seq_indices1 = np.random.randint(0, num_original_sequences, num_new_sequences)\n",
        "#     seq_indices2 = np.random.randint(0, num_original_sequences, num_new_sequences)\n",
        "#     lams = np.random.beta(alpha, alpha, size=num_new_sequences)\n",
        "\n",
        "#     # Get the actual sequence IDs for selected indices\n",
        "#     selected_seqs1 = representative_sequences.iloc[seq_indices1].values\n",
        "#     selected_seqs2 = representative_sequences.iloc[seq_indices2].values\n",
        "\n",
        "#     # Mix features (fully vectorized)\n",
        "#     X1 = representative_data.iloc[seq_indices1].values\n",
        "#     X2 = representative_data.iloc[seq_indices2].values\n",
        "#     X_mixed_base = lams[:, np.newaxis] * X1 + (1 - lams)[:, np.newaxis] * X2\n",
        "\n",
        "#     # Mix labels (vectorized hard assignment)\n",
        "#     y1 = representative_labels.iloc[seq_indices1].values\n",
        "#     y2 = representative_labels.iloc[seq_indices2].values\n",
        "#     y_mixed_base = np.where(lams > 0.5, y1, y2)\n",
        "\n",
        "#     # Calculate row counts for mixed sequences (vectorized)\n",
        "#     counts1 = sequence_counts[selected_seqs1].values\n",
        "#     counts2 = sequence_counts[selected_seqs2].values\n",
        "#     mixed_counts = ((counts1 + counts2) / 2).astype(int)\n",
        "\n",
        "#     # Calculate total rows needed\n",
        "#     total_new_rows = mixed_counts.sum()\n",
        "\n",
        "#     # Pre-allocate arrays for maximum speed\n",
        "#     X_new = np.empty((total_new_rows, X_train.shape[1]), dtype=X_train.dtypes.iloc[0])\n",
        "#     y_new = np.empty(total_new_rows, dtype=y_train.dtype)\n",
        "\n",
        "#     # Fill arrays using vectorized operations with repeat\n",
        "#     current_idx = 0\n",
        "#     for i, count in enumerate(mixed_counts):\n",
        "#         end_idx = current_idx + count\n",
        "#         # Use numpy repeat for each mixed sample\n",
        "#         X_new[current_idx:end_idx] = np.repeat(X_mixed_base[i:i+1], count, axis=0)\n",
        "#         y_new[current_idx:end_idx] = y_mixed_base[i]\n",
        "#         current_idx = end_idx\n",
        "\n",
        "#     # Convert to pandas (single operation)\n",
        "#     X_new_df = pd.DataFrame(X_new, columns=X_train.columns)\n",
        "#     y_new_series = pd.Series(y_new)\n",
        "\n",
        "#     # Concatenate (single operation)\n",
        "#     X_final = pd.concat([X_train, X_new_df], ignore_index=True)\n",
        "#     y_final = pd.concat([y_train, y_new_series], ignore_index=True)\n",
        "\n",
        "#     return X_final, y_final\n",
        "\n",
        "def generate_mixed_samples(df, features, target, n_mixed=1000):\n",
        "    \"\"\"Generate mixed samples for CatBoost training\"\"\"\n",
        "    mixed_samples = []\n",
        "\n",
        "    for _ in range(n_mixed):\n",
        "        # Sample two random sequences\n",
        "        idx1, idx2 = np.random.choice(len(df), 2, replace=False)\n",
        "        lam = np.random.beta(0.3, 0.3)\n",
        "\n",
        "        # Mix features\n",
        "        mixed_features = lam * df.iloc[idx1][features] + (1 - lam) * df.iloc[idx2][features]\n",
        "\n",
        "        # Choose dominant label\n",
        "        mixed_label = df.iloc[idx1][target] if lam > 0.5 else df.iloc[idx2][target]\n",
        "\n",
        "        # Create mixed sample\n",
        "        mixed_sample = mixed_features.copy()\n",
        "        mixed_sample[target] = mixed_label\n",
        "        mixed_sample['sequence_id'] = f\"mixed_{len(mixed_samples)}\"\n",
        "\n",
        "        mixed_samples.append(mixed_sample)\n",
        "\n",
        "    return pd.DataFrame(mixed_samples)\n",
        "\n",
        "\n",
        "# def mix_time_series_sequences(seq1, seq2, lam):\n",
        "#     \"\"\"Mix two time series sequences\"\"\"\n",
        "#     # Ensure same length\n",
        "#     min_len = min(len(seq1), len(seq2))\n",
        "#     seq1_cut = seq1[:min_len]\n",
        "#     seq2_cut = seq2[:min_len]\n",
        "\n",
        "#     # Linear interpolation\n",
        "#     mixed_seq = lam * seq1_cut + (1 - lam) * seq2_cut\n",
        "\n",
        "#     return mixed_seq\n",
        "\n",
        "# def create_mixed_sequences(df, n_mixed=500):\n",
        "#     \"\"\"Create mixed sequences, then extract features\"\"\"\n",
        "#     mixed_data = []\n",
        "\n",
        "#     sequence_ids = df['sequence_id'].unique()\n",
        "\n",
        "#     for _ in range(n_mixed):\n",
        "#         # Sample two sequences\n",
        "#         seq_id1, seq_id2 = np.random.choice(sequence_ids, 2, replace=False)\n",
        "#         seq1_data = df[df['sequence_id'] == seq_id1]\n",
        "#         seq2_data = df[df['sequence_id'] == seq_id2]\n",
        "\n",
        "#         lam = np.random.beta(0.3, 0.3)\n",
        "\n",
        "#         # Mix sensor readings\n",
        "#         mixed_features = {}\n",
        "#         for col in ['acc_x', 'acc_y', 'acc_z', 'euler_roll', 'euler_pitch', 'euler_yaw']:\n",
        "#             if col in df.columns:\n",
        "#                 mixed_col = lam * seq1_data[col].values + (1 - lam) * seq2_data[col].values\n",
        "#                 mixed_features[col] = mixed_col\n",
        "\n",
        "#         # Extract features from mixed sequence\n",
        "#         mixed_seq_features = extract_sequence_features(mixed_features)\n",
        "\n",
        "#         # Choose dominant label\n",
        "#         label = seq1_data['gesture'].iloc[0] if lam > 0.5 else seq2_data['gesture'].iloc[0]\n",
        "#         mixed_seq_features['gesture'] = label\n",
        "\n",
        "#         mixed_data.append(mixed_seq_features)\n",
        "\n",
        "#     return pd.DataFrame(mixed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJYVNzGqBS63"
      },
      "source": [
        "## METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQRvcrT_5Mb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hierarchical macro F1 metric for the CMI 2025 Challenge.\n",
        "\n",
        "This script defines a single entry point `score(solution, submission, row_id_column_name)`\n",
        "that the Kaggle metrics orchestrator will call.\n",
        "It performs validation on submission IDs and computes a combined binary & multiclass F1 score.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.target_gestures = [\n",
        "            'Above ear - pull hair',\n",
        "            'Cheek - pinch skin',\n",
        "            'Eyebrow - pull hair',\n",
        "            'Eyelash - pull hair',\n",
        "            'Forehead - pull hairline',\n",
        "            'Forehead - scratch',\n",
        "            'Neck - pinch skin',\n",
        "            'Neck - scratch',\n",
        "        ]\n",
        "        self.non_target_gestures = [\n",
        "            'Write name on leg',\n",
        "            'Wave hello',\n",
        "            'Glasses on/off',\n",
        "            'Text on phone',\n",
        "            'Write name in air',\n",
        "            'Feel around in tray and pull out an object',\n",
        "            'Scratch knee/leg skin',\n",
        "            'Pull air toward your face',\n",
        "            'Drink from bottle/cup',\n",
        "            'Pinch knee/leg skin'\n",
        "        ]\n",
        "        self.all_classes = self.target_gestures + self.non_target_gestures\n",
        "\n",
        "    def calculate_hierarchical_f1(\n",
        "        self,\n",
        "        sol: pd.DataFrame,\n",
        "        sub: pd.DataFrame\n",
        "    ) -> float:\n",
        "\n",
        "        # Validate gestures\n",
        "        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n",
        "        if invalid_types:\n",
        "            raise ParticipantVisibleError(\n",
        "                f\"Invalid gesture values in submission: {invalid_types}\"\n",
        "            )\n",
        "\n",
        "        # Compute binary F1 (Target vs Non-Target)\n",
        "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
        "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
        "        f1_binary = f1_score(\n",
        "            y_true_bin,\n",
        "            y_pred_bin,\n",
        "            pos_label=True,\n",
        "            zero_division=0,\n",
        "            average='binary'\n",
        "        )\n",
        "\n",
        "        # Build multi-class labels for gestures\n",
        "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "\n",
        "        # Compute macro F1 over all gesture classes\n",
        "        f1_macro = f1_score(\n",
        "            y_true_mc,\n",
        "            y_pred_mc,\n",
        "            average='macro',\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f'f1_binary score: {f1_binary}')\n",
        "        print(f'f1_macro score: {f1_macro}')\n",
        "\n",
        "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
        "\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute hierarchical macro F1 for the CMI 2025 challenge.\n",
        "\n",
        "    Expected input:\n",
        "      - solution and submission as pandas.DataFrame\n",
        "      - Column 'sequence_id': unique identifier for each sequence\n",
        "      - 'gesture': one of the eight target gestures or \"Non-Target\"\n",
        "\n",
        "    This metric averages:\n",
        "    1. Binary F1 on SequenceType (Target vs Non-Target)\n",
        "    2. Macro F1 on gesture (mapping non-targets to \"Non-Target\")\n",
        "\n",
        "    Raises ParticipantVisibleError for invalid submissions,\n",
        "    including invalid SequenceType or gesture values.\n",
        "\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> row_id_column_name = \"id\"\n",
        "    >>> solution = pd.DataFrame({'id': range(4), 'gesture': ['Eyebrow - pull hair']*4})\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Forehead - pull hairline']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.5\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Text on phone']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.0\n",
        "    >>> score(solution, solution, row_id_column_name=row_id_column_name)\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    # Validate required columns\n",
        "    for col in (row_id_column_name, 'gesture'):\n",
        "        if col not in solution.columns:\n",
        "            raise ParticipantVisibleError(f\"Solution file missing required column: '{col}'\")\n",
        "        if col not in submission.columns:\n",
        "            raise ParticipantVisibleError(f\"Submission file missing required column: '{col}'\")\n",
        "\n",
        "    metric = CompetitionMetric()\n",
        "    return metric.calculate_hierarchical_f1(solution, submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOYll9ZrZa93"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mThD-7pjoHUL"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "le = LabelEncoder()\n",
        "train['gesture'] = le.fit_transform(train['gesture'])\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "metric_calculator = CompetitionMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOZq-q6Xm0If",
        "outputId": "90540ba3-44e3-42b4-b428-dcb1f427eaf2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import uuid\n",
        "run_id  = uuid.uuid4()\n",
        "\n",
        "os.makedirs('models_full', exist_ok=True)\n",
        "n_splits=5\n",
        "\n",
        "t_d = train_demographics\n",
        "skf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(\n",
        "        skf.split(t_d, t_d[['adult_child', 'handedness', 'sex']])\n",
        "    ):\n",
        "    t_d.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "t_d['fold'] = t_d['fold'].astype(int)\n",
        "print(\"Demographics fold distribution:\\n\", t_d['fold'].value_counts(), \"\\n\")\n",
        "\n",
        "train = train.merge(t_d[['subject', 'fold']], on='subject', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWDITMDpoJbX"
      },
      "outputs": [],
      "source": [
        "\n",
        "realmlp_params = {\n",
        "    'n_cv'                : 4,\n",
        "    'n_epochs'            : 25,\n",
        "    'batch_size'          : 1024*4,\n",
        "    'verbosity'           : 2,\n",
        "    'lr'                  : 0.03,\n",
        "    'lr_sched'            : 'quad',\n",
        "    'p_drop'              : 0.2,\n",
        "    # 'max_one_hot_cat_size': 64,\n",
        "    # 'embedding_size'      : 256,\n",
        "    'device'              : 'cuda:0',\n",
        "    # 'tfms'                : [\"l1_normalize\"],\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_72ezcSRuAP"
      },
      "outputs": [],
      "source": [
        "import cloudpickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzMTA75hZqSo",
        "outputId": "e1f08165-4709-4d03-838b-f9147c800303"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "oof_preds_cat              = np.zeros(len(train), dtype=int)\n",
        "oof_preds_proba_cat        = np.zeros((len(train), CONFIG.NUM_CLASSES))\n",
        "oof_scores                 = []\n",
        "all_fold_importances       = []\n",
        "\n",
        "metric_calculator = CompetitionMetric()\n",
        "\n",
        "\n",
        "for fold in range(n_splits):\n",
        "    print(f\"{'#'*10} Fold {fold+1} {'#'*10}\")\n",
        "\n",
        "    train_idx = train.index[train['fold'] != fold].tolist()\n",
        "    valid_idx = train.index[train['fold'] == fold].tolist()\n",
        "\n",
        "    X_train = train.loc[train_idx, FEATURES].copy()\n",
        "    y_train = train.loc[train_idx, CONFIG.TARGET].copy()\n",
        "    sequence_ids = train.loc[train_idx, 'sequence_id'].copy()\n",
        "\n",
        "    if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n",
        "      print(f\"WARNING: Found NaN/Inf values in input data!\")\n",
        "      X_train.fillna(0, inplace=True)\n",
        "\n",
        "    # X_train = train.loc[train_idx, FEATURES].copy()\n",
        "    # y_train = train.loc[train_idx, CONFIG.TARGET].copy()\n",
        "\n",
        "    print(f\"Shape before Mixup: {X_train.shape}\")\n",
        "    print(f\"Valid shape before Mixup: {len(valid_idx)}\")\n",
        "\n",
        "    # X_train, y_train = feature_mixup_for_catboost(X_train_orig,\n",
        "    #                                               y_train_orig,\n",
        "    #                                               alpha=0.3, mix_prob=0.5)\n",
        "\n",
        "    print(f\"Shape after Mixup: {X_train.shape}\")\n",
        "\n",
        "    X_valid = train.loc[valid_idx, FEATURES].copy() #\n",
        "    y_valid = train.loc[valid_idx, CONFIG.TARGET].copy() #\n",
        "\n",
        "    print(f\"  X_train shape: {X_train.shape}, X_valid shape: {X_valid.shape}\")\n",
        "\n",
        "    if np.any(np.isnan(X_valid)) or np.any(np.isinf(X_valid)):\n",
        "        print(f\"WARNING: Found NaN/Inf values in input data!\")\n",
        "        X_valid.fillna(0, inplace=True)\n",
        "\n",
        "    model = RealMLP_TD_Classifier(**realmlp_params, random_state=42)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "    )\n",
        "\n",
        "    with open(f\"models_full/realmlp_fold_{fold+1}.pkl\", \"wb\") as f:\n",
        "        cloudpickle.dump(model, f)\n",
        "\n",
        "\n",
        "    # # joblib.dump(model, f\"/content/drive/MyDrive/cmi2025/models/realmlp_fold_{fold+1}_{run_id}.pkl\")\n",
        "    # joblib.dump(model, f\"models_full/realmlp_fold_{fold+1}.pkl\")\n",
        "\n",
        "    # all_fold_importances.append(model.get_feature_importance())\n",
        "\n",
        "    fold_proba = model.predict_proba(X_valid)\n",
        "    oof_preds_proba_cat[valid_idx] = fold_proba\n",
        "    fold_preds = np.argmax(fold_proba, axis=1)\n",
        "\n",
        "    y_valid_orig = le.inverse_transform(y_valid)\n",
        "    preds_orig   = le.inverse_transform(fold_preds)\n",
        "\n",
        "    temp_sol_df = pd.DataFrame({\"gesture\": y_valid_orig})\n",
        "    temp_sub_df = pd.DataFrame({\"gesture\": preds_orig})\n",
        "    fold_score  = metric_calculator.calculate_hierarchical_f1(temp_sol_df, temp_sub_df)\n",
        "\n",
        "    oof_scores.append(fold_score)\n",
        "    print(f\"  Fold {fold+1} Score: {fold_score}\\n\")\n",
        "\n",
        "print(f\"Mean OOF Score: {np.mean(oof_scores):.4f}\")\n",
        "print(f\"Std  OOF Score: {np.std(oof_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc7H0k-6atM-",
        "outputId": "381ef26e-7206-4b93-ad54-a2ec67ef9acf"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"Mean OOF Score: {np.mean(oof_scores):.4f}\")\n",
        "print(f\"Std  OOF Score: {np.std(oof_scores):.4f}\\n\")\n",
        "\n",
        "# === Final overall OOF calculation ===\n",
        "\n",
        "# 1. Recover original labels and OOF predictions\n",
        "original_labels = le.inverse_transform(train[CONFIG.TARGET])\n",
        "oof_preds_encoded = np.argmax(oof_preds_proba_cat, axis=1)\n",
        "oof_preds_original = le.inverse_transform(oof_preds_encoded)\n",
        "\n",
        "# 2. Build DataFrames for metric\n",
        "sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "sub_df = pd.DataFrame({\"gesture\": oof_preds_original})\n",
        "\n",
        "np.save('oof_preds_cat.npy', oof_preds_original)\n",
        "np.save('oof_preds_proba_cat.npy', oof_preds_proba_cat)\n",
        "\n",
        "# 3. Compute and print overall hierarchical F1\n",
        "overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "print(f\"Overall OOF Score: {overall_oof:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBQ_HXL-dgFX"
      },
      "outputs": [],
      "source": [
        "# Mean OOF Score: 0.7596\n",
        "# Std  OOF Score: 0.0059\n",
        "\n",
        "# f1_binary score: 0.9742198377349963\n",
        "# f1_macro score: 0.5457176742806236\n",
        "# Overall OOF Score: 0.7600"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0db3054994a2469fa6d39de78bf032e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "18c3186e0d294ef59a8c2cf9a5ad4991": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f332b5f88077416f9e688e9a49a4c591",
            "placeholder": "​",
            "style": "IPY_MODEL_537d4965a0e6456b8a7e0cdff51126cf",
            "value": ""
          }
        },
        "1f3d59b133b447fdbd78453cb755297f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516295f52abc4c4d9ca67651de81e91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe78b18b0d224a42b6bbc75144d6bee9",
            "placeholder": "​",
            "style": "IPY_MODEL_f90cdb2773a84aa7b1ae5aa4daf884ef",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "537d4965a0e6456b8a7e0cdff51126cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58eb998bb7284c3fbe7d120367985b72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f52edc9ddc34f8181474d226b6adecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3d59b133b447fdbd78453cb755297f",
            "placeholder": "​",
            "style": "IPY_MODEL_af83b654457641ddb536897ab0760bdc",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "8b6b85a1fc174486a1204db5a048b52d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9a5849f0ee49459a80e0b1e54cf7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "af83b654457641ddb536897ab0760bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0a5c5e0e65b4c1cb794b358e466c1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_58eb998bb7284c3fbe7d120367985b72",
            "placeholder": "​",
            "style": "IPY_MODEL_dac10dc8c9e542cd9f087b57ab127198",
            "value": ""
          }
        },
        "c028666291d54038ade2fe51bf2f66c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8b6b85a1fc174486a1204db5a048b52d",
            "style": "IPY_MODEL_0db3054994a2469fa6d39de78bf032e5",
            "tooltip": ""
          }
        },
        "c59a8c92799043908ef9b5d122892277": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_516295f52abc4c4d9ca67651de81e91a",
              "IPY_MODEL_b0a5c5e0e65b4c1cb794b358e466c1e4",
              "IPY_MODEL_18c3186e0d294ef59a8c2cf9a5ad4991",
              "IPY_MODEL_c028666291d54038ade2fe51bf2f66c5",
              "IPY_MODEL_6f52edc9ddc34f8181474d226b6adecc"
            ],
            "layout": "IPY_MODEL_8c9a5849f0ee49459a80e0b1e54cf7e4"
          }
        },
        "dac10dc8c9e542cd9f087b57ab127198": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f332b5f88077416f9e688e9a49a4c591": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f90cdb2773a84aa7b1ae5aa4daf884ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe78b18b0d224a42b6bbc75144d6bee9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
