{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFwooMNpVvrN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPH1bbFZz7V",
        "outputId": "3d8f69b5-1c19-4876-91db-f2622bec4b8e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "2259b654eec94d069520f16cac24309f",
            "77a862d263554f99ac0f14d7dfd9f843",
            "9343bf16eb9045429c062cf6cddd6c93",
            "2d9a8ce6b341495a841e80e860d6f643",
            "4d7300ff1d374cb08af241b78eb85b58",
            "3fc962878ac24cfd8563532614876248",
            "7c4921bd8f424e518142dad349a009a7",
            "8aa908f4807c41daa36c4f8ece24da6b",
            "93f74a8a9f454852ba6280b6b19edc9c",
            "e6c3a7f615dc4ac190162e5a9d29d864",
            "2d4a57ba796f4c3fb03d38dbf39fc2fd",
            "4a2c65e9f9f7492397ac57355a2533f0",
            "e030150d66294b4cbe907a4e88954a63",
            "b7f6a55bd73949fab49e0cd5ee38cd0f",
            "c8c9d059fbad438bb4ea3c06ee81fed1",
            "fd2b9177cf6842e688a5965b3ae3665a",
            "6b4a29d0d0ce4b628d9eeafa1c8e11be"
          ]
        },
        "id": "ka_oHwl5_5MN",
        "outputId": "9bf034dc-aa7e-4554-a563-edb799e77dea"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_g6C36S_5MR",
        "outputId": "15ea2ceb-aa56-497f-ac2e-78efb04af223"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "cmi_detect_behavior_with_sensor_data_path = kagglehub.competition_download('cmi-detect-behavior-with-sensor-data')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7jDg6tfTvdp",
        "outputId": "87e4264b-3cc3-4528-cae8-e7836c6f49fa"
      },
      "outputs": [],
      "source": [
        "!pip install iterative-stratification==0.1.7 -qq\n",
        "!pip install transformers==4.51.3 -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rYRqCy_5MT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import polars as pl\n",
        "import sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 2000)\n",
        "pd.set_option('display.max_rows', 2000)\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8MqxrueedN6",
        "outputId": "94b1aded-fdc9-41a5-f558-aaeebece1d2c"
      },
      "outputs": [],
      "source": [
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(pl.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(joblib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "gY6LM36r_5MW",
        "outputId": "e096ae59-6258-482b-a350-fa8614a58cd3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "train = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train.csv')\n",
        "test = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test.csv')\n",
        "train_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train_demographics.csv')\n",
        "test_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test_demographics.csv')\n",
        "\n",
        "train = train.to_pandas()\n",
        "test = test.to_pandas()\n",
        "train_demographics = train_demographics.to_pandas()\n",
        "test_demographics = test_demographics.to_pandas()\n",
        "\n",
        "train = pd.merge(train, train_demographics, on='subject', how='left')\n",
        "test = pd.merge(test, test_demographics, on='subject', how='left')\n",
        "\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STB7TmHka7dy",
        "outputId": "840757c4-6e05-4c32-f7e1-06019bbb4f41"
      },
      "outputs": [],
      "source": [
        "train.gesture.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ic-BSLy1d3"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "n-jo10ZXfIVQ",
        "outputId": "ad4f9af7-c2fe-457d-c4c7-4371445bfaad"
      },
      "outputs": [],
      "source": [
        "train_demographics.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaWmMuIS_5MZ"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "  TARGET = \"gesture\"\n",
        "  SUBJECT = \"subject\"\n",
        "  TRAIN_ONLY_COLS = ['sequence_type', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
        "  NUM_CLASSES = train.gesture.nunique()\n",
        "  FOLDS = 5\n",
        "  ERR = 1e-8\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "imu_cols = [\n",
        "            \"acc_x\", \"acc_y\", \"acc_z\",\n",
        "            \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
        "            \"acc_mag\",\n",
        "\n",
        "            \"euler_roll\", \"euler_pitch\", \"euler_yaw\",\n",
        "            \"euler_total\", \"pitch_roll_ratio\", \"yaw_pitch_ratio\",\n",
        "\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "\n",
        "            \"angular_jerk_x\", \"angular_jerk_y\", \"angular_jerk_z\",\n",
        "            ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7nJrmq5sdv"
      },
      "source": [
        "## FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTDoFvXb_5Ma",
        "outputId": "8fc1ae53-8c45-44d0-fb7d-bd96612b5c7d"
      },
      "outputs": [],
      "source": [
        "def cast_to_object(df):\n",
        "  df['adult_child'] = df['adult_child'].astype(\"category\")\n",
        "  df['sex'] = df['sex'].astype(\"category\")\n",
        "  df['handedness'] = df['handedness'].astype(\"category\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "\n",
        "    if isinstance(acc_data, pd.DataFrame):\n",
        "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    else:\n",
        "        acc_values = acc_data\n",
        "\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = acc_values.shape[0]\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "\n",
        "    gravity_world = np.array([0, 0, 9.81])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "             linear_accel[i, :] = acc_values[i, :]\n",
        "\n",
        "    return linear_accel\n",
        "\n",
        "\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_vel = np.zeros((num_samples, 3))\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q_t = quat_values[i]\n",
        "        q_t_plus_dt = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            \n",
        "            pass\n",
        "\n",
        "    return angular_vel\n",
        "\n",
        "def calculate_angular_distance(rot_data):\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_dist = np.zeros(num_samples)\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q1 = quat_values[i]\n",
        "        q2 = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
        "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
        "            angular_dist[i] = 0 \n",
        "            continue\n",
        "        try:\n",
        "            \n",
        "            r1 = R.from_quat(q1)\n",
        "            r2 = R.from_quat(q2)\n",
        "\n",
        "            relative_rotation = r1.inv() * r2\n",
        "\n",
        "\n",
        "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
        "            angular_dist[i] = angle\n",
        "        except ValueError:\n",
        "            angular_dist[i] = 0\n",
        "            pass\n",
        "\n",
        "    return angular_dist\n",
        "\n",
        "def calc_angular_velocity(df):\n",
        "    res = calculate_angular_velocity_from_quat( df[['rot_x', 'rot_y', 'rot_z', 'rot_w']] )\n",
        "    res = pd.DataFrame(res, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=df.index)\n",
        "    return res\n",
        "\n",
        "angular_velocity_df = train.groupby('sequence_id').apply(calc_angular_velocity, include_groups=False)\n",
        "angular_velocity_df = angular_velocity_df.droplevel('sequence_id')\n",
        "train = train.join(angular_velocity_df)\n",
        "\n",
        "def calc_angular_distance(df):\n",
        "    res = calculate_angular_distance(df[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
        "    res = pd.DataFrame(res, columns=['angular_distance'], index=df.index)\n",
        "    return res\n",
        "\n",
        "angular_distance_df = train.groupby('sequence_id').apply(calc_angular_distance, include_groups=False)\n",
        "angular_distance_df = angular_distance_df.droplevel('sequence_id')\n",
        "train = train.join(angular_distance_df)\n",
        "\n",
        "def quaternion_to_euler(w, x, y, z):\n",
        "    \"\"\"Convert quaternion to Euler angles\"\"\"\n",
        "    sinr_cosp = 2 * (w * x + y * z)\n",
        "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
        "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
        "\n",
        "    sinp = 2 * (w * y - z * x)\n",
        "    pitch = np.where(np.abs(sinp) >= 1, np.copysign(np.pi / 2, sinp), np.arcsin(sinp))\n",
        "\n",
        "    siny_cosp = 2 * (w * z + x * y)\n",
        "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "\n",
        "    return roll, pitch, yaw\n",
        "\n",
        "\n",
        "def mag_features(df):\n",
        "\n",
        "    df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"]**2 + df[\"acc_y\"]**2 + df[\"acc_z\"]**2)\n",
        "    df[\"rot_mag\"] = np.sqrt(df[\"rot_x\"]**2 + df[\"rot_y\"]**2 + df[\"rot_z\"]**2)\n",
        "\n",
        "    linear_acc = remove_gravity_from_acc(\n",
        "            df[['acc_x', 'acc_y', 'acc_z']],\n",
        "            df[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "        )\n",
        "    df['linear_acc_x'] = linear_acc[:, 0]\n",
        "    df['linear_acc_y'] = linear_acc[:, 1]\n",
        "    df['linear_acc_z'] = linear_acc[:, 2]\n",
        "\n",
        "    if 'linear_acc_x' in df.columns:\n",
        "        df[\"linear_acc_mag\"] = np.sqrt(df[\"linear_acc_x\"]**2 + df[\"linear_acc_y\"]**2 + df[\"linear_acc_z\"]**2)\n",
        "\n",
        "    # Angular velocity magnitude\n",
        "    if 'angular_vel_x' in df.columns:\n",
        "        df[\"angular_vel_mag\"] = np.sqrt(df[\"angular_vel_x\"]**2 + df[\"angular_vel_y\"]**2 + df[\"angular_vel_z\"]**2)\n",
        "\n",
        "    roll, pitch, yaw = quaternion_to_euler(df[\"rot_w\"], df[\"rot_x\"], df[\"rot_y\"], df[\"rot_z\"])\n",
        "    df[\"euler_roll\"] = roll\n",
        "    df[\"euler_pitch\"] = pitch\n",
        "    df[\"euler_yaw\"] = yaw\n",
        "    df[\"euler_mag\"] = np.sqrt(roll**2 + pitch**2 + yaw**2)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def temporal_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "        # Normalize sequence counter to 0-1 range per sequence\n",
        "    df = df.with_columns([\n",
        "        ((pl.col('sequence_counter') - pl.col('sequence_counter').min().over('sequence_id')) /\n",
        "         (pl.col('sequence_counter').max().over('sequence_id') - pl.col('sequence_counter').min().over('sequence_id') + CONFIG.ERR))\n",
        "        .alias('normalized_position')\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        (pl.col('normalized_position') * 2 * np.pi).sin().alias('position_sin'),\n",
        "        (pl.col('normalized_position') * 2 * np.pi).cos().alias('position_cos'),\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "    # Core IMU + derived columns\n",
        "    core_cols = [\"acc_x\", \"acc_y\", \"acc_z\", \"acc_mag\", \"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\", \"rot_mag\"]\n",
        "    if 'linear_acc_x' in df.columns:\n",
        "        core_cols.extend([\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\", \"linear_acc_mag\"])\n",
        "    if 'angular_vel_x' in df.columns:\n",
        "        core_cols.extend([\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\", \"angular_vel_mag\"])\n",
        "    if 'euler_roll' in df.columns:\n",
        "        core_cols.extend([\"euler_roll\", \"euler_pitch\", \"euler_yaw\", \"euler_mag\"])\n",
        "\n",
        "    core_cols.extend([\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\"\n",
        "    ])\n",
        "\n",
        "    for col in core_cols:\n",
        "        temporal_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').alias(f'{col}_vel'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(temporal_exprs)\n",
        "\n",
        "    # accel_exprs = []\n",
        "\n",
        "    # for col in ['angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']:\n",
        "    #     accel_exprs.extend([\n",
        "    #         pl.col(col).diff().over('sequence_id').alias(f'{col}_accel'),\n",
        "    #     ])\n",
        "\n",
        "    # df = df.with_columns(accel_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "def rolling_window_features(df, windows=[3, 5, 10]):\n",
        "    \"\"\"Add rolling window features that CNNs can learn from\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    rolling_exprs = []\n",
        "\n",
        "    key_cols = [\"acc_mag\", \"acc_x\", \"acc_y\", \"acc_z\", \"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\", \"angular_vel_distance\", \"angular_vel_mag\"]\n",
        "\n",
        "    for window in windows:\n",
        "        for col in key_cols:\n",
        "            if col in df.columns:\n",
        "                rolling_exprs.extend([\n",
        "                    pl.col(col).rolling_mean(window).over('sequence_id').alias(f'{col}_roll_mean_{window}'),\n",
        "                    pl.col(col).rolling_std(window).over('sequence_id').alias(f'{col}_roll_std_{window}'),\n",
        "                    # Rolling min/max can help identify peaks\n",
        "                    pl.col(col).rolling_max(window).over('sequence_id').alias(f'{col}_roll_max_{window}'),\n",
        "                    pl.col(col).rolling_min(window).over('sequence_id').alias(f'{col}_roll_min_{window}'),\n",
        "                ])\n",
        "\n",
        "    df = df.with_columns(rolling_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "\n",
        "def rotation_matrix_features(df):\n",
        "    \"\"\"Extract features from rotation matrices - captures 3D orientation relationships\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    rot_matrix_exprs = []\n",
        "\n",
        "    rot_matrix_exprs.extend([\n",
        "        (1 - 2*(pl.col('rot_y')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r11'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') - pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r12'),\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r13'),\n",
        "\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_y') + pl.col('rot_w')*pl.col('rot_z'))).alias('rot_matrix_r21'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_z')**2)).alias('rot_matrix_r22'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r23'),\n",
        "\n",
        "        (2*(pl.col('rot_x')*pl.col('rot_z') - pl.col('rot_w')*pl.col('rot_y'))).alias('rot_matrix_r31'),\n",
        "        (2*(pl.col('rot_y')*pl.col('rot_z') + pl.col('rot_w')*pl.col('rot_x'))).alias('rot_matrix_r32'),\n",
        "        (1 - 2*(pl.col('rot_x')**2 + pl.col('rot_y')**2)).alias('rot_matrix_r33'),\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns(rot_matrix_exprs)\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def world_space_features(df):\n",
        "    \"\"\"\n",
        "    Transforms key sensor vectors into the world coordinate frame.\n",
        "    \"\"\"\n",
        "    linear_acc_vals = df[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']].values\n",
        "    quat_vals = df[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "\n",
        "    world_accel = np.zeros_like(linear_acc_vals)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        quat = quat_vals[i]\n",
        "        if np.all(np.isnan(quat)) or np.all(np.isclose(quat, 0)):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rotation = R.from_quat(quat)\n",
        "            world_accel[i, :] = rotation.apply(linear_acc_vals[i])\n",
        "\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    df['world_acc_x'] = world_accel[:, 0]\n",
        "    df['world_acc_y'] = world_accel[:, 1]\n",
        "    df['world_acc_z'] = world_accel[:, 2] # This 'z' is now consistently 'up/down'\n",
        "    df['world_acc_mag'] = np.linalg.norm(world_accel, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def apply_feature_engineering(df):\n",
        "    print(\"  Applying feature engineering...\")\n",
        "    df = cast_to_object(df)\n",
        "    df = mag_features(df)\n",
        "    df = rotation_matrix_features(df)\n",
        "    df = world_space_features(df)\n",
        "    df = temporal_features(df)\n",
        "    df = rolling_window_features(df)\n",
        "    print(\"  Feature engineering complete.\")\n",
        "    return df\n",
        "\n",
        "train = apply_feature_engineering(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP1NFd0_UhCf"
      },
      "outputs": [],
      "source": [
        "def axis_angle_features(df):\n",
        "    \"\"\"\n",
        "    Converts quaternions to a more intuitive axis-angle representation.\n",
        "    \"\"\"\n",
        "    quat_vals = df[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    axis_angle_vals = np.zeros((len(df), 3))\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        quat = quat_vals[i]\n",
        "        if np.all(np.isnan(quat)) or np.all(np.isclose(quat, 0)) or np.linalg.norm(quat) == 0:\n",
        "            continue\n",
        "        try:\n",
        "            # R.from_quat handles normalization internally\n",
        "            axis_angle_vals[i, :] = R.from_quat(quat).as_rotvec()\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    df['rot_axis_x'] = axis_angle_vals[:, 0]\n",
        "    df['rot_axis_y'] = axis_angle_vals[:, 1]\n",
        "    df['rot_axis_z'] = axis_angle_vals[:, 2]\n",
        "    df['rot_angle'] = np.linalg.norm(axis_angle_vals, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "train = axis_angle_features(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ul16-2QYoT"
      },
      "source": [
        "## TOF THM FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX524iTnxA5r"
      },
      "outputs": [],
      "source": [
        "thm_cols = [\n",
        "    \"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\",\n",
        "\n",
        "    # \"thm_12_diff\", \"thm_13_diff\", \"thm_14_diff\",\n",
        "    # \"thm_15_diff\", \"thm_23_diff\", \"thm_24_diff\", \"thm_25_diff\",\n",
        "    # \"thm_34_diff\", \"thm_35_diff\", \"thm_45_diff\",\n",
        "\n",
        "]\n",
        "\n",
        "tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
        "\n",
        "tof_diff_cols = [f\"tof_{i}{j}_mean_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]\n",
        "tof_diff_cols += [f\"tof_{i}{j}_std_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRYrkG8sbfb7"
      },
      "outputs": [],
      "source": [
        "# train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-IITROb8keh"
      },
      "outputs": [],
      "source": [
        "def thm_features_func(df):\n",
        "    \"\"\"Extract features from thermopile sensors\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    thm_sensor_exprs = []\n",
        "    for col in [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]:\n",
        "        if col in df.columns:\n",
        "            thm_sensor_exprs.extend([\n",
        "                pl.col(col).diff().over('sequence_id').alias(f'{col}_vel'),\n",
        "                pl.col(col).diff().diff().over('sequence_id').alias(f'{col}_accel'),\n",
        "                (pl.col(col) - pl.col(col).mean().over('sequence_id')).alias(f'{col}_relative'),\n",
        "            ])\n",
        "\n",
        "    df = df.with_columns(thm_sensor_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "from scipy import ndimage, signal\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "\n",
        "def advanced_tof_features(df):\n",
        "    \"\"\"Advanced time-of-flight feature engineering\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_advanced_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        tof_advanced_exprs.extend([\n",
        "            # Local contrast (difference between max and min in local regions)\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[:16]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[:16]])).alias(f'tof_{sensor_idx}_contrast_q1'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[16:32]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[16:32]])).alias(f'tof_{sensor_idx}_contrast_q2'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[32:48]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[32:48]])).alias(f'tof_{sensor_idx}_contrast_q3'),\n",
        "\n",
        "            (pl.max_horizontal([pl.col(col) for col in pixel_cols[48:64]]) -\n",
        "             pl.min_horizontal([pl.col(col) for col in pixel_cols[48:64]])).alias(f'tof_{sensor_idx}_contrast_q4'),\n",
        "\n",
        "        ])\n",
        "\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def tof_features_func(df):\n",
        "    \"\"\"Extract features from time-of-flight sensors (proximity/distance)\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_sensor_exprs = []\n",
        "\n",
        "    for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        tof_sensor_exprs.extend([\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "              .list.mean().alias(f'tof_{sensor_idx}_mean_distance'),\n",
        "\n",
        "            pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "              .list.std().alias(f'tof_{sensor_idx}_std_distance'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "\n",
        "def tof_regional_features(df, tof_mode=\"stats\", include_regions=True):\n",
        "    \"\"\"\n",
        "    Extract features from time-of-flight sensors with regional analysis\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with ToF data\n",
        "        tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "        include_regions: Whether to include regional analysis features\n",
        "    \"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    tof_sensor_exprs = []\n",
        "\n",
        "    # Basic statistics for each ToF sensor (5 sensors total)\n",
        "    for sensor_idx in range(1, 6):\n",
        "        pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "        # # Basic stats (replace -1 with null for proper statistics)\n",
        "        # tof_sensor_exprs.extend([\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.mean().alias(f'tof_{sensor_idx}_mean'),\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.std().alias(f'tof_{sensor_idx}_std'),\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.min().alias(f'tof_{sensor_idx}_min'),\n",
        "        #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "        #       .list.max().alias(f'tof_{sensor_idx}_max'),\n",
        "        # ])\n",
        "\n",
        "        if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "            # Different region modes\n",
        "            region_modes = [2, 4, 8, 16] if tof_mode == \"regions\" else [4]\n",
        "\n",
        "            for mode in region_modes:\n",
        "                region_size = 64 // mode  # pixels per region\n",
        "\n",
        "                for region_idx in range(mode):\n",
        "                    start_pixel = region_idx * region_size\n",
        "                    end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "                    region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "                    tof_sensor_exprs.extend([\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.mean().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_mean'),\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.std().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_std'),\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.min().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_min'),\n",
        "                        pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "                          .list.max().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_max'),\n",
        "\n",
        "                    ])\n",
        "\n",
        "    df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "train = thm_features_func(train)\n",
        "\n",
        "train = tof_features_func(train)\n",
        "train = tof_regional_features(train, tof_mode=\"regions\")\n",
        "train = advanced_tof_features(train)\n",
        "\n",
        "\n",
        "# thm_feature_cols = [col for col in train.columns if 'thm_' in col and col not in thm_cols]\n",
        "# # tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "# tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "\n",
        "\n",
        "# FEATURES_FULL = FEATURES + thm_feature_cols + tof_feature_cols\n",
        "# print(len(FEATURES_FULL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJuAJBGWQWSk"
      },
      "source": [
        "## REDUCE MEMORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOxRa7Mt_5Mb"
      },
      "outputs": [],
      "source": [
        "# def reduce_mem_usage(df, verbose=True):\n",
        "#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2 # calculate current memory usage\n",
        "\n",
        "#     for col in df.columns:\n",
        "#         col_type = df[col].dtype\n",
        "#         if col_type in numerics: # check if column is numeric\n",
        "#             c_min = df[col].min()\n",
        "#             c_max = df[col].max()\n",
        "#             if str(col_type).startswith('int'): # if integer\n",
        "#                 # Check if data can be safely cast to smaller int types\n",
        "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "#                     df[col] = df[col].astype(np.int8)\n",
        "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "#                     df[col] = df[col].astype(np.int16)\n",
        "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "#                     df[col] = df[col].astype(np.int32)\n",
        "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "#                     df[col] = df[col].astype(np.int64) # Should already be this or smaller if loaded as int\n",
        "#             else: # if float\n",
        "#                 # Check if data can be safely cast to float32 (float16 often loses too much precision)\n",
        "#                 if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "#                     df[col] = df[col].astype(np.float32)\n",
        "#                 # else: # If not, keep as float64\n",
        "#                 #     df[col] = df[col].astype(np.float64) # Already this type\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     if verbose:\n",
        "#         print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "#     return df\n",
        "\n",
        "# print(\"Reducing memory for train_df:\")\n",
        "# train = reduce_mem_usage(train)\n",
        "# print(\"\\nReducing memory for test_df:\")\n",
        "# test = reduce_mem_usage(test)\n",
        "\n",
        "# print(\"\\nTrain DataFrame info after memory reduction:\")\n",
        "# train.info(memory_usage='deep')\n",
        "# print(\"\\nTest DataFrame info after memory reduction:\")\n",
        "# test.info(memory_usage='deep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO30PQozOlbg"
      },
      "outputs": [],
      "source": [
        "# numeric_df = train[FEATURES]\n",
        "# corr = numeric_df.corr(method = 'pearson')\n",
        "# corr = corr.abs()\n",
        "# # corr.style.background_gradient(cmap='inferno')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdESwz1wQHwM"
      },
      "outputs": [],
      "source": [
        "# upper_tri_mask = np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
        "# upper_tri = corr.where(upper_tri_mask)\n",
        "# highly_correlated_series = upper_tri.stack()\n",
        "# strong_pairs = highly_correlated_series[highly_correlated_series > 0.90]\n",
        "# strong_pairs_df = strong_pairs.reset_index()\n",
        "# strong_pairs_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "# strong_pairs_df_sorted = strong_pairs_df.sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
        "# print(f\"Found {len(strong_pairs_df_sorted)} pairs of features with correlation > 0.90\")\n",
        "# print(\"-\" * 50)\n",
        "# print(strong_pairs_df_sorted.head(200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJYVNzGqBS63"
      },
      "source": [
        "## METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQRvcrT_5Mb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hierarchical macro F1 metric for the CMI 2025 Challenge.\n",
        "\n",
        "This script defines a single entry point `score(solution, submission, row_id_column_name)`\n",
        "that the Kaggle metrics orchestrator will call.\n",
        "It performs validation on submission IDs and computes a combined binary & multiclass F1 score.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.target_gestures = [\n",
        "            'Above ear - pull hair',\n",
        "            'Cheek - pinch skin',\n",
        "            'Eyebrow - pull hair',\n",
        "            'Eyelash - pull hair',\n",
        "            'Forehead - pull hairline',\n",
        "            'Forehead - scratch',\n",
        "            'Neck - pinch skin',\n",
        "            'Neck - scratch',\n",
        "        ]\n",
        "        self.non_target_gestures = [\n",
        "            'Write name on leg',\n",
        "            'Wave hello',\n",
        "            'Glasses on/off',\n",
        "            'Text on phone',\n",
        "            'Write name in air',\n",
        "            'Feel around in tray and pull out an object',\n",
        "            'Scratch knee/leg skin',\n",
        "            'Pull air toward your face',\n",
        "            'Drink from bottle/cup',\n",
        "            'Pinch knee/leg skin'\n",
        "        ]\n",
        "        self.all_classes = self.target_gestures + self.non_target_gestures\n",
        "\n",
        "    def calculate_hierarchical_f1(\n",
        "        self,\n",
        "        sol: pd.DataFrame,\n",
        "        sub: pd.DataFrame\n",
        "    ) -> float:\n",
        "\n",
        "        # Validate gestures\n",
        "        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n",
        "        if invalid_types:\n",
        "            raise ParticipantVisibleError(\n",
        "                f\"Invalid gesture values in submission: {invalid_types}\"\n",
        "            )\n",
        "\n",
        "        # Compute binary F1 (Target vs Non-Target)\n",
        "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
        "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
        "        f1_binary = f1_score(\n",
        "            y_true_bin,\n",
        "            y_pred_bin,\n",
        "            pos_label=True,\n",
        "            zero_division=0,\n",
        "            average='binary'\n",
        "        )\n",
        "\n",
        "        # Build multi-class labels for gestures\n",
        "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "\n",
        "        # Compute macro F1 over all gesture classes\n",
        "        f1_macro = f1_score(\n",
        "            y_true_mc,\n",
        "            y_pred_mc,\n",
        "            average='macro',\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f'f1_binary score: {f1_binary}')\n",
        "        print(f'f1_macro score: {f1_macro}')\n",
        "\n",
        "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
        "\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute hierarchical macro F1 for the CMI 2025 challenge.\n",
        "\n",
        "    Expected input:\n",
        "      - solution and submission as pandas.DataFrame\n",
        "      - Column 'sequence_id': unique identifier for each sequence\n",
        "      - 'gesture': one of the eight target gestures or \"Non-Target\"\n",
        "\n",
        "    This metric averages:\n",
        "    1. Binary F1 on SequenceType (Target vs Non-Target)\n",
        "    2. Macro F1 on gesture (mapping non-targets to \"Non-Target\")\n",
        "\n",
        "    Raises ParticipantVisibleError for invalid submissions,\n",
        "    including invalid SequenceType or gesture values.\n",
        "\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> row_id_column_name = \"id\"\n",
        "    >>> solution = pd.DataFrame({'id': range(4), 'gesture': ['Eyebrow - pull hair']*4})\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Forehead - pull hairline']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.5\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Text on phone']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.0\n",
        "    >>> score(solution, solution, row_id_column_name=row_id_column_name)\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    # Validate required columns\n",
        "    for col in (row_id_column_name, 'gesture'):\n",
        "        if col not in solution.columns:\n",
        "            raise ParticipantVisibleError(f\"Solution file missing required column: '{col}'\")\n",
        "        if col not in submission.columns:\n",
        "            raise ParticipantVisibleError(f\"Submission file missing required column: '{col}'\")\n",
        "\n",
        "    metric = CompetitionMetric()\n",
        "    return metric.calculate_hierarchical_f1(solution, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRPA6tMxM3Gu"
      },
      "source": [
        "## MIXUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOrm302jDZ6P"
      },
      "source": [
        "## CNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpze6U9aCrS-"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel, BertConfig\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import uuid\n",
        "import random\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "metric_calculator = CompetitionMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xTMV2YDUFep",
        "outputId": "b7711d4c-761d-427c-a17f-545ec6538ff1"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.experimental.numpy.random.seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print(f\"Seeds set to {seed} for reproducibility\")\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XqhN_lyh2UK"
      },
      "outputs": [],
      "source": [
        "class SWACallback(callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Stochastic Weight Averaging callback for Keras\n",
        "    \"\"\"\n",
        "    def __init__(self, start_epoch=10, swa_freq=5, verbose=1):\n",
        "        super().__init__()\n",
        "        self.start_epoch = start_epoch\n",
        "        self.swa_freq = swa_freq\n",
        "        self.verbose = verbose\n",
        "        self.swa_weights = None\n",
        "        self.n_models = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch >= self.start_epoch and (epoch - self.start_epoch) % self.swa_freq == 0:\n",
        "            current_weights = self.model.get_weights()\n",
        "\n",
        "            if self.swa_weights is None:\n",
        "                self.swa_weights = [w.copy() for w in current_weights]\n",
        "                self.n_models = 1\n",
        "            else:\n",
        "                self.n_models += 1\n",
        "                for i in range(len(self.swa_weights)):\n",
        "                    self.swa_weights[i] = (\n",
        "                        (self.n_models - 1) * self.swa_weights[i] + current_weights[i]\n",
        "                    ) / self.n_models\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"SWA: Updated weights at epoch {epoch + 1} (n_models: {self.n_models})\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.swa_weights is not None:\n",
        "            self.model.set_weights(self.swa_weights)\n",
        "            if self.verbose:\n",
        "                print(f\"SWA: Applied averaged weights from {self.n_models} models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws7npjdw0Ox1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "class EMACallback(callbacks.Callback):\n",
        "    \"\"\"\n",
        "    A custom callback to implement Exponential Moving Average of model weights.\n",
        "\n",
        "    Args:\n",
        "        decay: The decay rate for the EMA. A float between 0 and 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, decay=0.999):\n",
        "        super(EMACallback, self).__init__()\n",
        "        self.decay = decay\n",
        "        self.shadow_weights = None\n",
        "        self.original_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        \"\"\"Initializes the shadow weights at the start of training.\"\"\"\n",
        "        if self.shadow_weights is None:\n",
        "            self.shadow_weights = [\n",
        "                tf.Variable(v, trainable=False, name=f'ema_shadow_{i}')\n",
        "                for i, v in enumerate(self.model.trainable_variables)\n",
        "            ]\n",
        "            print(\"EMA: Initialized shadow weights.\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        \"\"\"Updates the shadow weights after each training batch.\"\"\"\n",
        "        for model_var, shadow_var in zip(self.model.trainable_variables, self.shadow_weights):\n",
        "            shadow_var.assign(\n",
        "                self.decay * shadow_var + (1 - self.decay) * model_var\n",
        "            )\n",
        "\n",
        "    def assign_ema_weights(self):\n",
        "        \"\"\"\n",
        "        Assigns the averaged EMA weights to the model for evaluation.\n",
        "        It saves the original weights to be restored later.\n",
        "        \"\"\"\n",
        "        if self.shadow_weights is None:\n",
        "            print(\"EMA: Error - shadow weights not initialized. Was the model trained?\")\n",
        "            return\n",
        "\n",
        "        print(\"EMA: Assigning averaged weights to model for evaluation...\")\n",
        "        self.original_weights = [tf.identity(v) for v in self.model.trainable_variables]\n",
        "        for model_var, shadow_var in zip(self.model.trainable_variables, self.shadow_weights):\n",
        "            model_var.assign(shadow_var)\n",
        "\n",
        "    def restore_original_weights(self):\n",
        "        \"\"\"Restores the original model weights after evaluation.\"\"\"\n",
        "        if self.original_weights is None:\n",
        "            print(\"EMA: Error - original weights not saved.\")\n",
        "            return\n",
        "\n",
        "        print(\"EMA: Restoring original model weights...\")\n",
        "        for model_var, original_var in zip(self.model.trainable_variables, self.original_weights):\n",
        "            model_var.assign(original_var)\n",
        "        self.original_weights = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U63EBH5PzQt0"
      },
      "outputs": [],
      "source": [
        "# 7 5 3 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nfyJxvfbOfC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "class MixupSequence(Sequence):\n",
        "    def __init__(self, X, y, batch_size=CONFIG.BATCH_SIZE, alpha=0.3, mixup_prob=0.5, mixup_seed=42):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.mixup_prob = mixup_prob\n",
        "        self.indices = np.arange(len(X))\n",
        "        self.seed = mixup_seed\n",
        "\n",
        "        self.total_batches = int(np.ceil(len(X) / batch_size))\n",
        "        self.rng = np.random.RandomState(mixup_seed)\n",
        "        self.reset_random_states()\n",
        "\n",
        "    def reset_random_states(self):\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "        self.mixup_decisions = self.rng.rand(self.total_batches)\n",
        "        self.permutation_seeds = self.rng.randint(0, 10000, self.total_batches)\n",
        "        self.lambda_seeds = self.rng.randint(0, 10000, self.total_batches)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_batches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch = self.X[batch_indices]\n",
        "        y_batch = self.y[batch_indices]\n",
        "\n",
        "        # Apply mixup to batch\n",
        "        if self.mixup_decisions[idx] < self.mixup_prob:\n",
        "            X_batch, y_batch = self._mixup_batch_reproducible(X_batch, y_batch, idx)\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def _mixup_batch_reproducible(self, X_batch, y_batch, batch_idx):\n",
        "        batch_size = len(X_batch)\n",
        "\n",
        "        perm_rng = np.random.RandomState(self.permutation_seeds[batch_idx])\n",
        "        lambda_rng = np.random.RandomState(self.lambda_seeds[batch_idx])\n",
        "        indices = perm_rng.permutation(batch_size)\n",
        "        lam = lambda_rng.beta(self.alpha, self.alpha, batch_size)\n",
        "        X_mixed = lam[:, np.newaxis, np.newaxis] * X_batch + (1 - lam[:, np.newaxis, np.newaxis]) * X_batch[indices]\n",
        "        y_mixed = lam[:, np.newaxis] * y_batch + (1 - lam[:, np.newaxis]) * y_batch[indices]\n",
        "\n",
        "        return X_mixed, y_mixed\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        shuffle_rng = np.random.RandomState(self.seed + 1000)  \n",
        "        shuffle_rng.shuffle(self.indices)\n",
        "\n",
        "        self.seed += 1\n",
        "        self.reset_random_states()\n",
        "\n",
        "\n",
        "# def simple_additive_mixup(X, y, num_classes, mixup_prob=0.5, alpha=0.3, seed=42):\n",
        "#     \"\"\"\n",
        "#     Simple function to add mixup samples to your existing training data\n",
        "\n",
        "#     Args:\n",
        "#         X: Training sequences (n_samples, seq_len, features)\n",
        "#         y: Training labels (n_samples,) - integers\n",
        "#         num_classes: Number of classes\n",
        "#         augment_ratio: How much extra data to generate (0.5 = 50% more)\n",
        "#         alpha: Mixup alpha parameter\n",
        "#         seed: Random seed\n",
        "\n",
        "#     Returns:\n",
        "#         X_augmented: Original + mixed data\n",
        "#         y_augmented: Original + mixed labels (one-hot)\n",
        "#     \"\"\"\n",
        "#     np.random.seed(seed)\n",
        "\n",
        "#     n_original = len(X)\n",
        "#     n_mixed = int(n_original * mixup_prob)\n",
        "\n",
        "#     print(f\"Adding {n_mixed} mixed samples to {n_original} original samples\")\n",
        "\n",
        "#     # Generate random pairs\n",
        "#     idx1 = np.random.randint(0, n_original, n_mixed)\n",
        "#     idx2 = np.random.randint(0, n_original, n_mixed)\n",
        "\n",
        "#     # Generate lambda values\n",
        "#     lam = np.random.beta(alpha, alpha, n_mixed)\n",
        "\n",
        "#     # Create mixed samples\n",
        "#     X_mixed = np.zeros((n_mixed, X.shape[1], X.shape[2]))\n",
        "#     y_mixed = np.zeros((n_mixed, num_classes))\n",
        "\n",
        "#     for i in range(n_mixed):\n",
        "#         # Mix sequences\n",
        "#         X_mixed[i] = lam[i] * X[idx1[i]] + (1 - lam[i]) * X[idx2[i]]\n",
        "\n",
        "#         # Mix labels (soft labels)\n",
        "#         y_mixed[i, y[idx1[i]]] += lam[i]\n",
        "#         y_mixed[i, y[idx2[i]]] += (1 - lam[i])\n",
        "\n",
        "#     # Combine original + mixed\n",
        "#     X_augmented = np.vstack([X, X_mixed])\n",
        "#     y_original_onehot = to_categorical(y, num_classes)\n",
        "#     y_augmented = np.vstack([y_original_onehot, y_mixed])\n",
        "\n",
        "#     print(f\"Final dataset: {len(X_augmented)} samples\")\n",
        "#     return X_augmented, y_augmented\n",
        "\n",
        "\n",
        "# Modified fit function for your CNN class with SWA + Mixup\n",
        "def fit_with_swa_and_mixup(self, X, y, validation_data=None, epochs=100, batch_size=CONFIG.BATCH_SIZE,\n",
        "                          alpha=0.3, mixup_prob=0.5, swa_start=20, swa_freq=5, verbose=1):\n",
        "    \"\"\"Train the model with both SWA and mixup data generator\"\"\"\n",
        "\n",
        "    # Scale features\n",
        "    X_scaled = self.scale_features(X, fit=True)\n",
        "    y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "\n",
        "    train_generator = MixupSequence(\n",
        "        X_scaled, y_cat,\n",
        "        batch_size=batch_size,\n",
        "        alpha=alpha,\n",
        "        mixup_prob=mixup_prob,\n",
        "        mixup_seed = 42\n",
        "    )\n",
        "\n",
        "    val_data = None\n",
        "    if validation_data is not None:\n",
        "        X_val, y_val = validation_data\n",
        "        X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "        y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "        val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "    self.swa_callback = SWACallback(\n",
        "        start_epoch=swa_start,\n",
        "        swa_freq=swa_freq,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    callback_list = [\n",
        "        self.swa_callback,\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            patience=35,  \n",
        "            restore_best_weights=False, \n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            factor=0.7, \n",
        "            patience=10,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    ]\n",
        "\n",
        "    history = self.model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_data,\n",
        "        epochs=epochs,\n",
        "        callbacks=callback_list,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# def fit_with_swa_and_mixup(self, X, y, validation_data=None, epochs=100, batch_size=32,\n",
        "#                             alpha=0.3, mixup_prob=0.5, swa_start=20, swa_freq=5, verbose=1):\n",
        "#     \"\"\"\n",
        "#     Simple additive mixup: generate extra data once, then train normally\n",
        "#     \"\"\"\n",
        "\n",
        "#     # 1. Generate additional mixed data\n",
        "#     X_aug, y_aug = simple_additive_mixup(\n",
        "#         X, y, self.num_classes,\n",
        "#         alpha=alpha,\n",
        "#         mixup_prob=mixup_prob,\n",
        "#     )\n",
        "\n",
        "#     # 2. Scale features\n",
        "#     X_scaled = self.scale_features(X_aug, fit=True)\n",
        "\n",
        "#     # 3. Prepare validation data\n",
        "#     val_data = None\n",
        "#     if validation_data is not None:\n",
        "#         X_val, y_val = validation_data\n",
        "#         X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "#         y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "#         val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "#     # 4. SWA callback\n",
        "#     self.swa_callback = SWACallback(\n",
        "#         start_epoch=swa_start,\n",
        "#         swa_freq=swa_freq,\n",
        "#         verbose=verbose\n",
        "#     )\n",
        "\n",
        "#     # 5. Regular training (no special generators needed)\n",
        "#     history = self.model.fit(\n",
        "#         X_scaled, y_aug,  # Augmented data with soft labels\n",
        "#         validation_data=val_data,\n",
        "#         epochs=epochs,\n",
        "#         batch_size=batch_size,\n",
        "#         callbacks=[\n",
        "#             self.swa_callback,\n",
        "#             callbacks.EarlyStopping(monitor='val_loss', patience=35, verbose=1),\n",
        "#             callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=10, verbose=1)\n",
        "#         ],\n",
        "#         verbose=verbose\n",
        "#     )\n",
        "\n",
        "#     return history\n",
        "\n",
        "def add_swa_mixup_to_cnn_class():\n",
        "    CNN1DModelWithSWA.fit_with_swa_and_mixup = fit_with_swa_and_mixup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sESjzrGBhvXX"
      },
      "outputs": [],
      "source": [
        "class SEBlock(layers.Layer):\n",
        "    \"\"\"Squeeze-and-Excitation block\"\"\"\n",
        "    def __init__(self, channels, reduction=8, **kwargs):\n",
        "        super(SEBlock, self).__init__(**kwargs)\n",
        "        self.channels = channels\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.global_avg_pool = layers.GlobalAveragePooling1D()\n",
        "        self.dense1 = layers.Dense(self.channels // self.reduction, activation='relu', use_bias=False)\n",
        "        self.dense2 = layers.Dense(self.channels, activation='sigmoid', use_bias=False)\n",
        "        super(SEBlock, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        squeeze = self.global_avg_pool(inputs)  # (batch, channels)\n",
        "\n",
        "        excitation = self.dense1(squeeze)\n",
        "        excitation = self.dense2(excitation)\n",
        "        excitation = tf.expand_dims(excitation, axis=1)  # (batch, 1, channels)\n",
        "\n",
        "        return inputs * excitation\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SEBlock, self).get_config()\n",
        "        config.update({\n",
        "            'channels': self.channels,\n",
        "            'reduction': self.reduction\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def residual_se_cnn_block(x, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3, name_prefix=''):\n",
        "    \"\"\"Residual SE CNN block\"\"\"\n",
        "\n",
        "    conv1 = layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False,\n",
        "                         name=f'{name_prefix}_conv1')(x)\n",
        "    bn1 = layers.BatchNormalization(name=f'{name_prefix}_bn1')(conv1)\n",
        "    relu1 = layers.Activation('relu', name=f'{name_prefix}_relu1')(bn1)\n",
        "\n",
        "    conv2 = layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False,\n",
        "                         name=f'{name_prefix}_conv2')(relu1)\n",
        "    bn2 = layers.BatchNormalization(name=f'{name_prefix}_bn2')(conv2)\n",
        "\n",
        "    se_out = SEBlock(out_channels, name=f'{name_prefix}_se')(bn2)\n",
        "\n",
        "    if in_channels != out_channels:\n",
        "        shortcut = layers.Conv1D(out_channels, 1, use_bias=False,\n",
        "                               name=f'{name_prefix}_shortcut_conv')(x)\n",
        "        shortcut = layers.BatchNormalization(name=f'{name_prefix}_shortcut_bn')(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    add = layers.Add(name=f'{name_prefix}_add')([se_out, shortcut])\n",
        "    relu2 = layers.Activation('relu', name=f'{name_prefix}_relu2')(add)\n",
        "\n",
        "    pool = layers.MaxPooling1D(pool_size, name=f'{name_prefix}_pool')(relu2)\n",
        "    dropout_layer = layers.Dropout(dropout, name=f'{name_prefix}_dropout')(pool)\n",
        "\n",
        "    return dropout_layer\n",
        "\n",
        "\n",
        "# class AttentionLayer(layers.Layer):\n",
        "#     \"\"\"Attention mechanism\"\"\"\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "#     def build(self, input_shape):\n",
        "#         self.attention_dense = layers.Dense(1, use_bias=True)\n",
        "#         super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         # inputs shape: (batch, seq_len, hidden_dim)\n",
        "#         scores = tf.tanh(self.attention_dense(inputs))  # (batch, seq_len, 1)\n",
        "#         weights = tf.nn.softmax(tf.squeeze(scores, axis=-1), axis=1)  # (batch, seq_len)\n",
        "#         weights = tf.expand_dims(weights, axis=-1)  # (batch, seq_len, 1)\n",
        "#         context = tf.reduce_sum(inputs * weights, axis=1)  # (batch, hidden_dim)\n",
        "#         return context\n",
        "\n",
        "\n",
        "class CLSTokenLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom Keras layer to prepend a learnable [CLS] token to a sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CLSTokenLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        hidden_dim = input_shape[-1]\n",
        "        self.cls_token = self.add_weight(\n",
        "            name=\"cls_token\",\n",
        "            shape=(1, 1, hidden_dim),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        super(CLSTokenLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        cls_broadcasted = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        return tf.concat([cls_broadcasted, inputs], axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(CLSTokenLayer, self).get_config()\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqTjR792h2Qw"
      },
      "outputs": [],
      "source": [
        "class CNN1DModelWithSWA:\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=5e-4,\n",
        "                 bert_hidden_size=256, bert_layers=4, bert_heads=8):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.swa_callback = None\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the model with a BERT Encoder, using the correct calling convention\n",
        "        inside the Lambda layer.\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
        "\n",
        "        x = residual_se_cnn_block(\n",
        "            inputs, self.input_shape[-1], 64, kernel_size=3,\n",
        "            pool_size=2, dropout=0.3, name_prefix='imu_block1'\n",
        "        )\n",
        "        x = residual_se_cnn_block(\n",
        "            x, 64, 128, kernel_size=5,\n",
        "            pool_size=2, dropout=0.3, name_prefix='imu_block2'\n",
        "        )\n",
        "\n",
        "        gru_out = layers.Bidirectional(\n",
        "            layers.GRU(128, return_sequences=True, name='gru'),\n",
        "            name='bidirectional_gru'\n",
        "        )(x)\n",
        "        gru_dropout = layers.Dropout(0.4, name='gru_dropout')(gru_out)\n",
        "\n",
        "        bert_input = CLSTokenLayer(name='cls_token_layer')(gru_dropout)\n",
        "\n",
        "        config = BertConfig(\n",
        "            hidden_size=256,\n",
        "            num_hidden_layers=4,\n",
        "            num_attention_heads=8,\n",
        "            intermediate_size=256 * 4,\n",
        "            hidden_dropout_prob=0.2,\n",
        "            attention_probs_dropout_prob=0.2,\n",
        "        )\n",
        "        bert_model = TFBertModel(config, name=\"bert_model\")\n",
        "\n",
        "\n",
        "        bert_output_layer = layers.Lambda(\n",
        "            lambda x: bert_model({'inputs_embeds': x}, return_dict=True).last_hidden_state,\n",
        "            output_shape=bert_input.shape[1:],\n",
        "            name='bert_lambda_wrapper'\n",
        "        )\n",
        "\n",
        "        bert_outputs = bert_output_layer(bert_input)\n",
        "\n",
        "        cls_output = bert_outputs[:, 0, :]\n",
        "\n",
        "        dense1 = layers.Dense(256, use_bias=False, name='dense1')(cls_output)\n",
        "        bn_dense1 = layers.BatchNormalization(name='bn_dense1')(dense1)\n",
        "        relu_dense1 = layers.Activation('relu', name='relu_dense1')(bn_dense1)\n",
        "        drop1 = layers.Dropout(0.5, name='drop1')(relu_dense1)\n",
        "\n",
        "        dense2 = layers.Dense(128, use_bias=False, name='dense2')(drop1)\n",
        "        bn_dense2 = layers.BatchNormalization(name='bn_dense2')(dense2)\n",
        "        relu_dense2 = layers.Activation('relu', name='relu_dense2')(bn_dense2)\n",
        "        drop2 = layers.Dropout(0.3, name='drop2')(relu_dense2)\n",
        "\n",
        "        outputs = layers.Dense(self.num_classes, activation='softmax', name='classifier')(drop2)\n",
        "\n",
        "        model = models.Model(inputs=inputs, outputs=outputs, name='CMI_Model_TF_Replication')\n",
        "\n",
        "        optimizer = tf.keras.optimizers.AdamW(\n",
        "            learning_rate=self.learning_rate,\n",
        "            weight_decay=0.001,\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            # loss='categorical_crossentropy',\n",
        "            loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "              from_logits=False,\n",
        "              label_smoothing=0.1,\n",
        "              axis=-1,\n",
        "              reduction='sum_over_batch_size',\n",
        "              name='categorical_crossentropy'\n",
        "          ),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def prepare_sequences(self, df, features, target_col=None, sequence_length=None):\n",
        "        \"\"\"Fast sequence preparation using numpy operations\"\"\"\n",
        "        df_sorted = df.sort_values(['sequence_id', 'sequence_counter']).reset_index(drop=True)\n",
        "\n",
        "        seq_changes = df_sorted['sequence_id'].ne(df_sorted['sequence_id'].shift()).cumsum() - 1\n",
        "        unique_seqs, seq_starts = np.unique(seq_changes, return_index=True)\n",
        "        seq_ends = np.append(seq_starts[1:], len(df_sorted))\n",
        "        seq_lengths = seq_ends - seq_starts\n",
        "\n",
        "        if sequence_length is None:\n",
        "            sequence_length = seq_lengths.max()\n",
        "\n",
        "        num_sequences = len(seq_starts)\n",
        "        sequences = np.zeros((num_sequences, sequence_length, len(features)), dtype=np.float32)\n",
        "\n",
        "        feature_matrix = df_sorted[features].values.astype(np.float32)\n",
        "\n",
        "        for i, (start, end) in enumerate(zip(seq_starts, seq_ends)):\n",
        "            seq_len = end - start\n",
        "            actual_len = min(seq_len, sequence_length)\n",
        "            sequences[i, :actual_len] = feature_matrix[start:start + actual_len]\n",
        "\n",
        "        if target_col is not None:\n",
        "            targets = df_sorted.iloc[seq_starts][target_col].values\n",
        "            return sequences, targets\n",
        "        else:\n",
        "            return sequences\n",
        "\n",
        "    def fit_with_swa(self, X, y, validation_data=None, epochs=100, batch_size=CONFIG.BATCH_SIZE,\n",
        "                     swa_start=20, swa_freq=5, verbose=1):\n",
        "        \"\"\"Train the model with Stochastic Weight Averaging\"\"\"\n",
        "        X_scaled = self.scale_features(X, fit=True)\n",
        "\n",
        "        y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "\n",
        "        val_data = None\n",
        "        if validation_data is not None:\n",
        "            X_val, y_val = validation_data\n",
        "            X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "            y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "            val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "        self.swa_callback = SWACallback(\n",
        "            start_epoch=swa_start,\n",
        "            swa_freq=swa_freq,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        callback_list = [\n",
        "            self.swa_callback,  \n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=35,  \n",
        "                restore_best_weights=False,  \n",
        "                verbose=2\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.7, \n",
        "                patience=10,\n",
        "                min_lr=1e-7,\n",
        "                verbose=2\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model with SWA\n",
        "        history = self.model.fit(\n",
        "            X_scaled, y_cat,\n",
        "            validation_data=val_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callback_list,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def fit(self, X, y, validation_data=None, epochs=100, batch_size=CONFIG.BATCH_SIZE, verbose=1):\n",
        "        \"\"\"Standard training without SWA (for compatibility)\"\"\"\n",
        "        return self.fit_with_swa(\n",
        "            X, y, validation_data, epochs, batch_size,\n",
        "            swa_start=max(10, epochs//4),  \n",
        "            swa_freq=5,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities\"\"\"\n",
        "        X_scaled = self.scale_features(X, fit=False)\n",
        "        return self.model.predict(X_scaled)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict classes\"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)\n",
        "\n",
        "    def scale_features(self, X, fit=False):\n",
        "        \"\"\"Scale features across time and feature dimensions\"\"\"\n",
        "        original_shape = X.shape\n",
        "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "\n",
        "        if np.any(np.isnan(X_reshaped)) or np.any(np.isinf(X_reshaped)):\n",
        "            print(f\"WARNING: Found NaN/Inf values in input data!\")\n",
        "            print(f\"NaN count: {np.sum(np.isnan(X_reshaped))}\")\n",
        "            print(f\"Inf count: {np.sum(np.isinf(X_reshaped))}\")\n",
        "            X_reshaped = np.nan_to_num(X_reshaped, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if fit:\n",
        "            X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "        else:\n",
        "            X_scaled = self.scaler.transform(X_reshaped)\n",
        "\n",
        "        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n",
        "            print(f\"WARNING: Found NaN/Inf values in scaled data!\")\n",
        "            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        return X_scaled.reshape(original_shape)\n",
        "\n",
        "    def fit_with_swa_and_mixup(self, X, y, validation_data=None, epochs=100, batch_size=CONFIG.BATCH_SIZE,\n",
        "                              alpha=0.3, mixup_prob=0.5, swa_start=20, swa_freq=5, verbose=1):\n",
        "        \"\"\"Train the model with both SWA and mixup data generator\"\"\"\n",
        "\n",
        "        X_scaled = self.scale_features(X, fit=True)\n",
        "        y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "\n",
        "        train_generator = MixupSequence(\n",
        "            X_scaled, y_cat,\n",
        "            batch_size=batch_size,\n",
        "            alpha=alpha,\n",
        "            mixup_prob=mixup_prob,\n",
        "            mixup_seed=42\n",
        "        )\n",
        "\n",
        "        val_data = None\n",
        "        if validation_data is not None:\n",
        "            X_val, y_val = validation_data\n",
        "            X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "            y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "            val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "        self.swa_callback = SWACallback(\n",
        "            start_epoch=swa_start,\n",
        "            swa_freq=swa_freq,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        callback_list = [\n",
        "            self.swa_callback,  \n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_loss' if val_data else 'loss',\n",
        "                patience=35,  \n",
        "                restore_best_weights=False,  \n",
        "                verbose=1\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss' if val_data else 'loss',\n",
        "                factor=0.7,  \n",
        "                patience=10,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            train_generator,\n",
        "            validation_data=val_data,\n",
        "            epochs=epochs,\n",
        "            callbacks=callback_list,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37naYx5ih2OP"
      },
      "outputs": [],
      "source": [
        "def train_cnn_cross_validation_with_swa(train_df, features, target_col, demographics_df,\n",
        "                                       n_splits=5, label_encoder=None, aggregation_method=None, mixup_prob=1.0):\n",
        "    \"\"\"Modified cross-validation function for CNN with SWA\"\"\"\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "    import joblib\n",
        "    import uuid\n",
        "\n",
        "    run_id = uuid.uuid4()\n",
        "    os.makedirs('models_cnn_swa', exist_ok=True)\n",
        "\n",
        "    skf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(\n",
        "            skf.split(demographics_df, demographics_df[['adult_child', 'handedness', 'sex']])\n",
        "        ):\n",
        "        demographics_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "    demographics_df['fold'] = demographics_df['fold'].astype(int)\n",
        "    train_df = train_df.merge(demographics_df[['subject', 'fold']], on='subject', how='left')\n",
        "\n",
        "    if label_encoder is None:\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        unique_labels = train_df.groupby('sequence_id')[target_col].last().unique()\n",
        "        le.fit(unique_labels)\n",
        "        print(f\"Created label encoder with classes: {le.classes_}\")\n",
        "    else:\n",
        "        le = label_encoder\n",
        "        print(f\"Using provided label encoder with classes: {le.classes_}\")\n",
        "    joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    oof_preds = np.zeros(len(train_df.groupby('sequence_id').last()), dtype=int)\n",
        "    oof_proba = np.zeros((len(train_df.groupby('sequence_id').last()), num_classes))\n",
        "    oof_scores = []\n",
        "\n",
        "    seq_info = train_df.groupby('sequence_id').last()[['fold']].reset_index()\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        print(f\"{'#'*10} Fold {fold+1} with SWA {'#'*10}\")\n",
        "\n",
        "        # Split data by fold\n",
        "        train_sequences = seq_info[seq_info['fold'] != fold]['sequence_id'].values\n",
        "        valid_sequences = seq_info[seq_info['fold'] == fold]['sequence_id'].values\n",
        "\n",
        "        train_fold_df = train_df[train_df['sequence_id'].isin(train_sequences)]\n",
        "        valid_fold_df = train_df[train_df['sequence_id'].isin(valid_sequences)]\n",
        "\n",
        "        print(f\"  Train sequences: {len(train_sequences)}, Valid sequences: {len(valid_sequences)}\")\n",
        "\n",
        "        sequence_lengths = train_fold_df.groupby('sequence_id').size()\n",
        "        max_seq_length = int(sequence_lengths.quantile(0.99))\n",
        "\n",
        "        joblib.dump(max_seq_length, f\"models_cnn_swa/seq_length_fold_{fold+1}.pkl\")\n",
        "\n",
        "        cnn_model = CNN1DModelWithSWA(\n",
        "            input_shape=(max_seq_length, len(features)),\n",
        "            num_classes=num_classes,\n",
        "            learning_rate=1e-3  # Higher initial LR for SWA\n",
        "        )\n",
        "        cnn_model.build_model()\n",
        "\n",
        "        print(\"Initialized CNN model with SWA\")\n",
        "\n",
        "        X_train, y_train_str = cnn_model.prepare_sequences(\n",
        "            train_fold_df, features, target_col, sequence_length=max_seq_length\n",
        "        )\n",
        "        X_valid, y_valid_str = cnn_model.prepare_sequences(\n",
        "            valid_fold_df, features, target_col, sequence_length=max_seq_length\n",
        "        )\n",
        "\n",
        "        y_train = le.transform(y_train_str)\n",
        "        y_valid = le.transform(y_valid_str)\n",
        "\n",
        "        print(f\"  X_train shape: {X_train.shape}, X_valid shape: {X_valid.shape}\")\n",
        "        print(f\"  y_train shape: {y_train.shape}, y_valid shape: {y_valid.shape}\")\n",
        "\n",
        "        # ema_callback = EMACallback(decay=0.999)\n",
        "\n",
        "        # callback_list = [\n",
        "        #     ema_callback, # Our new callback\n",
        "        #     callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1),\n",
        "        #     callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=10, min_lr=1e-7, verbose=1)\n",
        "        # ]\n",
        "\n",
        "        if aggregation_method == \"swa\":\n",
        "            # Use standard SWA\n",
        "            # history = cnn_model.fit_with_swa(\n",
        "            #     X_train, y_train,\n",
        "            #     validation_data=(X_valid, y_valid),\n",
        "            #     epochs=100,\n",
        "            #     batch_size=32,\n",
        "            #     swa_start=20,\n",
        "            #     swa_freq=5,\n",
        "            #     verbose=2\n",
        "            # )\n",
        "\n",
        "            history = cnn_model.fit_with_swa_and_mixup(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_valid, y_valid),\n",
        "                epochs=150,\n",
        "                batch_size=CONFIG.BATCH_SIZE,\n",
        "                alpha=0.3,\n",
        "                mixup_prob=mixup_prob,\n",
        "                swa_start=50,\n",
        "                swa_freq=5,\n",
        "                verbose=2\n",
        "              )\n",
        "        # else:\n",
        "        #     history = cnn_model.model.fit(\n",
        "        #         # Using MixupSequence generator as before\n",
        "        #         MixupSequence(\n",
        "        #             cnn_model.scale_features(X_train, fit=True),\n",
        "        #             to_categorical(y_train, num_classes=num_classes),\n",
        "        #             batch_size=32, alpha=0.3, mixup_prob=mixup_prob\n",
        "        #         ),\n",
        "        #         validation_data=(\n",
        "        #             cnn_model.scale_features(X_valid, fit=False),\n",
        "        #             to_categorical(y_valid, num_classes=num_classes)\n",
        "        #         ),\n",
        "        #         epochs=100,\n",
        "        #         callbacks=callback_list, # Pass the list here\n",
        "        #         verbose=2\n",
        "        #     )\n",
        "\n",
        "        #     # 4. CRITICAL STEP: Assign the EMA weights to the model for prediction\n",
        "        #     ema_callback.assign_ema_weights()\n",
        "\n",
        "        cnn_model.model.save(f\"models_cnn_swa/cnn_swa_fold_{fold+1}.h5\")\n",
        "        joblib.dump(cnn_model.scaler, f\"models_cnn_swa/scaler_swa_fold_{fold+1}.pkl\")\n",
        "\n",
        "        fold_proba = cnn_model.predict_proba(X_valid)\n",
        "        fold_preds = np.argmax(fold_proba, axis=1)\n",
        "\n",
        "        valid_indices = seq_info[seq_info['fold'] == fold].index\n",
        "        oof_proba[valid_indices] = fold_proba\n",
        "        oof_preds[valid_indices] = fold_preds\n",
        "\n",
        "        y_valid_orig = le.inverse_transform(y_valid)\n",
        "        preds_orig = le.inverse_transform(fold_preds)\n",
        "\n",
        "        temp_sol_df = pd.DataFrame({\"gesture\": y_valid_orig})\n",
        "        temp_sub_df = pd.DataFrame({\"gesture\": preds_orig})\n",
        "        fold_score = metric_calculator.calculate_hierarchical_f1(temp_sol_df, temp_sub_df)\n",
        "\n",
        "        oof_scores.append(fold_score)\n",
        "        print(f\"  Fold {fold+1} SWA Score: {fold_score:.4f}\\n\")\n",
        "\n",
        "    print(f\"Mean OOF Score with SWA: {np.mean(oof_scores):.4f}\")\n",
        "    print(f\"Std  OOF Score with SWA: {np.std(oof_scores):.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL OOF CALCULATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    seq_level_df = train_df.groupby('sequence_id')[target_col].last().reset_index()\n",
        "    seq_level_df = seq_level_df.merge(seq_info[['sequence_id']], on='sequence_id', how='inner')\n",
        "    seq_level_df = seq_level_df.sort_values('sequence_id').reset_index(drop=True)\n",
        "\n",
        "    print(seq_level_df.head(100))\n",
        "\n",
        "    if isinstance(seq_level_df[target_col].iloc[0], (int, np.integer)):\n",
        "        original_labels = le.inverse_transform(seq_level_df[target_col])\n",
        "    else:\n",
        "        original_labels = seq_level_df[target_col].values\n",
        "\n",
        "    oof_preds_encoded = np.argmax(oof_proba, axis=1)\n",
        "    oof_preds_original = le.inverse_transform(oof_preds_encoded)\n",
        "\n",
        "    sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "    sub_df = pd.DataFrame({\"gesture\": oof_preds_original})\n",
        "\n",
        "    print(f\"Ground truth shape: {sol_df.shape}\")\n",
        "    print(f\"Predictions shape: {sub_df.shape}\")\n",
        "    print(f\"Unique ground truth gestures: {len(sol_df['gesture'].unique())}\")\n",
        "    print(f\"Unique predicted gestures: {len(sub_df['gesture'].unique())}\")\n",
        "\n",
        "    # 5. Save OOF predictions\n",
        "    np.save('oof_preds_cnn.npy', oof_preds_original)\n",
        "    np.save('oof_proba_cnn.npy', oof_proba)\n",
        "    print(\"Saved OOF predictions to 'oof_preds_cnn.npy' and 'oof_proba_cnn.npy'\")\n",
        "\n",
        "    # 6. Compute and print overall hierarchical F1\n",
        "    overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "    print(f\"\\nOverall CNN OOF Score: {overall_oof:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return oof_preds, oof_proba, oof_scores, overall_oof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "ePeIRVU3U3-r",
        "outputId": "a913b905-d499-4722-8260-7d796d03a967"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB8fna4crF8w"
      },
      "outputs": [],
      "source": [
        "# train.tof_1_mean_distance.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOf131tLh2L1",
        "outputId": "439efeff-8e01-4722-8344-54bbb30c88a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "set_seed(42)\n",
        "\n",
        "add_swa_mixup_to_cnn_class()\n",
        "\n",
        "velocity_features = []\n",
        "centered_vel_features = []\n",
        "rolling_features = []\n",
        "\n",
        "thm_vel_features = []\n",
        "thm_roll_features = []\n",
        "tof_vel_features = []\n",
        "tof_roll_features = []\n",
        "tof_regional_features = []\n",
        "\n",
        "\n",
        "imu_features = [\n",
        "                'acc_x', 'acc_y', 'acc_z',\n",
        "                'acc_mag',\n",
        "                'rot_w', 'rot_x', 'rot_y', 'rot_z',\n",
        "                'rot_mag',\n",
        "                'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
        "                'angular_distance',\n",
        "                ]\n",
        "\n",
        "thm_features = [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]\n",
        "tof_features = [\"tof_1_mean_distance\", \"tof_2_mean_distance\", \"tof_3_mean_distance\", \"tof_4_mean_distance\", \"tof_5_mean_distance\"]\n",
        "\n",
        "for col in ['acc_x', 'acc_y', 'acc_z', 'acc_mag', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z']:\n",
        "        velocity_features.append(f\"{col}_vel\")\n",
        "\n",
        "for window in [3, 5, 10]:\n",
        "  for col in ['angular_vel_x', 'angular_vel_y', 'angular_vel_z']:\n",
        "\n",
        "      rolling_features.extend([\n",
        "          f\"{col}_roll_std_{window}\",\n",
        "          f\"{col}_roll_min_{window}\",\n",
        "          # f\"{col}_roll_sum_{window}\",\n",
        "          # f\"{col}_roll_var_{window}\",\n",
        "      ])\n",
        "\n",
        "for col in thm_features:\n",
        "  thm_vel_features.append(f\"{col}_vel\")\n",
        "\n",
        "\n",
        "\n",
        "for mode in [4]: # 4, 8\n",
        "        for sensor_idx in range(1, 6):\n",
        "            for region_idx in range(mode):\n",
        "                tof_regional_features.extend([\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_mean',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_std',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_min',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_max',\n",
        "                ])\n",
        "\n",
        "\n",
        "\n",
        "features = imu_features + velocity_features + thm_features + thm_vel_features + tof_regional_features\n",
        "\n",
        "oof_preds, oof_proba, oof_scores, overall_oof = train_cnn_cross_validation_with_swa(\n",
        "    train_df=train,\n",
        "    features=features,  # or FEATURES\n",
        "    target_col='gesture',\n",
        "    demographics_df=train_demographics,\n",
        "    n_splits=5,\n",
        "    label_encoder=None,\n",
        "    aggregation_method=\"swa\",\n",
        "    mixup_prob = 1.0,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Individual Fold Scores: {[f'{score:.4f}' for score in oof_scores]}\")\n",
        "print(f\"Mean Fold Score: {np.mean(oof_scores):.4f}  {np.std(oof_scores):.4f}\")\n",
        "print(f\"Overall OOF Score: {overall_oof:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90wOIOwzFPHj"
      },
      "outputs": [],
      "source": [
        "## 1.0 mixup, 0.1 label smoothing\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7839', '0.7844', '0.7860', '0.7790', '0.7714']\n",
        "# Mean Fold Score: 0.7810  0.0053\n",
        "# Overall OOF Score: 0.7816"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTNGP9lR2dF2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2259b654eec94d069520f16cac24309f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77a862d263554f99ac0f14d7dfd9f843",
              "IPY_MODEL_9343bf16eb9045429c062cf6cddd6c93",
              "IPY_MODEL_2d9a8ce6b341495a841e80e860d6f643",
              "IPY_MODEL_4d7300ff1d374cb08af241b78eb85b58",
              "IPY_MODEL_3fc962878ac24cfd8563532614876248"
            ],
            "layout": "IPY_MODEL_7c4921bd8f424e518142dad349a009a7"
          }
        },
        "2d4a57ba796f4c3fb03d38dbf39fc2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d9a8ce6b341495a841e80e860d6f643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4a2c65e9f9f7492397ac57355a2533f0",
            "placeholder": "",
            "style": "IPY_MODEL_e030150d66294b4cbe907a4e88954a63",
            "value": ""
          }
        },
        "3fc962878ac24cfd8563532614876248": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd2b9177cf6842e688a5965b3ae3665a",
            "placeholder": "",
            "style": "IPY_MODEL_6b4a29d0d0ce4b628d9eeafa1c8e11be",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "4a2c65e9f9f7492397ac57355a2533f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7300ff1d374cb08af241b78eb85b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b7f6a55bd73949fab49e0cd5ee38cd0f",
            "style": "IPY_MODEL_c8c9d059fbad438bb4ea3c06ee81fed1",
            "tooltip": ""
          }
        },
        "6b4a29d0d0ce4b628d9eeafa1c8e11be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77a862d263554f99ac0f14d7dfd9f843": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa908f4807c41daa36c4f8ece24da6b",
            "placeholder": "",
            "style": "IPY_MODEL_93f74a8a9f454852ba6280b6b19edc9c",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "7c4921bd8f424e518142dad349a009a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8aa908f4807c41daa36c4f8ece24da6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9343bf16eb9045429c062cf6cddd6c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e6c3a7f615dc4ac190162e5a9d29d864",
            "placeholder": "",
            "style": "IPY_MODEL_2d4a57ba796f4c3fb03d38dbf39fc2fd",
            "value": ""
          }
        },
        "93f74a8a9f454852ba6280b6b19edc9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7f6a55bd73949fab49e0cd5ee38cd0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8c9d059fbad438bb4ea3c06ee81fed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e030150d66294b4cbe907a4e88954a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6c3a7f615dc4ac190162e5a9d29d864": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2b9177cf6842e688a5965b3ae3665a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
