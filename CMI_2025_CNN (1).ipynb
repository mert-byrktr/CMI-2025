{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JimYwYfhlVLS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPH1bbFZz7V",
        "outputId": "6a8b90fb-c799-49c2-f872-0528dfdfa22b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "a05291f0ba774954b16f737eb04f18be",
            "dc986db519c84ce7825c1622311a92b0",
            "cf09961130b246fcb97cdf95455a757b",
            "a807eba7ba85431883727ace3b43ebf4",
            "c609cf25010e4ff9b53b9371cbf1556d",
            "d254b5e883e54578b396d34a67b556e8",
            "7e9ac8cd73af4e90a5e19bee0cec5dcc",
            "bdb1cd702a894abf90600c1fda372928",
            "06753cd565854d1089d3bfe3fea964e7",
            "7e52f3ffba404f9cab372ab0448a3b36",
            "54476366eefc4670bad20da1d4efcdbf",
            "ba51e4bab47c451c815033386c68b785",
            "4ccc32405c8941cfb251570e05659654",
            "ccfcae3041754b5fb24af74aa3cff7e3",
            "3de39c331fa846169faf24cef49b17c2",
            "6b63363fdb884e96a694f5b8ae2de7e1",
            "d04417ccfebd412db9f00fe896036845"
          ]
        },
        "id": "ka_oHwl5_5MN",
        "outputId": "dcf0471a-6f8a-4d1b-fd4e-f55533a90c98"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_g6C36S_5MR",
        "outputId": "a094480c-57b1-44a5-b3cc-1befd634cac2"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "cmi_detect_behavior_with_sensor_data_path = kagglehub.competition_download('cmi-detect-behavior-with-sensor-data')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7jDg6tfTvdp",
        "outputId": "3187597a-3b49-43ca-d1f9-4ebfbc1e3649"
      },
      "outputs": [],
      "source": [
        "!pip install iterative-stratification==0.1.7 -qq\n",
        "!pip install polars==1.21.0 -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rYRqCy_5MT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import polars as pl\n",
        "import sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 2000)\n",
        "pd.set_option('display.max_rows', 2000)\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8MqxrueedN6",
        "outputId": "830d13a5-d44c-49f3-ee2c-39ba283d6c65"
      },
      "outputs": [],
      "source": [
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(pl.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(joblib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "gY6LM36r_5MW",
        "outputId": "0b4f1d65-e881-491e-b864-16fa14b86694"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "train = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train.csv')\n",
        "test = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test.csv')\n",
        "train_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/train_demographics.csv')\n",
        "test_demographics = pl.read_csv(f'{cmi_detect_behavior_with_sensor_data_path}/test_demographics.csv')\n",
        "\n",
        "train = train.to_pandas()\n",
        "test = test.to_pandas()\n",
        "train_demographics = train_demographics.to_pandas()\n",
        "test_demographics = test_demographics.to_pandas()\n",
        "\n",
        "train = pd.merge(train, train_demographics, on='subject', how='left')\n",
        "test = pd.merge(test, test_demographics, on='subject', how='left')\n",
        "\n",
        "train.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STB7TmHka7dy",
        "outputId": "33fd8251-d3a6-48df-fd80-df03fa9ab46a"
      },
      "outputs": [],
      "source": [
        "train.gesture.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ic-BSLy1d3"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaWmMuIS_5MZ"
      },
      "outputs": [],
      "source": [
        "class CONFIG:\n",
        "  TARGET = \"gesture\"\n",
        "  SUBJECT = \"subject\"\n",
        "  TRAIN_ONLY_COLS = ['sequence_type', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
        "  NUM_CLASSES = train.gesture.nunique()\n",
        "  FOLDS = 5\n",
        "  ERR = 1e-8\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "imu_cols = [\n",
        "            \"acc_x\", \"acc_y\", \"acc_z\",\n",
        "            \"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\",\n",
        "            \"acc_mag\",\n",
        "\n",
        "            \"euler_roll\", \"euler_pitch\", \"euler_yaw\",\n",
        "            \"euler_total\", \"pitch_roll_ratio\", \"yaw_pitch_ratio\",\n",
        "\n",
        "            \"rot_matrix_r11\", \"rot_matrix_r12\", \"rot_matrix_r13\",\n",
        "            \"rot_matrix_r21\", \"rot_matrix_r22\", \"rot_matrix_r23\",\n",
        "            \"rot_matrix_r31\", \"rot_matrix_r32\", \"rot_matrix_r33\",\n",
        "\n",
        "            \"angular_jerk_x\", \"angular_jerk_y\", \"angular_jerk_z\",\n",
        "            ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7nJrmq5sdv"
      },
      "source": [
        "## FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTDoFvXb_5Ma",
        "outputId": "3ed224cc-4b87-4731-b99e-ba63cc7116ab"
      },
      "outputs": [],
      "source": [
        "def cast_to_object(df):\n",
        "  df['adult_child'] = df['adult_child'].astype(\"category\")\n",
        "  df['sex'] = df['sex'].astype(\"category\")\n",
        "  df['handedness'] = df['handedness'].astype(\"category\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "\n",
        "    if isinstance(acc_data, pd.DataFrame):\n",
        "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    else:\n",
        "        acc_values = acc_data\n",
        "\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = acc_values.shape[0]\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "\n",
        "    gravity_world = np.array([0, 0, 9.81])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "             linear_accel[i, :] = acc_values[i, :]\n",
        "\n",
        "    return linear_accel\n",
        "\n",
        "\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_vel = np.zeros((num_samples, 3))\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q_t = quat_values[i]\n",
        "        q_t_plus_dt = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "\n",
        "\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "\n",
        "\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    return angular_vel\n",
        "\n",
        "def calculate_angular_distance(rot_data):\n",
        "    if isinstance(rot_data, pd.DataFrame):\n",
        "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    else:\n",
        "        quat_values = rot_data\n",
        "\n",
        "    num_samples = quat_values.shape[0]\n",
        "    angular_dist = np.zeros(num_samples)\n",
        "\n",
        "    for i in range(num_samples - 1):\n",
        "        q1 = quat_values[i]\n",
        "        q2 = quat_values[i+1]\n",
        "\n",
        "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
        "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
        "            angular_dist[i] = 0 \n",
        "            continue\n",
        "        try:\n",
        "            r1 = R.from_quat(q1)\n",
        "            r2 = R.from_quat(q2)\n",
        "\n",
        "\n",
        "            relative_rotation = r1.inv() * r2\n",
        "\n",
        "\n",
        "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
        "            angular_dist[i] = angle\n",
        "        except ValueError:\n",
        "            angular_dist[i] = 0 \n",
        "            pass\n",
        "\n",
        "    return angular_dist\n",
        "\n",
        "def calc_angular_velocity(df):\n",
        "    res = calculate_angular_velocity_from_quat( df[['rot_x', 'rot_y', 'rot_z', 'rot_w']] )\n",
        "    res = pd.DataFrame(res, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=df.index)\n",
        "    return res\n",
        "\n",
        "angular_velocity_df = train.groupby('sequence_id').apply(calc_angular_velocity, include_groups=False)\n",
        "angular_velocity_df = angular_velocity_df.droplevel('sequence_id')\n",
        "train = train.join(angular_velocity_df)\n",
        "\n",
        "def calc_angular_distance(df):\n",
        "    res = calculate_angular_distance(df[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
        "    res = pd.DataFrame(res, columns=['angular_distance'], index=df.index)\n",
        "    return res\n",
        "\n",
        "angular_distance_df = train.groupby('sequence_id').apply(calc_angular_distance, include_groups=False)\n",
        "angular_distance_df = angular_distance_df.droplevel('sequence_id')\n",
        "train = train.join(angular_distance_df)\n",
        "\n",
        "def quaternion_to_euler(w, x, y, z):\n",
        "    \"\"\"Convert quaternion to Euler angles\"\"\"\n",
        "    sinr_cosp = 2 * (w * x + y * z)\n",
        "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
        "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
        "\n",
        "    sinp = 2 * (w * y - z * x)\n",
        "    pitch = np.where(np.abs(sinp) >= 1, np.copysign(np.pi / 2, sinp), np.arcsin(sinp))\n",
        "\n",
        "    siny_cosp = 2 * (w * z + x * y)\n",
        "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
        "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
        "\n",
        "    return roll, pitch, yaw\n",
        "\n",
        "\n",
        "def mag_features(df):\n",
        "\n",
        "    df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"]**2 + df[\"acc_y\"]**2 + df[\"acc_z\"]**2)\n",
        "    df[\"rot_mag\"] = np.sqrt(df[\"rot_x\"]**2 + df[\"rot_y\"]**2 + df[\"rot_z\"]**2)\n",
        "\n",
        "    linear_acc = remove_gravity_from_acc(\n",
        "            df[['acc_x', 'acc_y', 'acc_z']],\n",
        "            df[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "        )\n",
        "    df['linear_acc_x'] = linear_acc[:, 0]\n",
        "    df['linear_acc_y'] = linear_acc[:, 1]\n",
        "    df['linear_acc_z'] = linear_acc[:, 2]\n",
        "\n",
        "    if 'linear_acc_x' in df.columns:\n",
        "        df[\"linear_acc_mag\"] = np.sqrt(df[\"linear_acc_x\"]**2 + df[\"linear_acc_y\"]**2 + df[\"linear_acc_z\"]**2)\n",
        "\n",
        "    if 'angular_vel_x' in df.columns:\n",
        "        df[\"angular_vel_mag\"] = np.sqrt(df[\"angular_vel_x\"]**2 + df[\"angular_vel_y\"]**2 + df[\"angular_vel_z\"]**2)\n",
        "\n",
        "    roll, pitch, yaw = quaternion_to_euler(df[\"rot_w\"], df[\"rot_x\"], df[\"rot_y\"], df[\"rot_z\"])\n",
        "    df[\"euler_roll\"] = roll\n",
        "    df[\"euler_pitch\"] = pitch\n",
        "    df[\"euler_yaw\"] = yaw\n",
        "    df[\"euler_mag\"] = np.sqrt(roll**2 + pitch**2 + yaw**2)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def temporal_features(df):\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "        # Normalize sequence counter to 0-1 range per sequence\n",
        "    df = df.with_columns([\n",
        "        ((pl.col('sequence_counter') - pl.col('sequence_counter').min().over('sequence_id')) /\n",
        "         (pl.col('sequence_counter').max().over('sequence_id') - pl.col('sequence_counter').min().over('sequence_id') + CONFIG.ERR))\n",
        "        .alias('normalized_position')\n",
        "    ])\n",
        "\n",
        "    df = df.with_columns([\n",
        "        (pl.col('normalized_position') * 2 * np.pi).sin().alias('position_sin'),\n",
        "        (pl.col('normalized_position') * 2 * np.pi).cos().alias('position_cos'),\n",
        "        (pl.col('normalized_position') * 4 * np.pi).sin().alias('position_sin_2h'),  # Second harmonic\n",
        "        (pl.col('normalized_position') * 4 * np.pi).cos().alias('position_cos_2h'),\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "    temporal_exprs = []\n",
        "\n",
        "    # Core IMU + derived columns\n",
        "    core_cols = [\"acc_x\", \"acc_y\", \"acc_z\", \"acc_mag\", \"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\", \"rot_mag\"]\n",
        "    if 'linear_acc_x' in df.columns:\n",
        "        core_cols.extend([\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\", \"linear_acc_mag\"])\n",
        "    if 'angular_vel_x' in df.columns:\n",
        "        core_cols.extend([\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\", \"angular_vel_mag\"])\n",
        "    if 'euler_roll' in df.columns:\n",
        "        core_cols.extend([\"euler_roll\", \"euler_pitch\", \"euler_yaw\", \"euler_mag\"])\n",
        "\n",
        "    for col in core_cols:\n",
        "        temporal_exprs.extend([\n",
        "            pl.col(col).diff().over('sequence_id').alias(f'{col}_vel'),\n",
        "        ])\n",
        "\n",
        "    df = df.with_columns(temporal_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "def rolling_window_features(df, windows=[3, 5, 10]):\n",
        "    \"\"\"Add rolling window features that CNNs can learn from\"\"\"\n",
        "    df = pl.from_pandas(df)\n",
        "\n",
        "    rolling_exprs = []\n",
        "\n",
        "    key_cols = [\"acc_mag\", \"acc_x\", \"acc_y\", \"acc_z\", 'rot_w', 'rot_x', 'rot_y', 'rot_z',\n",
        "                'rot_mag', \"linear_acc_mag\", \"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\",\n",
        "                \"euler_roll\", \"euler_pitch\", \"euler_yaw\", \"euler_mag\",\n",
        "                \"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\", \"angular_vel_distance\", \"angular_vel_mag\"]\n",
        "\n",
        "    for window in windows:\n",
        "        for col in key_cols:\n",
        "            if col in df.columns:\n",
        "                rolling_exprs.extend([\n",
        "                    pl.col(col).rolling_mean(window).over('sequence_id').alias(f'{col}_roll_mean_{window}'),\n",
        "                    pl.col(col).rolling_std(window).over('sequence_id').alias(f'{col}_roll_std_{window}'),\n",
        "                    pl.col(col).rolling_max(window).over('sequence_id').alias(f'{col}_roll_max_{window}'),\n",
        "                    pl.col(col).rolling_min(window).over('sequence_id').alias(f'{col}_roll_min_{window}'),\n",
        "                    pl.col(col).rolling_sum(window).over('sequence_id').alias(f'{col}_roll_sum_{window}'),\n",
        "\n",
        "                ])\n",
        "\n",
        "    df = df.with_columns(rolling_exprs)\n",
        "\n",
        "    return df.to_pandas()\n",
        "\n",
        "\n",
        "def apply_feature_engineering(df):\n",
        "    print(\"  Applying feature engineering...\")\n",
        "    df = cast_to_object(df)\n",
        "    df = mag_features(df)\n",
        "    df = temporal_features(df)\n",
        "    df = rolling_window_features(df)\n",
        "    print(\"  Feature engineering complete.\")\n",
        "    return df\n",
        "\n",
        "train = apply_feature_engineering(train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ul16-2QYoT"
      },
      "source": [
        "## TOF THM FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX524iTnxA5r"
      },
      "outputs": [],
      "source": [
        "# thm_cols = [\n",
        "#     \"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\",\n",
        "\n",
        "#     # \"thm_12_diff\", \"thm_13_diff\", \"thm_14_diff\",\n",
        "#     # \"thm_15_diff\", \"thm_23_diff\", \"thm_24_diff\", \"thm_25_diff\",\n",
        "#     # \"thm_34_diff\", \"thm_35_diff\", \"thm_45_diff\",\n",
        "\n",
        "# ]\n",
        "\n",
        "# tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
        "\n",
        "# tof_diff_cols = [f\"tof_{i}{j}_mean_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]\n",
        "# tof_diff_cols += [f\"tof_{i}{j}_std_diff\" for i in range(1, 6) for j in range(i+1, 6) if i != j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRYrkG8sbfb7"
      },
      "outputs": [],
      "source": [
        "# train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-IITROb8keh"
      },
      "outputs": [],
      "source": [
        "# def thm_features_func(df):\n",
        "#     \"\"\"Extract features from thermopile sensors\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     thm_sensor_exprs_2 = []\n",
        "#     for col in [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]:\n",
        "#         if col in df.columns:\n",
        "#             thm_sensor_exprs_2.extend([\n",
        "#                 pl.col(col).diff().over('sequence_id').alias(f'{col}_vel'),\n",
        "#             ])\n",
        "\n",
        "#     df = df.with_columns(thm_sensor_exprs_2)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# def tof_features_func(df):\n",
        "#     \"\"\"Extract features from time-of-flight sensors (proximity/distance)\"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     tof_sensor_exprs = []\n",
        "\n",
        "#     for sensor_idx in range(1, 6):  # 5 ToF sensors\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         tof_sensor_exprs.extend([\n",
        "#             pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#               .list.mean().alias(f'tof_{sensor_idx}_mean_distance'),\n",
        "\n",
        "#             pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#               .list.std().alias(f'tof_{sensor_idx}_std_distance'),\n",
        "#         ])\n",
        "\n",
        "#     df = df.with_columns(tof_sensor_exprs)\n",
        "\n",
        "#     return df.to_pandas()\n",
        "\n",
        "\n",
        "\n",
        "# def tof_regional_features(df, tof_mode=\"stats\", include_regions=True):\n",
        "#     \"\"\"\n",
        "#     Extract features from time-of-flight sensors with regional analysis\n",
        "\n",
        "#     Args:\n",
        "#         df: DataFrame with ToF data\n",
        "#         tof_mode: \"stats\" for basic stats, \"regions\" for regional analysis, \"multi\" for multi-resolution\n",
        "#         include_regions: Whether to include regional analysis features\n",
        "#     \"\"\"\n",
        "#     df = pl.from_pandas(df)\n",
        "\n",
        "#     tof_sensor_exprs = []\n",
        "\n",
        "#     # Basic statistics for each ToF sensor (5 sensors total)\n",
        "#     for sensor_idx in range(1, 6):\n",
        "#         pixel_cols = [f'tof_{sensor_idx}_v{i}' for i in range(64)]\n",
        "\n",
        "#         # # Basic stats (replace -1 with null for proper statistics)\n",
        "#         # tof_sensor_exprs.extend([\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.mean().alias(f'tof_{sensor_idx}_mean'),\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.std().alias(f'tof_{sensor_idx}_std'),\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.min().alias(f'tof_{sensor_idx}_min'),\n",
        "#         #     pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in pixel_cols])\n",
        "#         #       .list.max().alias(f'tof_{sensor_idx}_max'),\n",
        "#         # ])\n",
        "\n",
        "#         # Regional Analysis - divide 8x8 grid into regions\n",
        "#         if include_regions and tof_mode in [\"regions\", \"multi\"]:\n",
        "#             # Different region modes\n",
        "#             region_modes = [4] if tof_mode == \"regions\" else [2, 4, 8, 16, 32]\n",
        "\n",
        "#             for mode in region_modes:\n",
        "#                 region_size = 64 // mode  # pixels per region\n",
        "\n",
        "#                 for region_idx in range(mode):\n",
        "#                     start_pixel = region_idx * region_size\n",
        "#                     end_pixel = (region_idx + 1) * region_size\n",
        "\n",
        "#                     # Get pixel columns for this region\n",
        "#                     region_pixel_cols = pixel_cols[start_pixel:end_pixel]\n",
        "\n",
        "#                     # Calculate regional statistics\n",
        "#                     tof_sensor_exprs.extend([\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.mean().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_mean'),\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.std().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_std'),\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.min().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_min'),\n",
        "#                         pl.concat_list([pl.when(pl.col(col) != -1).then(pl.col(col)) for col in region_pixel_cols])\n",
        "#                           .list.max().alias(f'tof{mode}_{sensor_idx}_region_{region_idx}_max'),\n",
        "#                     ])\n",
        "\n",
        "#     df = df.with_columns(tof_sensor_exprs)\n",
        "#     return df.to_pandas()\n",
        "\n",
        "# train = thm_features_func(train)\n",
        "# train = tof_features_func(train)\n",
        "# train = tof_regional_features(train, tof_mode=\"regions\")\n",
        "\n",
        "\n",
        "# # thm_feature_cols = [col for col in train.columns if 'thm_' in col and col not in thm_cols]\n",
        "# # # tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "# # tof_feature_cols = [col for col in train.columns if 'tof_' in col and col not in tof_cols and col not in tof_diff_cols]\n",
        "\n",
        "\n",
        "# # FEATURES_FULL = FEATURES + thm_feature_cols + tof_feature_cols\n",
        "# # print(len(FEATURES_FULL))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJuAJBGWQWSk"
      },
      "source": [
        "## REDUCE MEMORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOxRa7Mt_5Mb",
        "outputId": "9b71a9fc-8577-4739-a3ec-2a4ec48a8490"
      },
      "outputs": [],
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2 # calculate current memory usage\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type in numerics: # check if column is numeric\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type).startswith('int'): # if integer\n",
        "                # Check if data can be safely cast to smaller int types\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64) # Should already be this or smaller if loaded as int\n",
        "            else: # if float\n",
        "                # Check if data can be safely cast to float32 (float16 often loses too much precision)\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                # else: # If not, keep as float64\n",
        "                #     df[col] = df[col].astype(np.float64) # Already this type\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(f'Memory usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "    return df\n",
        "\n",
        "print(\"Reducing memory for train_df:\")\n",
        "train = reduce_mem_usage(train)\n",
        "print(\"\\nReducing memory for test_df:\")\n",
        "test = reduce_mem_usage(test)\n",
        "\n",
        "print(\"\\nTrain DataFrame info after memory reduction:\")\n",
        "train.info(memory_usage='deep')\n",
        "print(\"\\nTest DataFrame info after memory reduction:\")\n",
        "test.info(memory_usage='deep')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO30PQozOlbg"
      },
      "outputs": [],
      "source": [
        "# numeric_df = train[FEATURES]\n",
        "# corr = numeric_df.corr(method = 'pearson')\n",
        "# corr = corr.abs()\n",
        "# # corr.style.background_gradient(cmap='inferno')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdESwz1wQHwM"
      },
      "outputs": [],
      "source": [
        "# upper_tri_mask = np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
        "# upper_tri = corr.where(upper_tri_mask)\n",
        "# highly_correlated_series = upper_tri.stack()\n",
        "# strong_pairs = highly_correlated_series[highly_correlated_series > 0.90]\n",
        "# strong_pairs_df = strong_pairs.reset_index()\n",
        "# strong_pairs_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
        "# strong_pairs_df_sorted = strong_pairs_df.sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
        "# print(f\"Found {len(strong_pairs_df_sorted)} pairs of features with correlation > 0.90\")\n",
        "# print(\"-\" * 50)\n",
        "# print(strong_pairs_df_sorted.head(200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJYVNzGqBS63"
      },
      "source": [
        "## METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQRvcrT_5Mb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hierarchical macro F1 metric for the CMI 2025 Challenge.\n",
        "\n",
        "This script defines a single entry point `score(solution, submission, row_id_column_name)`\n",
        "that the Kaggle metrics orchestrator will call.\n",
        "It performs validation on submission IDs and computes a combined binary & multiclass F1 score.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    \"\"\"Errors raised here will be shown directly to the competitor.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class CompetitionMetric:\n",
        "    \"\"\"Hierarchical macro F1 for the CMI 2025 challenge.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.target_gestures = [\n",
        "            'Above ear - pull hair',\n",
        "            'Cheek - pinch skin',\n",
        "            'Eyebrow - pull hair',\n",
        "            'Eyelash - pull hair',\n",
        "            'Forehead - pull hairline',\n",
        "            'Forehead - scratch',\n",
        "            'Neck - pinch skin',\n",
        "            'Neck - scratch',\n",
        "        ]\n",
        "        self.non_target_gestures = [\n",
        "            'Write name on leg',\n",
        "            'Wave hello',\n",
        "            'Glasses on/off',\n",
        "            'Text on phone',\n",
        "            'Write name in air',\n",
        "            'Feel around in tray and pull out an object',\n",
        "            'Scratch knee/leg skin',\n",
        "            'Pull air toward your face',\n",
        "            'Drink from bottle/cup',\n",
        "            'Pinch knee/leg skin'\n",
        "        ]\n",
        "        self.all_classes = self.target_gestures + self.non_target_gestures\n",
        "\n",
        "    def calculate_hierarchical_f1(\n",
        "        self,\n",
        "        sol: pd.DataFrame,\n",
        "        sub: pd.DataFrame\n",
        "    ) -> float:\n",
        "\n",
        "        # Validate gestures\n",
        "        invalid_types = {i for i in sub['gesture'].unique() if i not in self.all_classes}\n",
        "        if invalid_types:\n",
        "            raise ParticipantVisibleError(\n",
        "                f\"Invalid gesture values in submission: {invalid_types}\"\n",
        "            )\n",
        "\n",
        "        # Compute binary F1 (Target vs Non-Target)\n",
        "        y_true_bin = sol['gesture'].isin(self.target_gestures).values\n",
        "        y_pred_bin = sub['gesture'].isin(self.target_gestures).values\n",
        "        f1_binary = f1_score(\n",
        "            y_true_bin,\n",
        "            y_pred_bin,\n",
        "            pos_label=True,\n",
        "            zero_division=0,\n",
        "            average='binary'\n",
        "        )\n",
        "\n",
        "        # Build multi-class labels for gestures\n",
        "        y_true_mc = sol['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "        y_pred_mc = sub['gesture'].apply(lambda x: x if x in self.target_gestures else 'non_target')\n",
        "\n",
        "        # Compute macro F1 over all gesture classes\n",
        "        f1_macro = f1_score(\n",
        "            y_true_mc,\n",
        "            y_pred_mc,\n",
        "            average='macro',\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f'f1_binary score: {f1_binary}')\n",
        "        print(f'f1_macro score: {f1_macro}')\n",
        "\n",
        "        return 0.5 * f1_binary + 0.5 * f1_macro\n",
        "\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute hierarchical macro F1 for the CMI 2025 challenge.\n",
        "\n",
        "    Expected input:\n",
        "      - solution and submission as pandas.DataFrame\n",
        "      - Column 'sequence_id': unique identifier for each sequence\n",
        "      - 'gesture': one of the eight target gestures or \"Non-Target\"\n",
        "\n",
        "    This metric averages:\n",
        "    1. Binary F1 on SequenceType (Target vs Non-Target)\n",
        "    2. Macro F1 on gesture (mapping non-targets to \"Non-Target\")\n",
        "\n",
        "    Raises ParticipantVisibleError for invalid submissions,\n",
        "    including invalid SequenceType or gesture values.\n",
        "\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> row_id_column_name = \"id\"\n",
        "    >>> solution = pd.DataFrame({'id': range(4), 'gesture': ['Eyebrow - pull hair']*4})\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Forehead - pull hairline']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.5\n",
        "    >>> submission = pd.DataFrame({'id': range(4), 'gesture': ['Text on phone']*4})\n",
        "    >>> score(solution, submission, row_id_column_name=row_id_column_name)\n",
        "    0.0\n",
        "    >>> score(solution, solution, row_id_column_name=row_id_column_name)\n",
        "    1.0\n",
        "    \"\"\"\n",
        "    # Validate required columns\n",
        "    for col in (row_id_column_name, 'gesture'):\n",
        "        if col not in solution.columns:\n",
        "            raise ParticipantVisibleError(f\"Solution file missing required column: '{col}'\")\n",
        "        if col not in submission.columns:\n",
        "            raise ParticipantVisibleError(f\"Submission file missing required column: '{col}'\")\n",
        "\n",
        "    metric = CompetitionMetric()\n",
        "    return metric.calculate_hierarchical_f1(solution, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRPA6tMxM3Gu"
      },
      "source": [
        "## MIXUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "molB3aeLF64S"
      },
      "outputs": [],
      "source": [
        "# def feature_mixup_for_catboost(X_train, y_train, alpha=0.3, mix_prob=0.5):\n",
        "#     \"\"\"Apply mixup to engineered features for CatBoost\"\"\"\n",
        "#     X_mixed = []\n",
        "#     y_mixed = []\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     for i in range(len(X_train)):\n",
        "#         if np.random.rand() < mix_prob:\n",
        "#             # Apply mixup\n",
        "#             j = np.random.randint(0, len(X_train))\n",
        "#             lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "#             # Mix features (this works for statistical features)\n",
        "#             x_mix = lam * X_train.iloc[i] + (1 - lam) * X_train.iloc[j]\n",
        "\n",
        "#             # For labels, use the dominant class (hard decision)\n",
        "#             if lam > 0.5:\n",
        "#                 y_mix = y_train.iloc[i]\n",
        "#             else:\n",
        "#                 y_mix = y_train.iloc[j]\n",
        "\n",
        "#             X_mixed.append(x_mix)\n",
        "#             y_mixed.append(y_mix)\n",
        "#         else:\n",
        "#             # Keep original\n",
        "#             X_mixed.append(X_train.iloc[i])\n",
        "#             y_mixed.append(y_train.iloc[i])\n",
        "\n",
        "    # return pd.DataFrame(X_mixed), pd.Series(y_mixed)\n",
        "\n",
        "def feature_mixup_for_catboost(X_train, y_train, alpha=0.3, mix_prob=0.5):\n",
        "    \"\"\"\n",
        "    Applies feature space mixup by creating new synthetic rows and adding them\n",
        "    to the original dataset.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Original feature data.\n",
        "        y_train (pd.Series): Original label data.\n",
        "        alpha (float): Beta distribution parameter for Mixup.\n",
        "        augmentation_factor (float): The fraction of new data to generate.\n",
        "                                     0.5 means increase dataset size by 50%.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    num_original_samples = len(X_train)\n",
        "    num_new_samples = int(num_original_samples * mix_prob)\n",
        "\n",
        "    if num_new_samples == 0:\n",
        "        return X_train, y_train\n",
        "\n",
        "    print(f\"Original samples: {num_original_samples}. Adding {num_new_samples} mixed samples.\")\n",
        "\n",
        "    indices1 = np.random.randint(0, num_original_samples, num_new_samples)\n",
        "    indices2 = np.random.randint(0, num_original_samples, num_new_samples)\n",
        "\n",
        "    lams = np.random.beta(alpha, alpha, size=num_new_samples)\n",
        "\n",
        "    X1 = X_train.iloc[indices1].values\n",
        "    X2 = X_train.iloc[indices2].values\n",
        "    X_new = lams[:, np.newaxis] * X1 + (1 - lams)[:, np.newaxis] * X2\n",
        "\n",
        "    y1 = y_train.iloc[indices1].values\n",
        "    y2 = y_train.iloc[indices2].values\n",
        "    y_new = np.where(lams > 0.5, y1, y2)\n",
        "\n",
        "    X_augmented_df = pd.DataFrame(X_new, columns=X_train.columns)\n",
        "    y_augmented_series = pd.Series(y_new)\n",
        "\n",
        "    X_final = pd.concat([X_train, X_augmented_df], ignore_index=True)\n",
        "    y_final = pd.concat([y_train, y_augmented_series], ignore_index=True)\n",
        "\n",
        "    return X_final, y_final\n",
        "\n",
        "\n",
        "# def feature_mixup_for_catboost(X_train, y_train, sequence_ids, alpha=0.3, mix_prob=0.5):\n",
        "#     \"\"\"\n",
        "#     Fast vectorized version of sequence-level mixup.\n",
        "\n",
        "#     Args:\n",
        "#         X_train (pd.DataFrame): Original feature data.\n",
        "#         y_train (pd.Series): Original label data.\n",
        "#         sequence_ids (pd.Series): Sequence ID for each row.\n",
        "#         alpha (float): Beta distribution parameter for Mixup.\n",
        "#         mix_prob (float): The fraction of new sequences to generate.\n",
        "#     \"\"\"\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     # Get unique sequences and their representative indices\n",
        "#     unique_sequences = sequence_ids.unique()\n",
        "#     num_original_sequences = len(unique_sequences)\n",
        "#     num_new_sequences = int(num_original_sequences * mix_prob)\n",
        "\n",
        "#     if num_new_sequences == 0:\n",
        "#         return X_train, y_train\n",
        "\n",
        "#     print(f\"Original sequences: {num_original_sequences}. Adding {num_new_sequences} mixed sequences.\")\n",
        "\n",
        "#     # Create mapping from sequence to first occurrence index (vectorized)\n",
        "#     first_occurrence_mask = ~sequence_ids.duplicated()\n",
        "#     representative_data = X_train[first_occurrence_mask].copy()\n",
        "#     representative_labels = y_train[first_occurrence_mask].copy()\n",
        "#     representative_sequences = sequence_ids[first_occurrence_mask].copy()\n",
        "\n",
        "#     # Create sequence to row count mapping (vectorized)\n",
        "#     sequence_counts = sequence_ids.value_counts()\n",
        "\n",
        "#     # Generate random pairs and mixing weights (all vectorized)\n",
        "#     seq_indices1 = np.random.randint(0, num_original_sequences, num_new_sequences)\n",
        "#     seq_indices2 = np.random.randint(0, num_original_sequences, num_new_sequences)\n",
        "#     lams = np.random.beta(alpha, alpha, size=num_new_sequences)\n",
        "\n",
        "#     # Get the actual sequence IDs for selected indices\n",
        "#     selected_seqs1 = representative_sequences.iloc[seq_indices1].values\n",
        "#     selected_seqs2 = representative_sequences.iloc[seq_indices2].values\n",
        "\n",
        "#     # Mix features (fully vectorized)\n",
        "#     X1 = representative_data.iloc[seq_indices1].values\n",
        "#     X2 = representative_data.iloc[seq_indices2].values\n",
        "#     X_mixed_base = lams[:, np.newaxis] * X1 + (1 - lams)[:, np.newaxis] * X2\n",
        "\n",
        "#     # Mix labels (vectorized hard assignment)\n",
        "#     y1 = representative_labels.iloc[seq_indices1].values\n",
        "#     y2 = representative_labels.iloc[seq_indices2].values\n",
        "#     y_mixed_base = np.where(lams > 0.5, y1, y2)\n",
        "\n",
        "#     # Calculate row counts for mixed sequences (vectorized)\n",
        "#     counts1 = sequence_counts[selected_seqs1].values\n",
        "#     counts2 = sequence_counts[selected_seqs2].values\n",
        "#     mixed_counts = ((counts1 + counts2) / 2).astype(int)\n",
        "\n",
        "#     # Calculate total rows needed\n",
        "#     total_new_rows = mixed_counts.sum()\n",
        "\n",
        "#     # Pre-allocate arrays for maximum speed\n",
        "#     X_new = np.empty((total_new_rows, X_train.shape[1]), dtype=X_train.dtypes.iloc[0])\n",
        "#     y_new = np.empty(total_new_rows, dtype=y_train.dtype)\n",
        "\n",
        "#     # Fill arrays using vectorized operations with repeat\n",
        "#     current_idx = 0\n",
        "#     for i, count in enumerate(mixed_counts):\n",
        "#         end_idx = current_idx + count\n",
        "#         # Use numpy repeat for each mixed sample\n",
        "#         X_new[current_idx:end_idx] = np.repeat(X_mixed_base[i:i+1], count, axis=0)\n",
        "#         y_new[current_idx:end_idx] = y_mixed_base[i]\n",
        "#         current_idx = end_idx\n",
        "\n",
        "#     # Convert to pandas (single operation)\n",
        "#     X_new_df = pd.DataFrame(X_new, columns=X_train.columns)\n",
        "#     y_new_series = pd.Series(y_new)\n",
        "\n",
        "#     # Concatenate (single operation)\n",
        "#     X_final = pd.concat([X_train, X_new_df], ignore_index=True)\n",
        "#     y_final = pd.concat([y_train, y_new_series], ignore_index=True)\n",
        "\n",
        "#     return X_final, y_final\n",
        "\n",
        "def generate_mixed_samples(df, features, target, n_mixed=1000):\n",
        "    \"\"\"Generate mixed samples for CatBoost training\"\"\"\n",
        "    mixed_samples = []\n",
        "\n",
        "    for _ in range(n_mixed):\n",
        "        idx1, idx2 = np.random.choice(len(df), 2, replace=False)\n",
        "        lam = np.random.beta(0.3, 0.3)\n",
        "\n",
        "        mixed_features = lam * df.iloc[idx1][features] + (1 - lam) * df.iloc[idx2][features]\n",
        "        mixed_label = df.iloc[idx1][target] if lam > 0.5 else df.iloc[idx2][target]\n",
        "        mixed_sample = mixed_features.copy()\n",
        "        mixed_sample[target] = mixed_label\n",
        "        mixed_sample['sequence_id'] = f\"mixed_{len(mixed_samples)}\"\n",
        "\n",
        "        mixed_samples.append(mixed_sample)\n",
        "\n",
        "    return pd.DataFrame(mixed_samples)\n",
        "\n",
        "\n",
        "# def mix_time_series_sequences(seq1, seq2, lam):\n",
        "#     \"\"\"Mix two time series sequences\"\"\"\n",
        "#     # Ensure same length\n",
        "#     min_len = min(len(seq1), len(seq2))\n",
        "#     seq1_cut = seq1[:min_len]\n",
        "#     seq2_cut = seq2[:min_len]\n",
        "\n",
        "#     # Linear interpolation\n",
        "#     mixed_seq = lam * seq1_cut + (1 - lam) * seq2_cut\n",
        "\n",
        "#     return mixed_seq\n",
        "\n",
        "# def create_mixed_sequences(df, n_mixed=500):\n",
        "#     \"\"\"Create mixed sequences, then extract features\"\"\"\n",
        "#     mixed_data = []\n",
        "\n",
        "#     sequence_ids = df['sequence_id'].unique()\n",
        "\n",
        "#     for _ in range(n_mixed):\n",
        "#         # Sample two sequences\n",
        "#         seq_id1, seq_id2 = np.random.choice(sequence_ids, 2, replace=False)\n",
        "#         seq1_data = df[df['sequence_id'] == seq_id1]\n",
        "#         seq2_data = df[df['sequence_id'] == seq_id2]\n",
        "\n",
        "#         lam = np.random.beta(0.3, 0.3)\n",
        "\n",
        "#         # Mix sensor readings\n",
        "#         mixed_features = {}\n",
        "#         for col in ['acc_x', 'acc_y', 'acc_z', 'euler_roll', 'euler_pitch', 'euler_yaw']:\n",
        "#             if col in df.columns:\n",
        "#                 mixed_col = lam * seq1_data[col].values + (1 - lam) * seq2_data[col].values\n",
        "#                 mixed_features[col] = mixed_col\n",
        "\n",
        "#         # Extract features from mixed sequence\n",
        "#         mixed_seq_features = extract_sequence_features(mixed_features)\n",
        "\n",
        "#         # Choose dominant label\n",
        "#         label = seq1_data['gesture'].iloc[0] if lam > 0.5 else seq2_data['gesture'].iloc[0]\n",
        "#         mixed_seq_features['gesture'] = label\n",
        "\n",
        "#         mixed_data.append(mixed_seq_features)\n",
        "\n",
        "#     return pd.DataFrame(mixed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Iq3yxoGslbh"
      },
      "outputs": [],
      "source": [
        "# train = train.groupby('sequence_id').last().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOrm302jDZ6P"
      },
      "source": [
        "## CNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpze6U9aCrS-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import uuid\n",
        "import random\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "metric_calculator = CompetitionMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xTMV2YDUFep",
        "outputId": "746baaa3-e115-4177-9c0b-f924cb448ef0"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.experimental.numpy.random.seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print(f\"Seeds set to {seed} for reproducibility\")\n",
        "\n",
        "set_seed(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XqhN_lyh2UK"
      },
      "outputs": [],
      "source": [
        "class SWACallback(callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Stochastic Weight Averaging callback for Keras\n",
        "    \"\"\"\n",
        "    def __init__(self, start_epoch=10, swa_freq=5, verbose=1):\n",
        "        super().__init__()\n",
        "        self.start_epoch = start_epoch\n",
        "        self.swa_freq = swa_freq\n",
        "        self.verbose = verbose\n",
        "        self.swa_weights = None\n",
        "        self.n_models = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch >= self.start_epoch and (epoch - self.start_epoch) % self.swa_freq == 0:\n",
        "            current_weights = self.model.get_weights()\n",
        "\n",
        "            if self.swa_weights is None:\n",
        "                self.swa_weights = [w.copy() for w in current_weights]\n",
        "                self.n_models = 1\n",
        "            else:\n",
        "                self.n_models += 1\n",
        "                for i in range(len(self.swa_weights)):\n",
        "                    self.swa_weights[i] = (\n",
        "                        (self.n_models - 1) * self.swa_weights[i] + current_weights[i]\n",
        "                    ) / self.n_models\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"SWA: Updated weights at epoch {epoch + 1} (n_models: {self.n_models})\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.swa_weights is not None:\n",
        "            self.model.set_weights(self.swa_weights)\n",
        "            if self.verbose:\n",
        "                print(f\"SWA: Applied averaged weights from {self.n_models} models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nfyJxvfbOfC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "class MixupSequence(Sequence):\n",
        "    def __init__(self, X, y, batch_size=32, alpha=0.3, mixup_prob=0.5, mixup_seed=42):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.mixup_prob = mixup_prob\n",
        "        self.indices = np.arange(len(X))\n",
        "        self.seed = mixup_seed\n",
        "\n",
        "        self.total_batches = int(np.ceil(len(X) / batch_size))\n",
        "        self.rng = np.random.RandomState(mixup_seed)\n",
        "        self.reset_random_states()\n",
        "\n",
        "    def reset_random_states(self):\n",
        "        self.rng = np.random.RandomState(self.seed)\n",
        "        self.mixup_decisions = self.rng.rand(self.total_batches)\n",
        "        self.permutation_seeds = self.rng.randint(0, 10000, self.total_batches)\n",
        "        self.lambda_seeds = self.rng.randint(0, 10000, self.total_batches)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_batches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch = self.X[batch_indices]\n",
        "        y_batch = self.y[batch_indices]\n",
        "\n",
        "        if self.mixup_decisions[idx] < self.mixup_prob:\n",
        "            X_batch, y_batch = self._mixup_batch_reproducible(X_batch, y_batch, idx)\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def _mixup_batch_reproducible(self, X_batch, y_batch, batch_idx):\n",
        "        batch_size = len(X_batch)\n",
        "\n",
        "        perm_rng = np.random.RandomState(self.permutation_seeds[batch_idx])\n",
        "        lambda_rng = np.random.RandomState(self.lambda_seeds[batch_idx])\n",
        "        indices = perm_rng.permutation(batch_size)\n",
        "        lam = lambda_rng.beta(self.alpha, self.alpha, batch_size)\n",
        "        X_mixed = lam[:, np.newaxis, np.newaxis] * X_batch + (1 - lam[:, np.newaxis, np.newaxis]) * X_batch[indices]\n",
        "        y_mixed = lam[:, np.newaxis] * y_batch + (1 - lam[:, np.newaxis]) * y_batch[indices]\n",
        "\n",
        "        return X_mixed, y_mixed\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        shuffle_rng = np.random.RandomState(self.seed + 1000)\n",
        "        shuffle_rng.shuffle(self.indices)\n",
        "\n",
        "        self.seed += 1\n",
        "        self.reset_random_states()\n",
        "\n",
        "# def simple_additive_mixup(X, y, num_classes, mixup_prob=0.5, alpha=0.3, seed=42):\n",
        "#     \"\"\"\n",
        "#     Simple function to add mixup samples to your existing training data\n",
        "\n",
        "#     Args:\n",
        "#         X: Training sequences (n_samples, seq_len, features)\n",
        "#         y: Training labels (n_samples,) - integers\n",
        "#         num_classes: Number of classes\n",
        "#         augment_ratio: How much extra data to generate (0.5 = 50% more)\n",
        "#         alpha: Mixup alpha parameter\n",
        "#         seed: Random seed\n",
        "\n",
        "#     Returns:\n",
        "#         X_augmented: Original + mixed data\n",
        "#         y_augmented: Original + mixed labels (one-hot)\n",
        "#     \"\"\"\n",
        "#     np.random.seed(seed)\n",
        "\n",
        "#     n_original = len(X)\n",
        "#     n_mixed = int(n_original * mixup_prob)\n",
        "\n",
        "#     print(f\"Adding {n_mixed} mixed samples to {n_original} original samples\")\n",
        "\n",
        "#     # Generate random pairs\n",
        "#     idx1 = np.random.randint(0, n_original, n_mixed)\n",
        "#     idx2 = np.random.randint(0, n_original, n_mixed)\n",
        "\n",
        "#     # Generate lambda values\n",
        "#     lam = np.random.beta(alpha, alpha, n_mixed)\n",
        "\n",
        "#     # Create mixed samples\n",
        "#     X_mixed = np.zeros((n_mixed, X.shape[1], X.shape[2]))\n",
        "#     y_mixed = np.zeros((n_mixed, num_classes))\n",
        "\n",
        "#     for i in range(n_mixed):\n",
        "#         # Mix sequences\n",
        "#         X_mixed[i] = lam[i] * X[idx1[i]] + (1 - lam[i]) * X[idx2[i]]\n",
        "\n",
        "#         # Mix labels (soft labels)\n",
        "#         y_mixed[i, y[idx1[i]]] += lam[i]\n",
        "#         y_mixed[i, y[idx2[i]]] += (1 - lam[i])\n",
        "\n",
        "#     # Combine original + mixed\n",
        "#     X_augmented = np.vstack([X, X_mixed])\n",
        "#     y_original_onehot = to_categorical(y, num_classes)\n",
        "#     y_augmented = np.vstack([y_original_onehot, y_mixed])\n",
        "\n",
        "#     print(f\"Final dataset: {len(X_augmented)} samples\")\n",
        "#     return X_augmented, y_augmented\n",
        "\n",
        "\n",
        "# Modified fit function for your CNN class with SWA + Mixup\n",
        "def fit_with_swa_and_mixup(self, X, y, validation_data=None, epochs=100, batch_size=32,\n",
        "                          alpha=0.3, mixup_prob=0.5, swa_start=20, swa_freq=5, verbose=1):\n",
        "    \"\"\"Train the model with both SWA and mixup data generator\"\"\"\n",
        "\n",
        "    X_scaled = self.scale_features(X, fit=True)\n",
        "    y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "    train_generator = MixupSequence(\n",
        "        X_scaled, y_cat,\n",
        "        batch_size=batch_size,\n",
        "        alpha=alpha,\n",
        "        mixup_prob=mixup_prob,\n",
        "        mixup_seed = 42,\n",
        "    )\n",
        "    val_data = None\n",
        "    if validation_data is not None:\n",
        "        X_val, y_val = validation_data\n",
        "        X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "        y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "        val_data = (X_val_scaled, y_val_cat)\n",
        "    self.swa_callback = SWACallback(\n",
        "        start_epoch=swa_start,\n",
        "        swa_freq=swa_freq,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    # Callbacks including SWA\n",
        "    callback_list = [\n",
        "        self.swa_callback, \n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            patience=35,  \n",
        "            restore_best_weights=False,  \n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            factor=0.7,  \n",
        "            patience=10,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = self.model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_data,\n",
        "        epochs=epochs,\n",
        "        callbacks=callback_list,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# def fit_with_swa_and_mixup(self, X, y, validation_data=None, epochs=100, batch_size=32,\n",
        "#                             alpha=0.3, mixup_prob=0.5, swa_start=20, swa_freq=5, verbose=1):\n",
        "#     \"\"\"\n",
        "#     Simple additive mixup: generate extra data once, then train normally\n",
        "#     \"\"\"\n",
        "\n",
        "#     # 1. Generate additional mixed data\n",
        "#     X_aug, y_aug = simple_additive_mixup(\n",
        "#         X, y, self.num_classes,\n",
        "#         alpha=alpha,\n",
        "#         mixup_prob=mixup_prob,\n",
        "#     )\n",
        "\n",
        "#     # 2. Scale features\n",
        "#     X_scaled = self.scale_features(X_aug, fit=True)\n",
        "\n",
        "#     # 3. Prepare validation data\n",
        "#     val_data = None\n",
        "#     if validation_data is not None:\n",
        "#         X_val, y_val = validation_data\n",
        "#         X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "#         y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "#         val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "#     # 4. SWA callback\n",
        "#     self.swa_callback = SWACallback(\n",
        "#         start_epoch=swa_start,\n",
        "#         swa_freq=swa_freq,\n",
        "#         verbose=verbose\n",
        "#     )\n",
        "\n",
        "#     # 5. Regular training (no special generators needed)\n",
        "#     history = self.model.fit(\n",
        "#         X_scaled, y_aug,  # Augmented data with soft labels\n",
        "#         validation_data=val_data,\n",
        "#         epochs=epochs,\n",
        "#         batch_size=batch_size,\n",
        "#         callbacks=[\n",
        "#             self.swa_callback,\n",
        "#             callbacks.EarlyStopping(monitor='val_loss', patience=35, verbose=1),\n",
        "#             callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=10, verbose=1)\n",
        "#         ],\n",
        "#         verbose=verbose\n",
        "#     )\n",
        "\n",
        "#     return history\n",
        "\n",
        "def add_swa_mixup_to_cnn_class():\n",
        "    CNN1DModelWithSWA.fit_with_swa_and_mixup = fit_with_swa_and_mixup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu_svyANzJSK"
      },
      "outputs": [],
      "source": [
        "# class CNN1DModelWithSWA:\n",
        "#     def __init__(self, input_shape, num_classes, learning_rate=1e-3):\n",
        "#         self.input_shape = input_shape\n",
        "#         self.num_classes = num_classes\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.model = None\n",
        "#         self.scaler = StandardScaler()\n",
        "\n",
        "#     def create_branch(self, inputs, name_prefix, kernel_sizes, filters):\n",
        "#         \"\"\"Create a specialized branch\"\"\"\n",
        "#         x = inputs\n",
        "\n",
        "#         for i, (k_size, f) in enumerate(zip(kernel_sizes, filters)):\n",
        "#             x = layers.Conv1D(f, k_size, padding='same', activation='relu',\n",
        "#                             name=f'{name_prefix}_conv_{i}')(x)\n",
        "#             x = layers.BatchNormalization(name=f'{name_prefix}_bn_{i}')(x)\n",
        "#             if i < len(kernel_sizes) - 1:  # No pooling on last layer\n",
        "#                 x = layers.MaxPooling1D(2, name=f'{name_prefix}_pool_{i}')(x)\n",
        "#             x = layers.Dropout(0.2, name=f'{name_prefix}_dropout_{i}')(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def build_model(self):\n",
        "#         \"\"\"Multi-branch ensemble architecture\"\"\"\n",
        "#         inputs = layers.Input(shape=self.input_shape)\n",
        "\n",
        "#         # Branch 1: Short-term patterns (small kernels)\n",
        "#         branch1 = self.create_branch(\n",
        "#             inputs, 'short_term',\n",
        "#             kernel_sizes=[3, 3, 3],\n",
        "#             filters=[64, 128, 256]\n",
        "#         )\n",
        "\n",
        "#         # Branch 2: Medium-term patterns\n",
        "#         branch2 = self.create_branch(\n",
        "#             inputs, 'medium_term',\n",
        "#             kernel_sizes=[7, 5, 3],\n",
        "#             filters=[64, 128, 256]\n",
        "#         )\n",
        "\n",
        "#         # Branch 3: Long-term patterns (large kernels)\n",
        "#         branch3 = self.create_branch(\n",
        "#             inputs, 'long_term',\n",
        "#             kernel_sizes=[15, 11, 7],\n",
        "#             filters=[64, 128, 256]\n",
        "#         )\n",
        "\n",
        "#         # Combine branches\n",
        "#         combined = layers.Concatenate()([branch1, branch2, branch3])\n",
        "\n",
        "#         # Final processing\n",
        "#         x = layers.Conv1D(512, 3, padding='same', activation='relu')(combined)\n",
        "#         x = layers.BatchNormalization()(x)\n",
        "#         x = layers.GlobalMaxPooling1D()(x)\n",
        "#         x = layers.Dropout(0.4)(x)\n",
        "\n",
        "#         # Classification\n",
        "#         x = layers.Dense(256, activation='relu')(x)\n",
        "#         x = layers.BatchNormalization()(x)\n",
        "#         x = layers.Dropout(0.5)(x)\n",
        "\n",
        "#         outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "#         model = models.Model(inputs, outputs)\n",
        "#         model.compile(\n",
        "#             optimizer=tf.keras.optimizers.AdamW(learning_rate=self.learning_rate),\n",
        "#             loss='categorical_crossentropy',\n",
        "#             metrics=['accuracy']\n",
        "#         )\n",
        "\n",
        "#         self.model = model\n",
        "#         return model\n",
        "\n",
        "#     def prepare_sequences(self, df, features, target_col=None, sequence_length=None):\n",
        "#         \"\"\"Fast sequence preparation using numpy operations\"\"\"\n",
        "#         # Sort once\n",
        "#         df_sorted = df.sort_values(['sequence_id', 'sequence_counter']).reset_index(drop=True)\n",
        "\n",
        "#         # Get sequence boundaries\n",
        "#         seq_changes = df_sorted['sequence_id'].ne(df_sorted['sequence_id'].shift()).cumsum() - 1\n",
        "#         unique_seqs, seq_starts = np.unique(seq_changes, return_index=True)\n",
        "#         seq_ends = np.append(seq_starts[1:], len(df_sorted))\n",
        "#         seq_lengths = seq_ends - seq_starts\n",
        "\n",
        "#         # Determine sequence length\n",
        "#         if sequence_length is None:\n",
        "#             sequence_length = seq_lengths.max()\n",
        "\n",
        "#         # Pre-allocate\n",
        "#         num_sequences = len(seq_starts)\n",
        "#         sequences = np.zeros((num_sequences, sequence_length, len(features)), dtype=np.float32)\n",
        "\n",
        "#         # Get feature matrix\n",
        "#         feature_matrix = df_sorted[features].values.astype(np.float32)\n",
        "\n",
        "#         # Fill sequences using vectorized operations\n",
        "#         for i, (start, end) in enumerate(zip(seq_starts, seq_ends)):\n",
        "#             seq_len = end - start\n",
        "#             actual_len = min(seq_len, sequence_length)\n",
        "#             sequences[i, :actual_len] = feature_matrix[start:start + actual_len]\n",
        "\n",
        "#         if target_col is not None:\n",
        "#             # Get targets using the first occurrence of each sequence\n",
        "#             targets = df_sorted.iloc[seq_starts][target_col].values\n",
        "#             return sequences, targets\n",
        "#         else:\n",
        "#             return sequences\n",
        "\n",
        "#     def fit_with_swa(self, X, y, validation_data=None, epochs=100, batch_size=32,\n",
        "#                      swa_start=20, swa_freq=5, verbose=1):\n",
        "#         \"\"\"Train the model with Stochastic Weight Averaging\"\"\"\n",
        "#         # Scale features\n",
        "#         X_scaled = self.scale_features(X, fit=True)\n",
        "\n",
        "#         # Convert targets to categorical\n",
        "#         y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "\n",
        "#         # Prepare validation data if provided\n",
        "#         val_data = None\n",
        "#         if validation_data is not None:\n",
        "#             X_val, y_val = validation_data\n",
        "#             X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "#             y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "#             val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "#         # Initialize SWA callback\n",
        "#         self.swa_callback = SWACallback(\n",
        "#             start_epoch=swa_start,\n",
        "#             swa_freq=swa_freq,\n",
        "#             verbose=verbose\n",
        "#         )\n",
        "\n",
        "#         # Callbacks including SWA\n",
        "#         callback_list = [\n",
        "#             self.swa_callback,  # SWA callback first\n",
        "\n",
        "#             callbacks.EarlyStopping(\n",
        "#                 monitor='val_loss',\n",
        "#                 patience=35,  # Increased patience for SWA\n",
        "#                 restore_best_weights=False,  # Let SWA handle weights\n",
        "#                 verbose=2\n",
        "#             ),\n",
        "#             callbacks.ReduceLROnPlateau(\n",
        "#                 monitor='val_loss',\n",
        "#                 factor=0.7,  # Less aggressive LR reduction\n",
        "#                 patience=10,\n",
        "#                 min_lr=1e-7,\n",
        "#                 verbose=2\n",
        "#             )\n",
        "#         ]\n",
        "\n",
        "#         # Train model with SWA\n",
        "#         history = self.model.fit(\n",
        "#             X_scaled, y_cat,\n",
        "#             validation_data=val_data,\n",
        "#             epochs=epochs,\n",
        "#             batch_size=batch_size,\n",
        "#             callbacks=callback_list,\n",
        "#             verbose=verbose\n",
        "#         )\n",
        "\n",
        "#         return history\n",
        "\n",
        "#     def fit(self, X, y, validation_data=None, epochs=100, batch_size=32, verbose=1):\n",
        "#         \"\"\"Standard training without SWA (for compatibility)\"\"\"\n",
        "#         return self.fit_with_swa(\n",
        "#             X, y, validation_data, epochs, batch_size,\n",
        "#             swa_start=max(10, epochs//4),  # Start SWA at 1/4 of training\n",
        "#             swa_freq=5,\n",
        "#             verbose=verbose\n",
        "#         )\n",
        "\n",
        "#         # return self.fit_with_ema(\n",
        "#         #     X, y, validation_data, epochs, batch_size,\n",
        "#         #     ema_start=0,\n",
        "#         #     verbose=verbose\n",
        "#         # )\n",
        "\n",
        "#     def predict_proba(self, X):\n",
        "#         \"\"\"Predict probabilities\"\"\"\n",
        "#         X_scaled = self.scale_features(X, fit=False)\n",
        "#         return self.model.predict(X_scaled)\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         \"\"\"Predict classes\"\"\"\n",
        "#         proba = self.predict_proba(X)\n",
        "#         return np.argmax(proba, axis=1)\n",
        "\n",
        "#     def scale_features(self, X, fit=False):\n",
        "#         \"\"\"Scale features across time and feature dimensions\"\"\"\n",
        "#         original_shape = X.shape\n",
        "#         # Reshape to 2D for scaling\n",
        "#         X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "\n",
        "#         if np.any(np.isnan(X_reshaped)) or np.any(np.isinf(X_reshaped)):\n",
        "#             print(f\"WARNING: Found NaN/Inf values in input data!\")\n",
        "#             print(f\"NaN count: {np.sum(np.isnan(X_reshaped))}\")\n",
        "#             print(f\"Inf count: {np.sum(np.isinf(X_reshaped))}\")\n",
        "#             # Replace NaN/Inf with 0\n",
        "#             X_reshaped = np.nan_to_num(X_reshaped, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "#         if fit:\n",
        "#             X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "#         else:\n",
        "#             X_scaled = self.scaler.transform(X_reshaped)\n",
        "\n",
        "#         if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n",
        "#             print(f\"WARNING: Found NaN/Inf values in scaled data!\")\n",
        "#             X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "#         # Reshape back to original shape\n",
        "#         return X_scaled.reshape(original_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqTjR792h2Qw"
      },
      "outputs": [],
      "source": [
        "class CNN1DModelWithSWA:\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=1e-3):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.swa_callback = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build 1D CNN architecture optimized for sensor time series data\"\"\"\n",
        "\n",
        "        model = models.Sequential([\n",
        "\n",
        "            layers.Conv1D(64, kernel_size=7, activation='relu', padding='same', input_shape=self.input_shape),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(pool_size=2),\n",
        "            layers.Dropout(0.2),\n",
        "\n",
        "            layers.Conv1D(128, kernel_size=5, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(pool_size=2),\n",
        "            layers.Dropout(0.2),\n",
        "\n",
        "            layers.Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(pool_size=2),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            layers.Conv1D(512, kernel_size=3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.GlobalMaxPooling1D(),\n",
        "            # layers.GlobalAveragePooling1D(),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.5),\n",
        "\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            layers.Dense(self.num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.AdamW(\n",
        "                learning_rate=self.learning_rate,\n",
        "                weight_decay=1e-5,\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999,\n",
        "                epsilon=1e-8,\n",
        "                clipnorm=1.0\n",
        "            ),\n",
        "            # loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "            #         from_logits=False,\n",
        "            #         label_smoothing=0.05,\n",
        "            #         axis=-1,\n",
        "            #         reduction='sum_over_batch_size',\n",
        "            #         name='categorical_crossentropy'\n",
        "            #     ),\n",
        "            loss = 'categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def prepare_sequences(self, df, features, target_col=None, sequence_length=None):\n",
        "        \"\"\"Fast sequence preparation using numpy operations\"\"\"\n",
        "        df_sorted = df.sort_values(['sequence_id', 'sequence_counter']).reset_index(drop=True)\n",
        "\n",
        "        seq_changes = df_sorted['sequence_id'].ne(df_sorted['sequence_id'].shift()).cumsum() - 1\n",
        "        unique_seqs, seq_starts = np.unique(seq_changes, return_index=True)\n",
        "        seq_ends = np.append(seq_starts[1:], len(df_sorted))\n",
        "        seq_lengths = seq_ends - seq_starts\n",
        "\n",
        "        if sequence_length is None:\n",
        "            sequence_length = seq_lengths.max()\n",
        "\n",
        "        num_sequences = len(seq_starts)\n",
        "        sequences = np.zeros((num_sequences, sequence_length, len(features)), dtype=np.float32)\n",
        "\n",
        "        feature_matrix = df_sorted[features].values.astype(np.float32)\n",
        "\n",
        "        for i, (start, end) in enumerate(zip(seq_starts, seq_ends)):\n",
        "            seq_len = end - start\n",
        "            actual_len = min(seq_len, sequence_length)\n",
        "            sequences[i, :actual_len] = feature_matrix[start:start + actual_len]\n",
        "\n",
        "        if target_col is not None:\n",
        "            targets = df_sorted.iloc[seq_starts][target_col].values\n",
        "            return sequences, targets\n",
        "        else:\n",
        "            return sequences\n",
        "\n",
        "    def fit_with_swa(self, X, y, validation_data=None, epochs=100, batch_size=32,\n",
        "                     swa_start=20, swa_freq=5, verbose=1):\n",
        "        \"\"\"Train the model with Stochastic Weight Averaging\"\"\"\n",
        "        X_scaled = self.scale_features(X, fit=True)\n",
        "\n",
        "        y_cat = to_categorical(y, num_classes=self.num_classes)\n",
        "        val_data = None\n",
        "        if validation_data is not None:\n",
        "            X_val, y_val = validation_data\n",
        "            X_val_scaled = self.scale_features(X_val, fit=False)\n",
        "            y_val_cat = to_categorical(y_val, num_classes=self.num_classes)\n",
        "            val_data = (X_val_scaled, y_val_cat)\n",
        "\n",
        "        self.swa_callback = SWACallback(\n",
        "            start_epoch=swa_start,\n",
        "            swa_freq=swa_freq,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        callback_list = [\n",
        "            self.swa_callback,  \n",
        "\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=35,  \n",
        "                restore_best_weights=False,  \n",
        "                verbose=2\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.7,  \n",
        "                patience=10,\n",
        "                min_lr=1e-7,\n",
        "                verbose=2\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_scaled, y_cat,\n",
        "            validation_data=val_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callback_list,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def fit(self, X, y, validation_data=None, epochs=100, batch_size=32, verbose=1):\n",
        "        \"\"\"Standard training without SWA (for compatibility)\"\"\"\n",
        "        return self.fit_with_swa(\n",
        "            X, y, validation_data, epochs, batch_size,\n",
        "            swa_start=max(10, epochs//4),  \n",
        "            swa_freq=5,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        # return self.fit_with_ema(\n",
        "        #     X, y, validation_data, epochs, batch_size,\n",
        "        #     ema_start=0,\n",
        "        #     verbose=verbose\n",
        "        # )\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities\"\"\"\n",
        "        X_scaled = self.scale_features(X, fit=False)\n",
        "        return self.model.predict(X_scaled)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict classes\"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return np.argmax(proba, axis=1)\n",
        "\n",
        "    def scale_features(self, X, fit=False):\n",
        "        \"\"\"Scale features across time and feature dimensions\"\"\"\n",
        "        original_shape = X.shape\n",
        "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
        "\n",
        "        if np.any(np.isnan(X_reshaped)) or np.any(np.isinf(X_reshaped)):\n",
        "            print(f\"WARNING: Found NaN/Inf values in input data!\")\n",
        "            print(f\"NaN count: {np.sum(np.isnan(X_reshaped))}\")\n",
        "            print(f\"Inf count: {np.sum(np.isinf(X_reshaped))}\")\n",
        "            X_reshaped = np.nan_to_num(X_reshaped, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if fit:\n",
        "            X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "        else:\n",
        "            X_scaled = self.scaler.transform(X_reshaped)\n",
        "\n",
        "        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n",
        "            print(f\"WARNING: Found NaN/Inf values in scaled data!\")\n",
        "            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        return X_scaled.reshape(original_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEHIW5K7ouZa"
      },
      "source": [
        "## TRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37naYx5ih2OP"
      },
      "outputs": [],
      "source": [
        "def train_cnn_cross_validation_with_swa(train_df, features, target_col, demographics_df,\n",
        "                                       n_splits=5, label_encoder=None, aggregation_method=None, mixup_prob=1.0):\n",
        "    \"\"\"Modified cross-validation function for CNN with SWA\"\"\"\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "    import joblib\n",
        "    import uuid\n",
        "\n",
        "    run_id = uuid.uuid4()\n",
        "    os.makedirs('models_cnn_swa', exist_ok=True)\n",
        "\n",
        "    skf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(\n",
        "            skf.split(demographics_df, demographics_df[['adult_child', 'handedness', 'sex']])\n",
        "        ):\n",
        "        demographics_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "    demographics_df['fold'] = demographics_df['fold'].astype(int)\n",
        "    train_df = train_df.merge(demographics_df[['subject', 'fold']], on='subject', how='left')\n",
        "\n",
        "    if label_encoder is None:\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        unique_labels = train_df.groupby('sequence_id')[target_col].first().unique()\n",
        "        le.fit(unique_labels)\n",
        "        print(f\"Created label encoder with classes: {le.classes_}\")\n",
        "    else:\n",
        "        le = label_encoder\n",
        "        print(f\"Using provided label encoder with classes: {le.classes_}\")\n",
        "    joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    oof_preds = np.zeros(len(train_df.groupby('sequence_id').first()), dtype=int)\n",
        "    oof_proba = np.zeros((len(train_df.groupby('sequence_id').first()), num_classes))\n",
        "    oof_scores = []\n",
        "\n",
        "    seq_info = train_df.groupby('sequence_id').first()[['fold']].reset_index()\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        print(f\"{'#'*10} Fold {fold+1} with SWA {'#'*10}\")\n",
        "\n",
        "        train_sequences = seq_info[seq_info['fold'] != fold]['sequence_id'].values\n",
        "        valid_sequences = seq_info[seq_info['fold'] == fold]['sequence_id'].values\n",
        "\n",
        "        train_fold_df = train_df[train_df['sequence_id'].isin(train_sequences)]\n",
        "        valid_fold_df = train_df[train_df['sequence_id'].isin(valid_sequences)]\n",
        "\n",
        "        print(f\"  Train sequences: {len(train_sequences)}, Valid sequences: {len(valid_sequences)}\")\n",
        "\n",
        "        sequence_lengths = train_fold_df.groupby('sequence_id').size()\n",
        "        max_seq_length = int(sequence_lengths.quantile(0.99))\n",
        "\n",
        "        joblib.dump(max_seq_length, f\"models_cnn_swa/seq_length_fold_{fold+1}.pkl\")\n",
        "\n",
        "        cnn_model = CNN1DModelWithSWA(\n",
        "            input_shape=(max_seq_length, len(features)),\n",
        "            num_classes=num_classes,\n",
        "            learning_rate=1e-3\n",
        "        )\n",
        "        cnn_model.build_model()\n",
        "\n",
        "        print(\"Initialized CNN model with SWA\")\n",
        "\n",
        "        X_train, y_train_str = cnn_model.prepare_sequences(\n",
        "            train_fold_df, features, target_col, sequence_length=max_seq_length\n",
        "        )\n",
        "        X_valid, y_valid_str = cnn_model.prepare_sequences(\n",
        "            valid_fold_df, features, target_col, sequence_length=max_seq_length\n",
        "        )\n",
        "        y_train = le.transform(y_train_str)\n",
        "        y_valid = le.transform(y_valid_str)\n",
        "\n",
        "        print(f\"  X_train shape: {X_train.shape}, X_valid shape: {X_valid.shape}\")\n",
        "        print(f\"  y_train shape: {y_train.shape}, y_valid shape: {y_valid.shape}\")\n",
        "\n",
        "        if aggregation_method == \"swa\":\n",
        "            # Use standard SWA\n",
        "            # history = cnn_model.fit_with_swa(\n",
        "            #     X_train, y_train,\n",
        "            #     validation_data=(X_valid, y_valid),\n",
        "            #     epochs=100,\n",
        "            #     batch_size=32,\n",
        "            #     swa_start=20,\n",
        "            #     swa_freq=5,\n",
        "            #     verbose=2\n",
        "            # )\n",
        "\n",
        "            history = cnn_model.fit_with_swa_and_mixup(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_valid, y_valid),\n",
        "                epochs=150,\n",
        "                batch_size=32,\n",
        "                alpha=0.3,\n",
        "                mixup_prob=mixup_prob,\n",
        "                swa_start=40,\n",
        "                swa_freq=5,\n",
        "                verbose=2\n",
        "              )\n",
        "\n",
        "        cnn_model.model.save(f\"models_cnn_swa/cnn_swa_fold_{fold+1}.h5\")\n",
        "        joblib.dump(cnn_model.scaler, f\"models_cnn_swa/scaler_swa_fold_{fold+1}.pkl\")\n",
        "\n",
        "        fold_proba = cnn_model.predict_proba(X_valid)\n",
        "        fold_preds = np.argmax(fold_proba, axis=1)\n",
        "\n",
        "        valid_indices = seq_info[seq_info['fold'] == fold].index\n",
        "        oof_proba[valid_indices] = fold_proba\n",
        "        oof_preds[valid_indices] = fold_preds\n",
        "\n",
        "        y_valid_orig = le.inverse_transform(y_valid)\n",
        "        preds_orig = le.inverse_transform(fold_preds)\n",
        "\n",
        "        temp_sol_df = pd.DataFrame({\"gesture\": y_valid_orig})\n",
        "        temp_sub_df = pd.DataFrame({\"gesture\": preds_orig})\n",
        "        fold_score = metric_calculator.calculate_hierarchical_f1(temp_sol_df, temp_sub_df)\n",
        "\n",
        "        oof_scores.append(fold_score)\n",
        "        print(f\"  Fold {fold+1} SWA Score: {fold_score:.4f}\\n\")\n",
        "\n",
        "    print(f\"Mean OOF Score with SWA: {np.mean(oof_scores):.4f}\")\n",
        "    print(f\"Std  OOF Score with SWA: {np.std(oof_scores):.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL OOF CALCULATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    seq_level_df = train_df.groupby('sequence_id')[target_col].last().reset_index()\n",
        "    seq_level_df = seq_level_df.merge(seq_info[['sequence_id']], on='sequence_id', how='inner')\n",
        "    seq_level_df = seq_level_df.sort_values('sequence_id').reset_index(drop=True)\n",
        "\n",
        "    print(seq_level_df.head(100))\n",
        "\n",
        "    if isinstance(seq_level_df[target_col].iloc[0], (int, np.integer)):\n",
        "        original_labels = le.inverse_transform(seq_level_df[target_col])\n",
        "    else:\n",
        "        original_labels = seq_level_df[target_col].values\n",
        "\n",
        "    oof_preds_encoded = np.argmax(oof_proba, axis=1)\n",
        "    oof_preds_original = le.inverse_transform(oof_preds_encoded)\n",
        "\n",
        "    sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "    sub_df = pd.DataFrame({\"gesture\": oof_preds_original})\n",
        "\n",
        "    print(f\"Ground truth shape: {sol_df.shape}\")\n",
        "    print(f\"Predictions shape: {sub_df.shape}\")\n",
        "    print(f\"Unique ground truth gestures: {len(sol_df['gesture'].unique())}\")\n",
        "    print(f\"Unique predicted gestures: {len(sub_df['gesture'].unique())}\")\n",
        "\n",
        "    np.save('oof_preds_cnn.npy', oof_preds_original)\n",
        "    np.save('oof_proba_cnn.npy', oof_proba)\n",
        "    print(\"Saved OOF predictions to 'oof_preds_cnn.npy' and 'oof_proba_cnn.npy'\")\n",
        "\n",
        "    overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "    print(f\"\\nOverall CNN OOF Score: {overall_oof:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return oof_preds, oof_proba, oof_scores, overall_oof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "ePeIRVU3U3-r",
        "outputId": "822024fd-805a-4b48-c2bd-f49894a9db6f"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtpBdC9CheHk",
        "outputId": "eb195d94-acc0-420a-a022-1b30b2278143"
      },
      "outputs": [],
      "source": [
        "\n",
        "set_seed(42)\n",
        "\n",
        "add_swa_mixup_to_cnn_class()\n",
        "\n",
        "velocity_features = []\n",
        "rolling_features = []\n",
        "\n",
        "\n",
        "thm_vel_features = []\n",
        "thm_roll_features = []\n",
        "tof_vel_features = []\n",
        "tof_roll_features = []\n",
        "tof_regional_features = []\n",
        "\n",
        "imu_features = [\n",
        "                'acc_x', 'acc_y', 'acc_z',\n",
        "                'acc_mag',\n",
        "                'rot_w', 'rot_x', 'rot_y', 'rot_z',\n",
        "                'rot_mag',\n",
        "                'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
        "                'angular_distance',\n",
        "                ]\n",
        "\n",
        "thm_features = [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]\n",
        "tof_features = [\"tof_1_mean_distance\", \"tof_2_mean_distance\", \"tof_3_mean_distance\", \"tof_4_mean_distance\", \"tof_5_mean_distance\"]\n",
        "\n",
        "for col in ['acc_x', 'acc_y', 'acc_z', 'acc_mag']:\n",
        "        velocity_features.append(f\"{col}_vel\")\n",
        "\n",
        "for col in ['angular_vel_x', 'angular_vel_y', 'angular_vel_z']:\n",
        "        velocity_features.append(f\"{col}_vel\")\n",
        "\n",
        "for window in [3, 5, 10]:\n",
        "  for col in [\"acc_mag\", \"acc_x\", \"acc_y\", \"acc_z\", \"angular_vel_mag\",\"angular_vel_x\", \"angular_vel_y\", \"angular_vel_z\"]:\n",
        "\n",
        "      rolling_features.extend([\n",
        "          f\"{col}_roll_std_{window}\",\n",
        "          f\"{col}_roll_min_{window}\",\n",
        "      ])\n",
        "\n",
        "\n",
        "for col in [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]:\n",
        "    thm_vel_features.append(f\"{col}_vel\")\n",
        "\n",
        "\n",
        "for mode in [4]: # 4, 8\n",
        "        for sensor_idx in range(1, 6):\n",
        "            for region_idx in range(mode):\n",
        "                tof_regional_features.extend([\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_mean',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_std',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_min',\n",
        "                    f'tof{mode}_{sensor_idx}_region_{region_idx}_max',\n",
        "                ])\n",
        "\n",
        "\n",
        "features = imu_features + velocity_features + rolling_features\n",
        "#  + thm_features + thm_vel_features + tof_features + tof_regional_features\n",
        "\n",
        "\n",
        "oof_preds, oof_proba, oof_scores, overall_oof = train_cnn_cross_validation_with_swa(\n",
        "    train_df=train,\n",
        "    features=features,  # or FEATURES\n",
        "    target_col='gesture',\n",
        "    demographics_df=train_demographics,\n",
        "    n_splits=5,\n",
        "    label_encoder=None,\n",
        "    aggregation_method=\"swa\",\n",
        "    mixup_prob = 1.0,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Individual Fold Scores: {[f'{score:.4f}' for score in oof_scores]}\")\n",
        "print(f\"Mean Fold Score: {np.mean(oof_scores):.4f}  {np.std(oof_scores):.4f}\")\n",
        "print(f\"Overall OOF Score: {overall_oof:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90wOIOwzFPHj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "# Individual Fold Scores: ['0.7829', '0.7798', '0.7665', '0.7715', '0.7788']\n",
        "# Mean Fold Score: 0.7759  0.0060\n",
        "# Overall OOF Score: 0.7763\n",
        "\n",
        "\n",
        "\n",
        "# mode = 4 score: 0.8263, std: 0.0148 (thm, tof features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWCguK6KZhvs"
      },
      "outputs": [],
      "source": [
        "oof_proba_cat = np.load('/content/oof_preds_proba_cat.npy')\n",
        "oof_proba_cnn = np.load('/content/oof_proba_cnn.npy')\n",
        "oof_proba_lstm = np.load('/content/oof_proba_lstm.npy')\n",
        "oof_proba_cnn_custom = np.load('/content/oof_proba_cnn_custom.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTNGP9lR2dF2",
        "outputId": "76742f16-4716-4f27-aa4f-6d2c95729479"
      },
      "outputs": [],
      "source": [
        "oof_proba_cat.shape, oof_proba_cnn.shape, oof_proba_lstm.shape, oof_proba_cnn_custom.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcThpRZuanYU",
        "outputId": "482efad7-c0ef-46b0-e87f-a2bc43415e4c"
      },
      "outputs": [],
      "source": [
        "changes = np.where(np.any(np.diff(oof_proba_cat, axis=0) != 0, axis=1))[0] + 1\n",
        "last_indices = np.append(changes - 1, len(oof_proba_cat) - 1)\n",
        "cat_sequence_proba = oof_proba_cat[last_indices]\n",
        "cat_sequence_proba.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehdYIAw9cKZy"
      },
      "outputs": [],
      "source": [
        "le = joblib.load('/content/label_encoder.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "P5HC4N6Da7WF",
        "outputId": "24d1cc7d-9598-4ff8-c456-e058f227f54e"
      },
      "outputs": [],
      "source": [
        "original_labels = le.inverse_transform(train[CONFIG.TARGET])\n",
        "oof_preds_encoded = np.argmax(cat_sequence_proba, axis=1)\n",
        "oof_preds_original = le.inverse_transform(oof_preds_encoded)\n",
        "\n",
        "sol_df = pd.DataFrame({\"gesture\": original_labels})\n",
        "sub_df = pd.DataFrame({\"gesture\": oof_preds_original})\n",
        "\n",
        "np.save('oof_preds_cat.npy', oof_preds_original)\n",
        "np.save('oof_preds_proba_cat.npy', cat_sequence_proba)\n",
        "\n",
        "overall_oof = metric_calculator.calculate_hierarchical_f1(sol_df, sub_df)\n",
        "print(f\"Overall OOF Score: {overall_oof:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scyW4OaPbr_9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "M6ul16-2QYoT",
        "DJuAJBGWQWSk"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06753cd565854d1089d3bfe3fea964e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3de39c331fa846169faf24cef49b17c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4ccc32405c8941cfb251570e05659654": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54476366eefc4670bad20da1d4efcdbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b63363fdb884e96a694f5b8ae2de7e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e52f3ffba404f9cab372ab0448a3b36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9ac8cd73af4e90a5e19bee0cec5dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "a05291f0ba774954b16f737eb04f18be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc986db519c84ce7825c1622311a92b0",
              "IPY_MODEL_cf09961130b246fcb97cdf95455a757b",
              "IPY_MODEL_a807eba7ba85431883727ace3b43ebf4",
              "IPY_MODEL_c609cf25010e4ff9b53b9371cbf1556d",
              "IPY_MODEL_d254b5e883e54578b396d34a67b556e8"
            ],
            "layout": "IPY_MODEL_7e9ac8cd73af4e90a5e19bee0cec5dcc"
          }
        },
        "a807eba7ba85431883727ace3b43ebf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ba51e4bab47c451c815033386c68b785",
            "placeholder": "",
            "style": "IPY_MODEL_4ccc32405c8941cfb251570e05659654",
            "value": ""
          }
        },
        "ba51e4bab47c451c815033386c68b785": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdb1cd702a894abf90600c1fda372928": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c609cf25010e4ff9b53b9371cbf1556d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ccfcae3041754b5fb24af74aa3cff7e3",
            "style": "IPY_MODEL_3de39c331fa846169faf24cef49b17c2",
            "tooltip": ""
          }
        },
        "ccfcae3041754b5fb24af74aa3cff7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf09961130b246fcb97cdf95455a757b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7e52f3ffba404f9cab372ab0448a3b36",
            "placeholder": "",
            "style": "IPY_MODEL_54476366eefc4670bad20da1d4efcdbf",
            "value": ""
          }
        },
        "d04417ccfebd412db9f00fe896036845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d254b5e883e54578b396d34a67b556e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b63363fdb884e96a694f5b8ae2de7e1",
            "placeholder": "",
            "style": "IPY_MODEL_d04417ccfebd412db9f00fe896036845",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "dc986db519c84ce7825c1622311a92b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb1cd702a894abf90600c1fda372928",
            "placeholder": "",
            "style": "IPY_MODEL_06753cd565854d1089d3bfe3fea964e7",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
